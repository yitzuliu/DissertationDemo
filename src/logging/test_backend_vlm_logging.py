#!/usr/bin/env python3
"""
å¾Œç«¯VLMè™•ç†æ—¥èªŒè¨˜éŒ„ç«¯åˆ°ç«¯æ¸¬è©¦

æ¨¡æ“¬å®Œæ•´çš„VLMè™•ç†æµç¨‹ï¼Œé©—è­‰æ‰€æœ‰æ—¥èªŒè¨˜éŒ„åŠŸèƒ½

ä½¿ç”¨æ–¹æ³•:
1. å…ˆå•Ÿå‹•SmolVLMæœå‹™å™¨: python src/models/smolvlm/run_smolvlm.py
2. åœ¨å¦ä¸€å€‹çµ‚ç«¯é‹è¡Œæ­¤æ¸¬è©¦: python src/logging/test_backend_vlm_logging.py
3. æ¸¬è©¦å®Œæˆå¾Œå¯ä»¥åœæ­¢SmolVLMæœå‹™å™¨ (Ctrl+C)
"""

import asyncio
import json
import time
import uuid
import subprocess
import requests
import signal
import sys
from datetime import datetime
from typing import Dict, Any
from pathlib import Path

class SmolVLMServerManager:
    """SmolVLMæœå‹™å™¨ç®¡ç†å™¨"""
    
    def __init__(self):
        self.process = None
        self.model_port = 8080
        self.base_dir = Path(__file__).parent.parent.parent
        self.server_script = self.base_dir / "src" / "models" / "smolvlm" / "run_smolvlm.py"
        
    def check_server_running(self):
        """æª¢æŸ¥æœå‹™å™¨æ˜¯å¦é‹è¡Œ"""
        try:
            response = requests.get(f"http://localhost:{self.model_port}/v1/models", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def start_server(self):
        """å•Ÿå‹•SmolVLMæœå‹™å™¨"""
        if self.check_server_running():
            print("âœ… SmolVLMæœå‹™å™¨å·²ç¶“åœ¨é‹è¡Œ")
            return True
            
        if not self.server_script.exists():
            print(f"âŒ æ‰¾ä¸åˆ°SmolVLMæœå‹™å™¨è…³æœ¬: {self.server_script}")
            return False
            
        print("ğŸš€ å•Ÿå‹•SmolVLMæœå‹™å™¨...")
        try:
            self.process = subprocess.Popen(
                [sys.executable, str(self.server_script)],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            # ç­‰å¾…æœå‹™å™¨å•Ÿå‹•
            print("â³ ç­‰å¾…æœå‹™å™¨å•Ÿå‹•...")
            for i in range(30):  # ç­‰å¾…æœ€å¤š30ç§’
                if self.check_server_running():
                    print("âœ… SmolVLMæœå‹™å™¨å•Ÿå‹•æˆåŠŸ")
                    return True
                time.sleep(1)
                print(f"   ç­‰å¾…ä¸­... ({i+1}/30)")
            
            print("âŒ SmolVLMæœå‹™å™¨å•Ÿå‹•è¶…æ™‚")
            self.stop_server()
            return False
            
        except Exception as e:
            print(f"âŒ å•Ÿå‹•SmolVLMæœå‹™å™¨å¤±æ•—: {e}")
            return False
    
    def stop_server(self):
        """åœæ­¢SmolVLMæœå‹™å™¨"""
        if self.process:
            print("ğŸ›‘ åœæ­¢SmolVLMæœå‹™å™¨...")
            self.process.terminate()
            try:
                self.process.wait(timeout=5)
                print("âœ… SmolVLMæœå‹™å™¨å·²åœæ­¢")
            except subprocess.TimeoutExpired:
                print("âš ï¸ å¼·åˆ¶åœæ­¢SmolVLMæœå‹™å™¨...")
                self.process.kill()
                self.process.wait()
                print("âœ… SmolVLMæœå‹™å™¨å·²å¼·åˆ¶åœæ­¢")
            self.process = None


# æ¨¡æ“¬å¾Œç«¯VLMè™•ç†æµç¨‹
class MockVLMProcessor:
    def __init__(self):
        # å°å…¥è¦–è¦ºæ—¥èªŒè¨˜éŒ„å™¨
        import sys
        import os
        sys.path.append(os.path.dirname(__file__))
        from visual_logger import get_visual_logger
        
        self.visual_logger = get_visual_logger()
        self.model = "smolvlm"
    
    async def process_chat_completion(self, request_data: Dict[str, Any]):
        """æ¨¡æ“¬å®Œæ•´çš„chat completionè™•ç†æµç¨‹"""
        request_start_time = time.time()
        request_id = f"req_{int(request_start_time * 1000)}"
        observation_id = f"obs_{int(request_start_time * 1000)}_{uuid.uuid4().hex[:8]}"
        
        print(f"ğŸš€ é–‹å§‹è™•ç†VLMè«‹æ±‚ - è§€å¯ŸID: {observation_id}")
        
        try:
            # 1. è¨˜éŒ„å¾Œç«¯æ¥æ”¶
            print("ğŸ“¥ æ­¥é©Ÿ 1: è¨˜éŒ„å¾Œç«¯æ¥æ”¶")
            self.visual_logger.log_backend_receive(observation_id, request_id, request_data)
            await asyncio.sleep(0.01)
            
            # 2. åœ–åƒè™•ç†
            print("ğŸ–¼ï¸ æ­¥é©Ÿ 2: åœ–åƒè™•ç†")
            image_count = self._count_images(request_data)
            self.visual_logger.log_image_processing_start(observation_id, request_id, image_count, self.model)
            
            # æ¨¡æ“¬åœ–åƒè™•ç†æ™‚é–“
            image_processing_time = 0.15
            await asyncio.sleep(image_processing_time)
            
            self.visual_logger.log_image_processing_result(
                observation_id, request_id, image_processing_time, True,
                {"image_count": image_count, "model": self.model, "resolution": "1024x768"}
            )
            
            # 3. VLMè«‹æ±‚
            print("ğŸ¤– æ­¥é©Ÿ 3: VLMæ¨¡å‹è«‹æ±‚")
            prompt_length = self._calculate_prompt_length(request_data)
            self.visual_logger.log_vlm_request(observation_id, request_id, self.model, prompt_length, image_count)
            
            # æ¨¡æ“¬VLMæ¨ç†æ™‚é–“
            vlm_processing_time = 0.85
            await asyncio.sleep(vlm_processing_time)
            
            # æ¨¡æ“¬VLMå›æ‡‰
            vlm_response = "I can see coffee brewing equipment including a pour-over dripper, coffee filter, gooseneck kettle, digital scale, and coffee beans. This appears to be step 1 of the coffee brewing process - gathering equipment and ingredients."
            response_length = len(vlm_response)
            
            self.visual_logger.log_vlm_response(
                observation_id, request_id, response_length, vlm_processing_time, True, self.model
            )
            
            # 4. RAGè³‡æ–™å‚³é
            print("ğŸ”„ æ­¥é©Ÿ 4: RAGè³‡æ–™å‚³é")
            self.visual_logger.log_rag_data_transfer(observation_id, vlm_response, True)
            
            # 5. ç‹€æ…‹è¿½è¹¤å™¨æ•´åˆ
            print("ğŸ“Š æ­¥é©Ÿ 5: ç‹€æ…‹è¿½è¹¤å™¨æ•´åˆ")
            state_processing_time = 0.05
            await asyncio.sleep(state_processing_time)
            
            state_updated = True  # æ¨¡æ“¬ç‹€æ…‹æ›´æ–°æˆåŠŸ
            self.visual_logger.log_state_tracker_integration(observation_id, state_updated, state_processing_time)
            
            # 6. æ€§èƒ½æŒ‡æ¨™è¨˜éŒ„
            print("âš¡ æ­¥é©Ÿ 6: æ€§èƒ½æŒ‡æ¨™è¨˜éŒ„")
            total_time = time.time() - request_start_time
            
            self.visual_logger.log_performance_metric(observation_id, "total_processing_time", total_time, "s")
            self.visual_logger.log_performance_metric(observation_id, "image_processing_time", image_processing_time, "s")
            self.visual_logger.log_performance_metric(observation_id, "model_inference_time", vlm_processing_time, "s")
            self.visual_logger.log_performance_metric(observation_id, "state_tracker_time", state_processing_time, "s")
            
            print(f"âœ… VLMè™•ç†å®Œæˆ - ç¸½æ™‚é–“: {total_time:.2f}s")
            
            return {
                "choices": [
                    {
                        "message": {
                            "content": vlm_response,
                            "role": "assistant"
                        }
                    }
                ],
                "model": self.model,
                "usage": {
                    "prompt_tokens": prompt_length,
                    "completion_tokens": response_length,
                    "total_tokens": prompt_length + response_length
                }
            }
            
        except Exception as e:
            # éŒ¯èª¤è™•ç†
            error_time = time.time() - request_start_time
            print(f"âŒ è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
            
            self.visual_logger.log_error(observation_id, request_id, type(e).__name__, str(e), "vlm_processing")
            self.visual_logger.log_performance_metric(observation_id, "error_time", error_time, "s")
            
            raise
    
    def _count_images(self, request_data: Dict[str, Any]) -> int:
        """è¨ˆç®—è«‹æ±‚ä¸­çš„åœ–åƒæ•¸é‡"""
        image_count = 0
        messages = request_data.get("messages", [])
        
        for message in messages:
            content = message.get("content", [])
            if isinstance(content, list):
                for item in content:
                    if isinstance(item, dict) and item.get("type") == "image_url":
                        image_count += 1
        
        return image_count
    
    def _calculate_prompt_length(self, request_data: Dict[str, Any]) -> int:
        """è¨ˆç®—æç¤ºè©é•·åº¦"""
        prompt_length = 0
        messages = request_data.get("messages", [])
        
        for message in messages:
            content = message.get("content", [])
            if isinstance(content, str):
                prompt_length += len(content)
            elif isinstance(content, list):
                for item in content:
                    if isinstance(item, dict) and item.get("type") == "text":
                        prompt_length += len(item.get("text", ""))
        
        return prompt_length


async def test_single_image_request():
    """æ¸¬è©¦å–®åœ–åƒè«‹æ±‚"""
    print("ğŸ§ª æ¸¬è©¦ 1: å–®åœ–åƒè«‹æ±‚")
    print("-" * 40)
    
    processor = MockVLMProcessor()
    
    request_data = {
        "model": "smolvlm",
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "What do you see in this image?"},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": "data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD..."
                        }
                    }
                ]
            }
        ],
        "max_tokens": 100,
        "temperature": 0.7
    }
    
    result = await processor.process_chat_completion(request_data)
    print(f"âœ… æ¸¬è©¦ 1 å®Œæˆ - å›æ‡‰é•·åº¦: {len(result['choices'][0]['message']['content'])}")
    return True


async def test_multiple_images_request():
    """æ¸¬è©¦å¤šåœ–åƒè«‹æ±‚"""
    print("\nğŸ§ª æ¸¬è©¦ 2: å¤šåœ–åƒè«‹æ±‚")
    print("-" * 40)
    
    processor = MockVLMProcessor()
    
    request_data = {
        "model": "smolvlm",
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "Compare these two images and describe the differences"},
                    {
                        "type": "image_url",
                        "image_url": {"url": "data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD..."}
                    },
                    {
                        "type": "image_url",
                        "image_url": {"url": "data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD..."}
                    }
                ]
            }
        ],
        "max_tokens": 200
    }
    
    result = await processor.process_chat_completion(request_data)
    print(f"âœ… æ¸¬è©¦ 2 å®Œæˆ - å›æ‡‰é•·åº¦: {len(result['choices'][0]['message']['content'])}")
    return True


async def test_text_only_request():
    """æ¸¬è©¦ç´”æ–‡æœ¬è«‹æ±‚"""
    print("\nğŸ§ª æ¸¬è©¦ 3: ç´”æ–‡æœ¬è«‹æ±‚")
    print("-" * 40)
    
    processor = MockVLMProcessor()
    
    request_data = {
        "model": "smolvlm",
        "messages": [
            {
                "role": "user",
                "content": "Explain the coffee brewing process step by step"
            }
        ],
        "max_tokens": 150
    }
    
    result = await processor.process_chat_completion(request_data)
    print(f"âœ… æ¸¬è©¦ 3 å®Œæˆ - å›æ‡‰é•·åº¦: {len(result['choices'][0]['message']['content'])}")
    return True


async def test_error_scenario():
    """æ¸¬è©¦éŒ¯èª¤å ´æ™¯"""
    print("\nğŸ§ª æ¸¬è©¦ 4: éŒ¯èª¤å ´æ™¯")
    print("-" * 40)
    
    processor = MockVLMProcessor()
    
    # æ¨¡æ“¬æœƒå°è‡´éŒ¯èª¤çš„è«‹æ±‚
    request_data = {
        "model": "invalid_model",
        "messages": [],  # ç©ºæ¶ˆæ¯åˆ—è¡¨
        "max_tokens": -1  # ç„¡æ•ˆçš„tokenæ•¸é‡
    }
    
    try:
        # åœ¨VLMè«‹æ±‚éšæ®µæ¨¡æ“¬éŒ¯èª¤
        original_method = processor.visual_logger.log_vlm_request
        
        def mock_vlm_request(*args, **kwargs):
            original_method(*args, **kwargs)
            raise ConnectionError("Failed to connect to model server")
        
        processor.visual_logger.log_vlm_request = mock_vlm_request
        
        await processor.process_chat_completion(request_data)
        print("âŒ æ¸¬è©¦ 4 å¤±æ•— - æ‡‰è©²æ‹‹å‡ºç•°å¸¸")
        return False
        
    except ConnectionError:
        # æ¢å¾©åŸå§‹æ–¹æ³•
        processor.visual_logger.log_vlm_request = original_method
        print("âœ… æ¸¬è©¦ 4 å®Œæˆ - éŒ¯èª¤æ­£ç¢ºè™•ç†å’Œè¨˜éŒ„")
        return True
    except Exception as e:
        # æ¢å¾©åŸå§‹æ–¹æ³•
        processor.visual_logger.log_vlm_request = original_method
        print(f"âš ï¸ æ¸¬è©¦ 4 éƒ¨åˆ†æˆåŠŸ - æ•ç²åˆ°ç•°å¸¸: {type(e).__name__}")
        return True


async def test_performance_monitoring():
    """æ¸¬è©¦æ€§èƒ½ç›£æ§"""
    print("\nğŸ§ª æ¸¬è©¦ 5: æ€§èƒ½ç›£æ§")
    print("-" * 40)
    
    # å‰µå»ºæ–°çš„è™•ç†å™¨å¯¦ä¾‹ï¼Œé¿å…ä¹‹å‰æ¸¬è©¦çš„mockå½±éŸ¿
    processor = MockVLMProcessor()
    
    # åŸ·è¡Œå¤šå€‹è«‹æ±‚ä¾†æ¸¬è©¦æ€§èƒ½ç›£æ§
    requests = []
    for i in range(3):
        request_data = {
            "model": "smolvlm",
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": f"Analyze image {i+1}"},
                        {
                            "type": "image_url",
                            "image_url": {"url": f"data:image/jpeg;base64,image_{i+1}_data"}
                        }
                    ]
                }
            ],
            "max_tokens": 100
        }
        requests.append(request_data)
    
    start_time = time.time()
    results = []
    
    for i, request_data in enumerate(requests):
        print(f"  è™•ç†è«‹æ±‚ {i+1}/3")
        result = await processor.process_chat_completion(request_data)
        results.append(result)
        await asyncio.sleep(0.1)  # é–“éš”æ™‚é–“
    
    total_time = time.time() - start_time
    print(f"âœ… æ¸¬è©¦ 5 å®Œæˆ - è™•ç†äº† {len(results)} å€‹è«‹æ±‚ï¼Œç¸½æ™‚é–“: {total_time:.2f}s")
    return True


async def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print("ğŸ§ª å¾Œç«¯VLMè™•ç†æ—¥èªŒè¨˜éŒ„ç«¯åˆ°ç«¯æ¸¬è©¦")
    print("=" * 60)
    
    # æª¢æŸ¥æ˜¯å¦éœ€è¦å•Ÿå‹•æœå‹™å™¨
    server_manager = SmolVLMServerManager()
    server_started_by_test = False
    
    print("ğŸ” æª¢æŸ¥SmolVLMæœå‹™å™¨ç‹€æ…‹...")
    if not server_manager.check_server_running():
        print("âš ï¸ SmolVLMæœå‹™å™¨æœªé‹è¡Œ")
        print("\næœ‰å…©ç¨®æ–¹å¼é‹è¡Œæ­¤æ¸¬è©¦:")
        print("1. è‡ªå‹•å•Ÿå‹•æœå‹™å™¨ (æ¨è–¦)")
        print("2. æ‰‹å‹•å•Ÿå‹•æœå‹™å™¨")
        print("\né¸æ“‡è‡ªå‹•å•Ÿå‹•? (y/n): ", end="")
        
        try:
            choice = input().lower().strip()
            if choice in ['y', 'yes', '']:
                if server_manager.start_server():
                    server_started_by_test = True
                else:
                    print("âŒ ç„¡æ³•å•Ÿå‹•æœå‹™å™¨ï¼Œè«‹æ‰‹å‹•å•Ÿå‹•å¾Œé‡è©¦")
                    print("æ‰‹å‹•å•Ÿå‹•å‘½ä»¤: python src/models/smolvlm/run_smolvlm.py")
                    return False
            else:
                print("è«‹å…ˆæ‰‹å‹•å•Ÿå‹•SmolVLMæœå‹™å™¨:")
                print("python src/models/smolvlm/run_smolvlm.py")
                print("ç„¶å¾Œé‡æ–°é‹è¡Œæ­¤æ¸¬è©¦")
                return False
        except KeyboardInterrupt:
            print("\næ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
            return False
    else:
        print("âœ… SmolVLMæœå‹™å™¨æ­£åœ¨é‹è¡Œ")
    
    test_results = []
    
    try:
        print("\nğŸ¯ é–‹å§‹åŸ·è¡Œæ¸¬è©¦...")
        print("=" * 60)
        
        # åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦
        test_results.append(await test_single_image_request())
        test_results.append(await test_multiple_images_request())
        test_results.append(await test_text_only_request())
        test_results.append(await test_error_scenario())
        test_results.append(await test_performance_monitoring())
        
        # é¡¯ç¤ºæ¸¬è©¦çµæœ
        print("\n" + "=" * 60)
        print("ğŸ“Š æ¸¬è©¦çµæœæ‘˜è¦")
        print("=" * 60)
        
        total_tests = len(test_results)
        passed_tests = sum(test_results)
        success_rate = (passed_tests / total_tests * 100)
        
        print(f"ç¸½æ¸¬è©¦æ•¸é‡: {total_tests}")
        print(f"é€šéæ¸¬è©¦: {passed_tests}")
        print(f"æˆåŠŸç‡: {success_rate:.1f}%")
        
        if passed_tests == total_tests:
            print("\nğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼å¾Œç«¯VLMè™•ç†æ—¥èªŒè¨˜éŒ„åŠŸèƒ½æ­£å¸¸å·¥ä½œã€‚")
        else:
            print("\nâš ï¸ éƒ¨åˆ†æ¸¬è©¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥æ—¥èªŒè¼¸å‡ºã€‚")
        
        # æª¢æŸ¥æ—¥èªŒæ–‡ä»¶
        import os
        log_dir = os.path.join(os.path.dirname(__file__), "..", "..", "logs")
        if os.path.exists(log_dir):
            visual_log_files = [f for f in os.listdir(log_dir) if f.startswith("visual_")]
            if visual_log_files:
                print(f"\nğŸ“ ç”Ÿæˆçš„è¦–è¦ºæ—¥èªŒæ–‡ä»¶: {visual_log_files}")
                
                # é¡¯ç¤ºæœ€æ–°æ—¥èªŒçš„æœ€å¾Œå¹¾è¡Œ
                latest_log = max(visual_log_files)
                log_path = os.path.join(log_dir, latest_log)
                try:
                    with open(log_path, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                        if lines:
                            print(f"\nğŸ“„ æœ€æ–°æ—¥èªŒå…§å®¹ï¼ˆæœ€å¾Œ5è¡Œï¼‰:")
                            for line in lines[-5:]:
                                print(f"   {line.strip()}")
                except Exception as e:
                    print(f"   ç„¡æ³•è®€å–æ—¥èªŒæ–‡ä»¶: {e}")
        
        return passed_tests == total_tests
        
    except Exception as e:
        print(f"âŒ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        # æ¸…ç†æœå‹™å™¨
        if server_started_by_test:
            print("\nğŸ§¹ æ¸…ç†æ¸¬è©¦ç’°å¢ƒ...")
            server_manager.stop_server()


def signal_handler(signum, frame):
    """è™•ç†ä¸­æ–·ä¿¡è™Ÿ"""
    print("\nğŸ›‘ æ¸¬è©¦è¢«ä¸­æ–·ï¼Œæ­£åœ¨æ¸…ç†...")
    sys.exit(0)


if __name__ == "__main__":
    # è¨»å†Šä¿¡è™Ÿè™•ç†å™¨
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    success = asyncio.run(main())
    exit(0 if success else 1)