{
    "model_name": "Phi-3.5-Vision",
    "model_path": "microsoft/Phi-3.5-vision-instruct",
    "server": {
        "command": "python",
        "args": ["-m", "vllm.entrypoints.openai.api_server", "--model", "microsoft/Phi-3.5-vision-instruct", "--trust-remote-code", "--port", "8080"],
        "port": 8080,
        "health_endpoint": "/health"
    },
    "image_processing": {
        "size": [336, 336],
        "format": "jpeg",
        "jpeg_quality": 95
    },
    "model_config": {
        "max_new_tokens": 500,
        "temperature": 0.3,
        "top_p": 0.9,
        "frequency_penalty": 0.1,
        "presence_penalty": 0.1
    },
    "api": {
        "endpoint": "/v1/chat/completions",
        "timeout": 120,
        "max_tokens": 500
    },
    "ui": {
        "default_instruction": "Please describe this image in detail, including main objects, colors, scene, and other visual elements.",
        "capture_intervals": [1000, 2000, 3000, 5000],
        "default_interval": 2000
    },
    "memory": {
        "max_previous_scenes": 5,
        "max_context_history": 3
    },
    "vllm_config": {
        "gpu_memory_utilization": 0.8,
        "max_num_batched_tokens": 4096,
        "trust_remote_code": true,
        "dtype": "auto"
    },
    "version": "2.0.0",
    "description": "Microsoft Phi-3.5 Vision 4.2B multimodal model with vLLM acceleration",
    "features": [
        "Enhanced multi-modal understanding",
        "Improved video understanding", 
        "Advanced OCR capabilities",
        "Better multi-image support",
        "128K context window",
        "Enhanced reasoning capabilities"
    ]
} 