"""
Phi-3.5-Vision Model Implementation

This module implements the BaseVisionModel interface for the Phi-3.5-Vision model.
Standard version using transformers for maximum compatibility.
"""

import time
import json
import base64
import numpy as np
from typing import Dict, Any, Optional, Union, List
from PIL import Image
import logging
import io
import torch
from transformers import AutoModelForCausalLM, AutoProcessor
import threading
import gc
from pathlib import Path

# Add parent directories to path for imports
import sys
import os
# Add the project root to the path (go up 4 levels: file -> Phi_3.5_Vision MLX -> models -> src -> root)
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
sys.path.append(project_root)

# Import with fallback strategy
try:
    from src.models.base_model import BaseVisionModel
except ImportError:
    try:
        from models.base_model import BaseVisionModel
    except ImportError:
        # Minimal replacement if imports fail
        class BaseVisionModel:
            def __init__(self, model_name: str, config: Dict[str, Any]):
                self.model_name = model_name
                self.config = config
                self.loaded = False
                self.load_time = 0
                self.stats = {"requests": 0, "total_time": 0.0}
            
            def _update_stats(self, processing_time: float):
                self.stats["requests"] += 1
                self.stats["total_time"] += processing_time

try:
    from src.backend.utils.image_processing import preprocess_for_model
except ImportError:
    # Fallback image preprocessing
    def preprocess_for_model(image, model_type="phi3_vision", config=None, return_format="pil"):
        if isinstance(image, np.ndarray):
            image = Image.fromarray(image)
        if image.mode != 'RGB':
            image = image.convert('RGB')
        return image

logger = logging.getLogger(__name__)

class Phi3VisionMemoryManager:
    """
    Memory management utilities for Phi-3.5-Vision
    """
    
    @staticmethod
    def cleanup_memory(aggressive=False):
        """Clean up memory with optional aggressive mode"""
        try:
            if aggressive:
                gc.collect()  # Force Python garbage collection
                logger.debug("Python garbage collected")
            
            if torch.backends.mps.is_available():
                torch.mps.empty_cache()
                logger.debug("MPS cache cleared")
            elif torch.cuda.is_available():
                torch.cuda.empty_cache()
                logger.debug("CUDA cache cleared")
                
        except Exception as e:
            logger.warning(f"Memory cleanup warning: {e}")
    
    @staticmethod
    def check_memory_availability(required_gb=8.0):
        """Check if enough memory is available for operation"""
        try:
            if torch.backends.mps.is_available():
                current_memory = torch.mps.current_allocated_memory() / 1024**3
                max_memory = 18.0  # Typical M1/M2 unified memory
                available = max_memory - current_memory
                
                if available < required_gb:
                    logger.warning(f"Low memory: {available:.2f} GB available, {required_gb:.2f} GB required")
                    Phi3VisionMemoryManager.cleanup_memory(aggressive=True)
                    return False
                else:
                    logger.debug(f"Memory OK: {available:.2f} GB available")
                    return True
            return True  # Assume OK if MPS not available
                
        except Exception as e:
            logger.warning(f"Memory check failed: {e}")
            return True  # Assume OK if can't check

class Phi3VisionServer:
    """
    Manages the Phi-3.5-Vision model in-process (standard version)
    """
    
    def __init__(self, model_path: str, device: str = "cpu"):
        self.model_path = model_path
        self.device = device
        self.model = None
        self.processor = None
        self.loaded = False
        self.memory_manager = Phi3VisionMemoryManager()
        
    def start(self) -> bool:
        """Load Phi-3.5-Vision model into memory"""
        if self.loaded:
            logger.info("Phi-3.5-Vision model already loaded")
            return True
            
        logger.info(f"Loading Phi-3.5-Vision model from: {self.model_path}")
        
        try:
            start_time = time.time()
            
            # Check memory before loading
            if not self.memory_manager.check_memory_availability(required_gb=8.0):
                logger.error("Insufficient memory for Phi-3.5-Vision model")
                return False
            
            # Load processor
            processor_start = time.time()
            self.processor = AutoProcessor.from_pretrained(
                self.model_path, 
                trust_remote_code=True
            )
            processor_time = time.time() - processor_start
            
            # Load model with CPU-optimized settings for compatibility
            model_start = time.time()
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                torch_dtype=torch.float16,  # Use float16 to save memory
                device_map="cpu",  # Force CPU for maximum compatibility
                trust_remote_code=True,
                _attn_implementation="eager",  # Disable FlashAttention2 for compatibility
                low_cpu_mem_usage=True  # Use less CPU memory
            )
            model_time = time.time() - model_start
            
            total_time = time.time() - start_time
            
            logger.info(f"Processor loaded: {processor_time:.2f}s")
            logger.info(f"Model loaded: {model_time:.2f}s")
            logger.info(f"Total loading time: {total_time:.2f}s")
            logger.info(f"Device: {self.device}")
            logger.info("⚠️ Using CPU for maximum compatibility (may be slower)")
            
            self.loaded = True
            return True
            
        except Exception as e:
            logger.error(f"Error loading Phi-3.5-Vision model: {e}")
            self.loaded = False
            return False
    
    def stop(self) -> bool:
        """Unload model from memory"""
        if not self.loaded:
            return True
            
        logger.info("Unloading Phi-3.5-Vision model...")
        try:
            if hasattr(self, 'model') and self.model is not None:
                del self.model
                self.model = None
            if hasattr(self, 'processor') and self.processor is not None:
                del self.processor
                self.processor = None
            
            self.memory_manager.cleanup_memory(aggressive=True)
            self.loaded = False
            logger.info("Phi-3.5-Vision model unloaded successfully")
            return True
            
        except Exception as e:
            logger.error(f"Error unloading Phi-3.5-Vision model: {e}")
            return False
    
    def is_running(self) -> bool:
        """Check if model is loaded and ready"""
        return self.loaded and self.model is not None and self.processor is not None
    
    def generate_response(self, image: Image.Image, prompt: str, max_tokens: int = 100) -> Dict[str, Any]:
        """Generate response from Phi-3.5-Vision model"""
        if not self.is_running():
            return {"error": "Model not loaded"}
        
        try:
            # Pre-check memory availability
            if not self.memory_manager.check_memory_availability(required_gb=4.0):
                return {"error": "Insufficient memory for inference"}
            
            start_time = time.time()
            
            # Phi-3.5-Vision special format
            messages = [
                {"role": "user", "content": f"<|image_1|>\n{prompt}"}
            ]
            
            # Apply chat template
            prompt_text = self.processor.tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            # Process inputs
            inputs = self.processor(prompt_text, [image], return_tensors="pt")
            
            # Move to device
            device = next(self.model.parameters()).device
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # Generate response with error handling
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    do_sample=False,
                    use_cache=False,  # Disable cache to avoid DynamicCache error
                    pad_token_id=self.processor.tokenizer.eos_token_id
                )
            
            response = self.processor.decode(outputs[0], skip_special_tokens=True)
            
            inference_time = time.time() - start_time
            
            # Memory cleanup
            del inputs, outputs
            self.memory_manager.cleanup_memory()
            
            return {
                "success": True,
                "response": response,
                "inference_time": inference_time
            }
            
        except Exception as e:
            logger.error(f"Error during Phi-3.5-Vision inference: {e}")
            # Cleanup on error
            self.memory_manager.cleanup_memory(aggressive=True)
            return {"error": f"Inference failed: {str(e)}"}


class Phi3VisionModel(BaseVisionModel):
    """
    Standard implementation of the Phi-3.5-Vision model using transformers.
    """
    
    def __init__(self, model_name: str, config: Dict[str, Any]):
        """
        Initialize the Phi-3.5-Vision model.
        
        Args:
            model_name: The name of the model
            config: A dictionary containing model configuration
        """
        super().__init__(model_name, config)
        
        # Get model path from config
        self.model_path = config.get("model_path", "microsoft/Phi-3.5-vision-instruct")
        self.device = config.get("device", "cpu")  # Default to CPU for compatibility
        self.timeout = config.get("timeout", 180)  # Longer timeout for CPU inference
        self.max_tokens = config.get("max_tokens", 100)
        
        logger.info(f"🔧 Model path: {self.model_path}")
        logger.info(f"🔧 Device: {self.device} (standard version)")
        
        # Create server
        self.server = Phi3VisionServer(self.model_path, self.device)
    
    def load_model(self) -> bool:
        """
        Load the Phi-3.5-Vision model.
        
        Returns:
            True if loading was successful, False otherwise
        """
        try:
            start_time = time.time()
            
            logger.info(f"Loading Phi-3.5-Vision model: {self.model_name}")
            if not self.server.start():
                logger.error("Failed to start Phi-3.5-Vision server")
                return False
                
            self.loaded = True
            self.load_time = time.time() - start_time
            
            logger.info(f"Phi-3.5-Vision model {self.model_name} ready")
            return True
            
        except Exception as e:
            logger.error(f"Error loading Phi-3.5-Vision model: {str(e)}")
            self.loaded = False
            return False
    
    def preprocess_image(self, image: Union[Image.Image, np.ndarray]) -> Image.Image:
        """
        Preprocess the input image for the model.
        
        Args:
            image: The input image, either as PIL Image or numpy array
            
        Returns:
            PIL Image ready for the model
        """
        try:
            # Process the image to get a PIL image
            pil_image = preprocess_for_model(
                image=image,
                model_type="phi3_vision",
                config=self.config,
                return_format="pil"
            )
            
            # Apply Phi-3.5-Vision-specific preprocessing
            image_config = self.config.get("image_processing", {})
            max_size = image_config.get("max_size", 1024)
            
            if max(pil_image.size) > max_size:
                scale = max_size / max(pil_image.size)
                new_size = (int(pil_image.size[0] * scale), int(pil_image.size[1] * scale))
                pil_image = pil_image.resize(new_size, Image.Resampling.LANCZOS)
                logger.debug(f"Resized image to {new_size}")
            
            # Ensure RGB
            if pil_image.mode != 'RGB':
                pil_image = pil_image.convert('RGB')
            
            return pil_image
            
        except Exception as e:
            logger.error(f"Error preprocessing image for Phi-3.5-Vision: {str(e)}")
            raise e
    
    def predict(self, 
                image: Union[Image.Image, np.ndarray], 
                prompt: str, 
                options: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Generate a prediction from the model.
        
        Args:
            image: The input image
            prompt: The text prompt to guide the model
            options: Additional model-specific options
            
        Returns:
            A dictionary containing the model's response
        """
        if not self.loaded:
            if not self.load_model():
                return {"error": "Model is not available"}
        
        try:
            start_time = time.time()
            
            # Apply default options if not provided
            if options is None:
                options = {}
                
            # Get generation parameters
            max_tokens = options.get("max_tokens", self.max_tokens)
            
            # Preprocess the image
            processed_image = self.preprocess_image(image)
            
            # Generate response using the server
            result = self.server.generate_response(processed_image, prompt, max_tokens)
            
            if "error" in result:
                return result
            
            processing_time = time.time() - start_time
            self._update_stats(processing_time)
            
            # Format the response in standard format
            formatted_result = self.format_response(result["response"])
            formatted_result["processing_time"] = processing_time
            formatted_result["inference_time"] = result.get("inference_time", 0)
            
            return formatted_result
            
        except Exception as e:
            logger.error(f"Error during prediction with {self.model_name}: {str(e)}")
            return {
                "error": f"Failed to analyze image with {self.model_name}",
                "details": str(e)
            }
    
    def format_response(self, raw_response: str) -> Dict[str, Any]:
        """
        Format the raw model response into a standardized format.
        
        Args:
            raw_response: The raw text output from the model
            
        Returns:
            A standardized response dictionary
        """
        try:
            # Clean up response
            clean_text = raw_response.strip()
            
            # Try to parse as JSON if it looks like JSON
            if clean_text.startswith('{') and clean_text.endswith('}'):
                try:
                    parsed_json = json.loads(clean_text)
                    return {
                        "success": True,
                        "response": parsed_json,
                        "raw_response": raw_response
                    }
                except json.JSONDecodeError:
                    pass
            
            # Return as text response
            return {
                "success": True,
                "response": {
                    "text": clean_text
                },
                "raw_response": raw_response
            }
                
        except Exception as e:
            logger.error(f"Error formatting response: {str(e)}")
            return {
                "success": False,
                "error": "Failed to format model response",
                "raw_response": raw_response
            }
    
    def unload_model(self) -> bool:
        """
        Unload the model from memory to free resources.
        
        Returns:
            True if unloading was successful, False otherwise
        """
        try:
            if self.server:
                self.server.stop()
                
            self.loaded = False
            logger.info(f"Model {self.model_name} unloaded successfully")
            return True
        except Exception as e:
            logger.error(f"Error unloading model {self.model_name}: {str(e)}")
            return False

    def get_model_info(self) -> Dict[str, Any]:
        """Get model information"""
        return {
            "model_name": self.model_name,
            "model_path": self.model_path,
            "device": self.device,
            "loaded": self.loaded,
            "load_time": getattr(self, 'load_time', 0),
            "stats": getattr(self, 'stats', {}),
            "version": "standard"
        } 