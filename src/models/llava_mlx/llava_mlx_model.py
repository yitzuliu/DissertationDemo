from pathlib import Path
import sys
import os
import tempfile
import base64
import gc
import torch
from io import BytesIO
from PIL import Image
from typing import Dict, Any, cast, Optional
from transformers.tokenization_utils import PreTrainedTokenizer

# Add project root to path for module imports
project_root = Path(__file__).resolve().parent.parent.parent.parent
sys.path.append(str(project_root))

from src.models.base_model import BaseVisionModel

class LlavaMlxModel(BaseVisionModel):
    def __init__(self, model_name: str, config: Dict[str, Any]):
        super().__init__(model_name=model_name, config=config)
        
        # çµ±ä¸€é…ç½®éµåçš„ä½¿ç”¨
        self.model_id = (
            self.config.get("model_path") or 
            self.config.get("model_id") or 
            "mlx-community/llava-v1.6-mistral-7b-4bit"
        )
        
        if self.model_id == model_name:
            self.model_id = "mlx-community/llava-v1.6-mistral-7b-4bit"
        
        print(f"ðŸ”§ LLaVA MLX Model initialized with model_path: {self.model_id}")
        
        self.model = None
        self.processor = None
        self.loaded = False
        self.stats = {"requests": 0, "total_time": 0.0}
        
        # æ·»åŠ ç‹€æ…‹è¿½è¹¤ - åƒè€ƒæ¸¬è©¦æ¡†æž¶çš„å–®æ¬¡æŽ¨ç†æ¨¡å¼
        self.inference_count = 0
        self.max_inferences_before_reload = 1  # æ¯æ¬¡æŽ¨ç†å¾Œé‡è¼‰æ¨¡åž‹

    def load_model(self):
        """Load LLaVA MLX model using MLX-VLM framework (åƒè€ƒ vlm_tester.py çš„æˆåŠŸå¯¦ç¾)"""
        print(f"Loading LLaVA MLX model: {self.model_id}...")
        try:
            from mlx_vlm import load
            print(f"ðŸš€ Using MLX-VLM to load: {self.model_id}")
            
            # åƒè€ƒ vlm_tester.py çš„è¼‰å…¥æ–¹å¼
            self.model, self.processor = load(self.model_id)
            self.loaded = True
            print("âœ… LLaVA MLX model loaded successfully.")
            return True
        except ImportError as e:
            print(f"âŒ MLX-VLM not available: {e}")
            raise RuntimeError("MLX-VLM is not installed. Please run: pip install mlx-vlm")
        except Exception as e:
            print(f"âŒ Failed to load LLaVA MLX model: {e}")
            raise RuntimeError(f"Failed to load LLaVA MLX model from {self.model_id}: {e}")

    def predict(self, image: Any, prompt: str, options: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Predict method for compatibility with BaseVisionModel (åƒè€ƒ vlm_tester.py)"""
        try:
            # ç¢ºä¿è¼¸å…¥åœ–åƒæ˜¯ PIL Image
            if not isinstance(image, Image.Image):
                if hasattr(image, 'shape'):  # numpy array
                    image = Image.fromarray(image)
                else:
                    raise ValueError("Invalid image format")
            
            # ç²å– max_tokens åƒæ•¸
            max_tokens = 150  # é»˜èªå€¼
            if options and "max_tokens" in options:
                max_tokens = options["max_tokens"]
            
            # èª¿ç”¨ generate_response æ–¹æ³•
            response_text = self.generate_response(image, prompt, max_tokens)
            
            # ç¢ºä¿è¿”å›žæ­£ç¢ºçš„æ ¼å¼
            return {
                "success": True,
                "response": {"text": response_text}
            }
            
        except Exception as e:
            print(f"âŒ LLaVA MLX prediction error: {e}")
            return {
                "success": False,
                "error": str(e),
                "response": {"text": f"Error: {str(e)}"}
            }

    def generate_response(self, image: Image.Image, prompt: str, max_tokens: int = 100) -> str:
        """Generate response with automatic model reloading - åƒè€ƒæ¸¬è©¦æ¡†æž¶çš„æˆåŠŸæ–¹æ³•"""
        # æª¢æŸ¥æ˜¯å¦éœ€è¦é‡è¼‰æ¨¡åž‹ä¾†é¿å…ç‹€æ…‹ç´¯ç©
        if self.inference_count >= self.max_inferences_before_reload:
            print(f"ðŸ”„ Reloading model after {self.inference_count} inferences to prevent state issues")
            self.unload_model()
            if not self.load_model():
                return "Failed to reload model"
            self.inference_count = 0
        
        if not self.loaded or self.model is None or self.processor is None:
            if not self.load_model():
                return "Model failed to load"
        
        print(f"Generating response with LLaVA MLX (inference #{self.inference_count + 1})...")
        
        try:
            # åƒè€ƒæ¸¬è©¦æ¡†æž¶çš„åœ–åƒè™•ç†
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            # ä½¿ç”¨æ¸¬è©¦æ¡†æž¶é©—è­‰çš„å°ºå¯¸è™•ç†
            unified_image_size = 1024
            original_size = image.size
            if max(image.size) > unified_image_size:
                ratio = unified_image_size / max(image.size)
                new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))
                image = image.resize(new_size, Image.Resampling.LANCZOS)
                print(f"ðŸ”§ Resized image: {original_size} â†’ {new_size}")
            
            # ä½¿ç”¨å”¯ä¸€çš„è‡¨æ™‚æ–‡ä»¶åé¿å…è¡çª
            import uuid
            temp_image_path = f"temp_mlx_image_{uuid.uuid4().hex[:8]}.jpg"
            
            try:
                # ä¿å­˜åœ–åƒ - åƒè€ƒæ¸¬è©¦æ¡†æž¶
                image.save(temp_image_path, 'JPEG', quality=95, optimize=True)
                print(f"ðŸ’¾ Saved image to: {temp_image_path}")

                from mlx_vlm import generate
                
                # ä½¿ç”¨æ¸¬è©¦æ¡†æž¶é©—è­‰çš„ç”Ÿæˆåƒæ•¸
                response = generate(
                    model=self.model,
                    processor=self.processor,
                    prompt=prompt,
                    image=temp_image_path,
                    max_tokens=max_tokens,
                    verbose=False
                )

                # è™•ç†éŸ¿æ‡‰ - åƒè€ƒæ¸¬è©¦æ¡†æž¶
                if isinstance(response, tuple) and len(response) >= 1:
                    text_response = response[0] if response[0] else ""
                elif isinstance(response, list) and len(response) > 0:
                    text_response = response[0] if isinstance(response[0], str) else str(response[0])
                else:
                    text_response = str(response) if response else ""
                
                text_response = text_response.strip()
                if not text_response:
                    text_response = "No response generated"
                
                # å¢žåŠ æŽ¨ç†è¨ˆæ•¸
                self.inference_count += 1
                
                print(f"âœ… Generated response: {text_response[:100]}...")
                return text_response

            except Exception as e:
                print(f"âŒ MLX generation error: {e}")
                # ç™¼ç”ŸéŒ¯èª¤æ™‚ç«‹å³é‡è¼‰æ¨¡åž‹
                if "axis remapping" in str(e):
                    print("ðŸ”„ Axis remapping error detected - forcing model reload")
                    self.inference_count = self.max_inferences_before_reload
                return f"MLX inference failed: {str(e)}"
                
            finally:
                # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
                try:
                    if os.path.exists(temp_image_path):
                        os.remove(temp_image_path)
                        print(f"ðŸ—‘ï¸ Cleaned up temp file: {temp_image_path}")
                except Exception as cleanup_error:
                    print(f"âš ï¸ Warning: Could not clean up temp file: {cleanup_error}")

        except Exception as e:
            print(f"âŒ Error during LLaVA MLX inference: {e}")
            # ç™¼ç”ŸéŒ¯èª¤æ™‚æ¨™è¨˜éœ€è¦é‡è¼‰
            self.inference_count = self.max_inferences_before_reload
            return f"Inference failed: {str(e)}"

    def clear_cache(self):
        """Enhanced cache clearing with model state reset"""
        if self.model is None:
            return

        try:
            # æ›´ç©æ¥µçš„ç·©å­˜æ¸…ç†
            if hasattr(self.model, 'language_model') and hasattr(self.model.language_model, 'model'):
                layers = self.model.language_model.model.layers
                for layer in layers:
                    if hasattr(layer, "self_attn"):
                        # æ¸…ç†æ‰€æœ‰å¯èƒ½çš„ç·©å­˜å±¬æ€§
                        for cache_attr in ["cache", "kv_cache", "_cache", "past_key_values"]:
                            if hasattr(layer.self_attn, cache_attr):
                                delattr(layer.self_attn, cache_attr)
        except Exception as e:
            print(f"Cache clearing error: {e}")
        
        # å¼·åˆ¶åžƒåœ¾å›žæ”¶
        gc.collect()

    def unload_model(self) -> bool:
        """Enhanced model unloading"""
        try:
            # æ¸…ç†ç·©å­˜
            self.clear_cache()
            
            # åˆªé™¤æ¨¡åž‹å°è±¡
            if self.model is not None:
                del self.model
                self.model = None
            if self.processor is not None:
                del self.processor
                self.processor = None
            
            self.loaded = False
            self.inference_count = 0
            
            # å¼·åˆ¶åžƒåœ¾å›žæ”¶
            gc.collect()
            
            print("ðŸ—‘ï¸ Model unloaded successfully")
            return True
        except Exception as e:
            print(f"Error unloading LLaVA MLX model: {e}")
            return False

    def process_messages(self, messages):
        """Process messages in the format expected by the API"""
        prompt = "Describe the image."
        image_data_url = None

        user_content = messages[0].get("content", [])
        for item in user_content:
            if item.get("type") == "text":
                prompt = item.get("text", prompt)
            elif item.get("type") == "image_url":
                image_data_url = item["image_url"]["url"]

        if not image_data_url:
            raise ValueError("No image provided in the message content.")
        
        # Manually decode the base64 image data
        if image_data_url.startswith('data:image/'):
            base64_data = image_data_url.split(',')[1]
            image_bytes = base64.b64decode(base64_data)
            image = Image.open(BytesIO(image_bytes)).convert("RGB")
            return image, prompt
        else:
            raise ValueError("Unsupported image URL format. Please use base64 data URI.")

    def preprocess_image(self, image: Any) -> Any:
        """Preprocess image for LLaVA MLX model (unified preprocessing) - å¯¦ç¾æŠ½è±¡æ–¹æ³•"""
        # çµ±ä¸€åœ–åƒé è™•ç†ï¼ˆåƒè€ƒ vlm_tester.pyï¼‰
        if isinstance(image, Image.Image):
            original_size = image.size
            unified_image_size = 1024
            if max(image.size) > unified_image_size:
                ratio = unified_image_size / max(image.size)
                new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))
                image = image.resize(new_size, Image.Resampling.LANCZOS)
        return image
    
    def format_response(self, raw_response: Any) -> Dict[str, Any]:
        """Format response for API compatibility - å¯¦ç¾æŠ½è±¡æ–¹æ³•"""
        if isinstance(raw_response, dict):
            return raw_response
        else:
            return {"response": raw_response}

    def get_model_info(self) -> Dict[str, Any]:
        """Get model information"""
        return {
            "model_name": self.model_name,
            "model_id": self.model_id,
            "loaded": self.loaded,
            "framework": "MLX-VLM",
            "device": "Apple Silicon (M1/M2/M3)",
            "stats": self.stats
        }

if __name__ == '__main__':
    print("This script is not meant to be run directly. Use a runner script.")