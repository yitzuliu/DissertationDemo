# SmolVLM Model

**Fast and efficient Vision-Language Model implementation for the AI Manual Assistant**

## üöÄ Quick Start Commands

Complete system startup commands - execute in sequence to start the entire AI Manual Assistant system:

### 1. Environment Check
```bash
source ai_vision_env/bin/activate && python --version
# Expected output: Python 3.13.3
```

### 2. Start SmolVLM Model Server (Port 8080)
```bash
# Execute in first terminal
source ai_vision_env/bin/activate && cd src/models/smolvlm && python -m llama_cpp.server --model ./smolvlm-instruct-v0_2-q4_k_m.gguf --port 8080 --chat_format chatml-llava --clip_model_path ./smolvlm-instruct-v0_2-mmproj-f16.gguf
```

### 3. Start Backend Server (Port 8000)
```bash
# Execute in second terminal
source ai_vision_env/bin/activate && cd src/backend && python main.py
```

### 4. Start Frontend Server (Port 5500)
```bash
# Execute in third terminal
source ai_vision_env/bin/activate && cd src/frontend && python -m http.server 5500
```

### 5. Health Check
```bash
# Check SmolVLM server status
source ai_vision_env/bin/activate && sleep 10 && curl -s http://localhost:8080/health

# Check backend server status
curl -s http://localhost:8000/status

# Access frontend interface
open http://localhost:5500
```

### 6. Stop All Servers
```bash
# One-command stop all services
pkill -f "python main.py" && pkill -f "http.server 5500" && pkill -f "llama-server"
```

---

## üìñ Detailed Documentation

SmolVLM is a lightweight vision-language model optimized for real-time image analysis and instruction generation. This implementation uses `llama-server` with OpenAI API compatibility.

### System Architecture
- **SmolVLM Model Server (Port 8080)** - Handles vision-language model inference
- **Backend API Server (Port 8000)** - Handles image preprocessing and API proxy
- **Frontend Web Server (Port 5500)** - Provides user interface

### Main Features
- **Real-time image analysis** - Fast processing for live camera feeds
- **Step-by-step guidance** - Generating detailed instructions for manual tasks
- **Object detection and description** - Identifying and describing objects in images
- **Resource efficiency** - Optimized for edge deployment with minimal memory footprint

### Key Features
- üöÄ **High Performance**: Optimized inference with llama.cpp backend
- üîß **Easy Integration**: OpenAI-compatible API for seamless integration
- üíæ **Memory Efficient**: Quantized models for reduced memory usage
- üîÑ **Auto-scaling**: Automatic server management and health monitoring
- üìä **Real-time Processing**: Low latency for interactive applications

## üõ†Ô∏è Installation & Setup

### Prerequisites
- Python 3.8+
- llama.cpp with `llama-server` installed
- Sufficient system memory (4GB+ recommended)
- GPU with CUDA support (optional but recommended)

### Quick Setup
1. **Install llama.cpp**:
   ```bash
   # Download and build llama.cpp
   git clone https://github.com/ggerganov/llama.cpp.git
   cd llama.cpp
   make llama-server
   
   # Add to PATH or use absolute path
   export PATH=$PATH:/path/to/llama.cpp
   ```

2. **Verify Installation**:
   ```bash
   which llama-server
   llama-server --help
   ```

## üì° API Reference

### SmolVLMModel Class
Main interface for the SmolVLM model.

#### Constructor
```python
SmolVLMModel(model_name: str, config: Dict[str, Any])
```

**Parameters:**
- `model_name`: Model instance identifier
- `config`: Configuration dictionary

**Configuration Options:**
- `smolvlm_version`: Model variant (default: "ggml-org/SmolVLM-500M-Instruct-GGUF")
- `port`: Server port (default: 8080)
- `manage_server`: Auto-manage server lifecycle (default: True)
- `timeout`: Request timeout in seconds (default: 60)

#### Main Methods

##### `load_model() -> bool`
Initialize and start the model server.
**Returns:** True if successful, False otherwise

##### `predict(image, prompt, options=None) -> Dict[str, Any]`
Generate model predictions for image and text input.

**Parameters:**
- `image`: PIL Image or numpy array
- `prompt`: Text prompt for the model
- `options`: Optional parameters (temperature, max_tokens)

**Returns:** Dictionary with prediction results

##### `unload_model() -> bool`
Stop the model server and free resources.
**Returns:** True if successful, False otherwise

### HTTP API Endpoints

When the server is running, these endpoints are available:

#### Health Check
```http
GET http://localhost:8080/health
```

#### Chat Completion
```http
POST http://localhost:8080/v1/chat/completions
Content-Type: application/json

{
  "messages": [
    {
      "role": "user",
      "content": [
        {"type": "text", "text": "Describe this image"},
        {"type": "image_url", "image_url": {"url": "data:image/jpeg;base64,..."}}
      ]
    }
  ],
  "max_tokens": 512,
  "temperature": 0.0
}
```

## üéØ Use Cases

### Object Detection and Description
```python
result = model.predict(image, "List all objects in this image with their positions")
# Returns detailed object descriptions and locations
```

### Step-by-Step Instructions
```python
result = model.predict(image, "How do I repair this device? Provide step-by-step instructions")
# Returns detailed repair instructions based on visual analysis
```

### Safety Assessment
```python
result = model.predict(image, "Identify any safety hazards in this workspace")
# Returns safety warnings and recommendations
```

### Progress Monitoring
```python
result = model.predict(image, "What step of the assembly process is shown here?")
# Returns current progress and next steps
```

## ‚öôÔ∏è Configuration

### Model Variants
- **SmolVLM-500M-Instruct**: Fastest inference, good for real-time applications
- **SmolVLM-1.7B-Instruct**: Better accuracy, higher resource requirements

### Performance Tuning

#### GPU Configuration
```bash
# Use all GPU layers (fastest)
llama-server -hf ggml-org/SmolVLM-500M-Instruct-GGUF -ngl 99 --port 8080

# Partial GPU offload
llama-server -hf ggml-org/SmolVLM-500M-Instruct-GGUF -ngl 20 --port 8080

# CPU only
llama-server -hf ggml-org/SmolVLM-500M-Instruct-GGUF -ngl 0 --port 8080
```

#### Memory Optimization
```python
config = {
    "max_tokens": 256,  # Reduce output length
    "temperature": 0.0,  # More deterministic
    "timeout": 30  # Faster timeout
}
```

## üîß Troubleshooting

### Common Issues

#### 1. `llama-server` Not Found
```bash
# Check installation
which llama-server

# Install llama.cpp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp && make llama-server

# Add to PATH
export PATH=$PATH:/path/to/llama.cpp
```

#### 2. Out of Memory Error
```bash
# Reduce GPU layers
llama-server -hf ggml-org/SmolVLM-500M-Instruct-GGUF -ngl 10 --port 8080

# Use CPU only
llama-server -hf ggml-org/SmolVLM-500M-Instruct-GGUF -ngl 0 --port 8080
```

#### 3. Server Connection Failed
```python
# Check server status
import requests
response = requests.get("http://localhost:8080/health")
print(response.status_code)

# Wait for server startup
time.sleep(10)
```

#### 4. Slow Inference
- Use GPU acceleration (`-ngl 99`)
- Reduce `max_tokens` in requests
- Use smaller model variant
- Enable model quantization

### Performance Metrics

Typical performance on modern hardware:

| Hardware | Model Size | Inference Time | Memory Usage |
|----------|------------|----------------|--------------|
| RTX 4090 | 500M | ~200ms | 2GB VRAM |
| RTX 3080 | 500M | ~300ms | 2GB VRAM |
| CPU (16 cores) | 500M | ~2s | 4GB RAM |

## üìä Monitoring & Logging

### Enable Debug Logging
```python
import logging
logging.getLogger('src.models.smolvlm').setLevel(logging.DEBUG)
```

### Health Monitoring
```python
# Check server health
health = requests.get(f"http://localhost:{port}/health")
if health.status_code == 200:
    print("Server healthy")
```

### Performance Tracking
```python
# The model tracks processing time automatically
result = model.predict(image, prompt)
print(f"Processing time: {result.get('processing_time', 0):.2f}s")
```

## üìÑ Files Overview

- **`smolvlm_model.py`** - Main model implementation with BaseVisionModel interface
- **`run_smolvlm.py`** - Standalone server launcher script
- **`__init__.py`** - Module exports and initialization
- **`README.md`** - This documentation file
- **`LICENSE`** - License information

## üîó Integration

This SmolVLM implementation integrates with:

- **AI Manual Assistant Backend** - Via the BaseVisionModel interface
- **Configuration System** - Through model_configs/smolvlm.json
- **Image Processing Pipeline** - Using shared preprocessing utilities
- **API Gateway** - Through standardized response formats

## üìù License

See [LICENSE](LICENSE) file for details.

---

For more information about the AI Manual Assistant project, see the main [documentation](../../../docs/README.md).