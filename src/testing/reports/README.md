# Testing Reports Directory

This directory contains comprehensive analysis reports for Vision-Language Model (VLM) testing results.

## üìÅ Report Files

### **`vqa_analysis.md`** - VQA 2.0 Performance Analysis
- **Purpose:** Comprehensive analysis of VQA 2.0 test results
- **Content:** Model performance rankings, question-type analysis, critical issues
- **Data Source:** `vqa2_results_coco_20250729_131258.json`
- **Key Findings:** Moondream2 best overall (65.0%), SmolVLM fastest (0.39s), LLaVA-MLX critical issues

### **`model_performance_guide.md`** - Production Performance Guide
- **Purpose:** Production recommendations and model selection guide
- **Content:** Performance overview, loading methods, production recommendations
- **Data Source:** Latest VQA 2.0, context understanding, and performance benchmarking results
- **Key Findings:** Clear guidance for different use cases (accuracy vs speed vs balanced)

### **`context_understanding_analysis.md`** - Context Understanding Analysis
- **Purpose:** Analysis of multi-turn conversation capabilities
- **Content:** Context understanding test results, failure analysis, implications
- **Data Source:** `context_understanding_test_results_20250728_203410.json`
- **Key Findings:** 0% context understanding across all models, critical production implications

## üìä Key Findings Summary

### **Performance Rankings (Latest Results)**
1. **ü•á Moondream2:** 65.0% simple, 62.5% VQA, 8.35s inference - Best Overall
2. **ü•à SmolVLM2-MLX:** 55.0% simple, 52.5% VQA, 8.41s inference - Balanced
3. **‚ö° SmolVLM-GGUF:** 35.0% simple, 36.0% VQA, 0.39s inference - Fastest
4. **ü•â Phi-3.5-MLX:** 35.0% simple, 35.0% VQA, 5.29s inference - Fast
5. **‚ö†Ô∏è LLaVA-MLX:** 20.0% simple, 21.0% VQA, 24.15s inference - Critical Issues

### **Critical Limitations**
- **Context Understanding:** 0% capability across all models
- **Text Reading:** Universal failure on text in images
- **Counting Tasks:** Poor performance (0-50% accuracy)
- **LLaVA-MLX:** Critical performance crisis (24.15s inference, 20.0% accuracy)

### **Production Recommendations**
- **High Accuracy:** Use Moondream2 (65.0% accuracy)
- **Real-Time:** Use SmolVLM-GGUF (0.39s inference)
- **Balanced:** Use SmolVLM2-MLX (55.0% accuracy, 8.41s)
- **Avoid:** LLaVA-MLX for any production use

## üîÑ Report Updates

### **Latest Update:** 2025-07-29 13:12:58
- Updated all reports with latest VQA 2.0 results
- Standardized file naming convention (lowercase with underscores)
- Removed unnecessary meta-documentation files
- Consolidated production recommendations

### **Data Sources**
- **VQA 2.0:** `vqa2_results_coco_20250729_131258.json`
- **Context Understanding:** `context_understanding_test_results_20250728_203410.json`
- **Performance Benchmarking:** `test_results_20250728_190743.json`

### **Test Environment**
- **Hardware:** MacBook Air M3 (16GB RAM, MPS available)
- **Framework:** VQA 2.0 Standard Evaluation + Custom Context Testing
- **Dataset:** COCO val2014 (20 questions) + Custom test images

---

**For detailed analysis, see individual report files.**  
**All metrics verified against source JSON files.**  
**Documentation suitable for academic and production use.**