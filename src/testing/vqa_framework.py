#!/usr/bin/env python3
"""
VQA 2.0 æ ¸å¿ƒæ¡†æ¶ - æ•´åˆç‰ˆæœ¬
æ•´åˆäº†VQAæ¸¬è©¦å™¨ã€å·¥å…·å‡½æ•¸ã€åœ–åƒè™•ç†ç­‰æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½

Author: AI Manual Assistant Team
Date: 2025-01-27
"""

import os
import sys
import json
import time
import requests
import zipfile
import string
import re
import random
import traceback
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
import torch
import numpy as np
from PIL import Image
from tqdm import tqdm

# Import existing VLM infrastructure
from vlm_tester import VLMModelLoader, clear_model_memory, get_memory_usage

class VQAFramework:
    """VQA 2.0 çµ±ä¸€æ¡†æ¶ - æ•´åˆæ‰€æœ‰åŠŸèƒ½"""
    
    def __init__(self, data_dir: str = None):
        """åˆå§‹åŒ–VQAæ¡†æ¶
        
        Args:
            data_dir: VQAæ•¸æ“šç›®éŒ„è·¯å¾‘
        """
        self.data_dir = Path(data_dir) if data_dir else Path(__file__).parent / "testing_material" / "vqa2"
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        self.images_dir = self.data_dir / "images"
        self.images_dir.mkdir(exist_ok=True)
        
        self.results_dir = Path(__file__).parent / "results"
        self.results_dir.mkdir(exist_ok=True)
        
        # VQA 2.0 æ•¸æ“šé›†URLs
        self.vqa2_urls = {
            "val_questions": "https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Val_mscoco.zip",
            "val_annotations": "https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Val_mscoco.zip",
        }
        
        # æ”¯æŒçš„æ¨¡å‹é…ç½®
        self.models_config = {
            "smolvlm_instruct": {
                "loader": VLMModelLoader.load_smolvlm_instruct,
                "model_id": "HuggingFaceTB/SmolVLM-500M-Instruct"
            },
            "smolvlm_v2_instruct": {
                "loader": VLMModelLoader.load_smolvlm2_video,
                "model_id": "HuggingFaceTB/SmolVLM2-500M-Video-Instruct"
            },
            "moondream2": {
                "loader": VLMModelLoader.load_moondream2,
                "model_id": "vikhyatk/moondream2"
            },
            "llava_mlx": {
                "loader": VLMModelLoader.load_llava_mlx,
                "model_id": "mlx-community/llava-v1.6-mistral-7b-4bit"
            },
            "phi35_vision": {
                "loader": VLMModelLoader.load_phi3_vision,
                "model_id": "lokinfey/Phi-3.5-vision-mlx-int4"
            }
        }
        
        # ç”Ÿæˆåƒæ•¸
        self.generation_params = {
            "max_new_tokens": 50,
            "do_sample": False
        }
        
        # åœ–åƒç·©å­˜
        self.image_cache = {}
        
        print(f"ğŸ§ª VQA 2.0 Framework Initialized")
        print(f"ğŸ“ Data directory: {self.data_dir}")
    
    def download_vqa_data(self):
        """ä¸‹è¼‰VQA 2.0æ•¸æ“šé›†"""
        print("ğŸ“¥ Downloading VQA 2.0 Dataset...")
        
        for component, url in self.vqa2_urls.items():
            zip_file = self.data_dir / f"{component}.zip"
            
            if not zip_file.exists():
                print(f"ğŸ“¥ Downloading {component}...")
                
                response = requests.get(url, stream=True)
                total_size = int(response.headers.get('content-length', 0))
                
                with open(zip_file, 'wb') as f, tqdm(
                    desc=component,
                    total=total_size,
                    unit='B',
                    unit_scale=True,
                    unit_divisor=1024,
                ) as pbar:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                            pbar.update(len(chunk))
                
                print(f"âœ… Downloaded: {zip_file}")
                
                # è§£å£“æ–‡ä»¶
                print(f"ğŸ“‚ Extracting {zip_file.name}...")
                with zipfile.ZipFile(zip_file, 'r') as zip_ref:
                    zip_ref.extractall(self.data_dir)
                print(f"âœ… Extracted: {zip_file.name}")
            else:
                print(f"âœ… {component} already downloaded")
        
        print("âœ… VQA 2.0 Dataset ready")
    
    def load_sample_data(self, sample_size: int = 20) -> Tuple[List[Dict], Dict]:
        """åŠ è¼‰çœŸå¯¦VQA 2.0æ•¸æ“šçš„æ¨£æœ¬"""
        # é¦–å…ˆç¢ºä¿çœŸå¯¦æ•¸æ“šå·²ä¸‹è¼‰
        try:
            questions, annotations = self.load_real_data(sample_size)
            print(f"âœ… Using real VQA 2.0 data: {len(questions)} questions")
            
            # ä¸‹è¼‰å°æ‡‰çš„COCOåœ–åƒ
            self._ensure_coco_images_for_questions(questions)
            
            return questions, annotations
            
        except Exception as e:
            print(f"âš ï¸ Could not load real VQA data: {e}")
            print("ğŸ“¥ Attempting to download VQA 2.0 dataset...")
            
            # å˜—è©¦ä¸‹è¼‰VQAæ•¸æ“š
            self.download_vqa_data()
            
            # é‡æ–°å˜—è©¦åŠ è¼‰
            try:
                questions, annotations = self.load_real_data(sample_size)
                print(f"âœ… Using real VQA 2.0 data: {len(questions)} questions")
                
                # ä¸‹è¼‰å°æ‡‰çš„COCOåœ–åƒ
                self._ensure_coco_images_for_questions(questions)
                
                return questions, annotations
                
            except Exception as e2:
                print(f"âŒ Still failed to load real data: {e2}")
                print("ï¿½ Falling back to sample data...")
                
                # æœ€å¾Œå›é€€åˆ°æ¨£æœ¬æ•¸æ“š
                return self._create_sample_data(sample_size)
    
    def load_real_data(self, sample_size: int = 20) -> Tuple[List[Dict], Dict]:
        """åŠ è¼‰çœŸå¯¦VQA 2.0æ•¸æ“š"""
        print(f"ğŸ“– Loading real VQA 2.0 data...")
        
        # åŠ è¼‰å•é¡Œ
        questions_file = self.data_dir / "v2_OpenEnded_mscoco_val2014_questions.json"
        if not questions_file.exists():
            raise FileNotFoundError(f"Questions file not found: {questions_file}")
            
        with open(questions_file, 'r') as f:
            questions_data = json.load(f)
            
        questions = questions_data['questions']
        
        # åŠ è¼‰æ¨™è¨»
        annotations_file = self.data_dir / "v2_mscoco_val2014_annotations.json"
        annotations_dict = {}
        
        if annotations_file.exists():
            with open(annotations_file, 'r') as f:
                annotations_data = json.load(f)
            annotations_dict = {ann['question_id']: ann for ann in annotations_data['annotations']}
        else:
            print(f"âš ï¸ Annotations file not found: {annotations_file}")
        
        # æ™ºèƒ½æ¡æ¨£ï¼šç¢ºä¿å•é¡Œæœ‰å°æ‡‰çš„æ¨™è¨»
        if sample_size and sample_size < len(questions):
            # ç¯©é¸æœ‰æ¨™è¨»çš„å•é¡Œ
            questions_with_annotations = [
                q for q in questions 
                if q['question_id'] in annotations_dict
            ]
            
            if len(questions_with_annotations) < sample_size:
                print(f"âš ï¸ Only {len(questions_with_annotations)} questions have annotations, using all")
                sample_size = len(questions_with_annotations)
            
            # éš¨æ©Ÿæ¡æ¨£
            import random
            random.seed(42)  # ç¢ºä¿çµæœå¯é‡ç¾
            questions = random.sample(questions_with_annotations, sample_size)
            print(f"ğŸ“ Sampled {sample_size} questions from {len(questions_with_annotations)} annotated questions")
        
        return questions, annotations_dict
    
    def _create_sample_data(self, sample_size: int) -> Tuple[List[Dict], Dict]:
        """å‰µå»ºæ¨£æœ¬æ•¸æ“š - ä½¿ç”¨çœŸå¯¦COCOåœ–åƒID"""
        # ç°¡å–®çš„æ¨£æœ¬å•é¡Œ
        sample_questions_template = [
            ("What color is the car?", "red"),
            ("How many people are in the image?", "2"),
            ("Is this person wearing a hat?", "yes"),
            ("What is the weather like?", "sunny"),
            ("Where is this photo taken?", "park"),
        ]
        
        # çœŸå¯¦COCO val2014 åœ–åƒIDï¼ˆå‰20å€‹ï¼‰
        coco_image_ids = [
            139, 285, 632, 724, 776, 785, 802, 872, 885, 1000,
            1268, 1296, 1353, 1584, 1818, 2006, 2149, 2153, 2157, 2261
        ]
        
        questions = []
        annotations = {}
        
        for i in range(sample_size):
            template_idx = i % len(sample_questions_template)
            question_text, answer = sample_questions_template[template_idx]
            
            question_id = 1000 + i
            image_id = coco_image_ids[i % len(coco_image_ids)]  # ä½¿ç”¨çœŸå¯¦COCO ID
            
            question = {
                "question_id": question_id,
                "image_id": image_id,
                "question": question_text
            }
            questions.append(question)
            
            # å‰µå»ºæ¨™è¨»ï¼ˆæ¨¡æ“¬10å€‹äººå·¥æ¨™è¨»è€…ï¼‰
            annotation = {
                "question_id": question_id,
                "question_type": "other",
                "multiple_choice_answer": answer,
                "answers": [{"answer": answer, "answer_confidence": "yes"} for _ in range(10)]
            }
            annotations[question_id] = annotation
        
        # ä¿å­˜æ¨£æœ¬æ•¸æ“š
        questions_data = {"questions": questions}
        annotations_data = {"annotations": list(annotations.values())}
        
        with open(self.data_dir / "val_questions_sample.json", 'w') as f:
            json.dump(questions_data, f, indent=2)
            
        with open(self.data_dir / "val_annotations_sample.json", 'w') as f:
            json.dump(annotations_data, f, indent=2)
        
        print(f"âœ… Created sample data: {len(questions)} questions with real COCO image IDs")
        return questions, annotations
    
    def _ensure_sample_images(self, sample_size: int):
        """ç¢ºä¿COCOæ¨£æœ¬åœ–åƒå­˜åœ¨"""
        sample_dir = self.images_dir / "val2014_sample"
        sample_dir.mkdir(exist_ok=True)
        
        # æª¢æŸ¥æ˜¯å¦å·²æœ‰è¶³å¤ çš„åœ–åƒ
        existing_images = list(sample_dir.glob("*.jpg"))
        if len(existing_images) >= min(sample_size, 20):  # æœ€å¤š20å¼µä¸åŒåœ–åƒ
            return
        
        print(f"ğŸ“¥ Downloading COCO val2014 sample images (first 20 images)...")
        
        # COCO val2014 å‰20å¼µåœ–åƒçš„çœŸå¯¦ID
        coco_image_ids = [
            "000000000139", "000000000285", "000000000632", "000000000724", "000000000776",
            "000000000785", "000000000802", "000000000872", "000000000885", "000000001000",
            "000000001268", "000000001296", "000000001353", "000000001584", "000000001818",
            "000000002006", "000000002149", "000000002153", "000000002157", "000000002261"
        ]
        
        # COCO val2014 ä¸‹è¼‰åŸºç¤URL
        base_url = "http://images.cocodataset.org/val2014"
        
        downloaded_count = 0
        for i, image_id in enumerate(coco_image_ids[:min(sample_size, 20)]):
            image_filename = f"COCO_val2014_{image_id}.jpg"
            image_path = sample_dir / image_filename
            
            if not image_path.exists():
                try:
                    image_url = f"{base_url}/{image_filename}"
                    print(f"ğŸ“¥ Downloading {image_filename}...")
                    
                    response = requests.get(image_url, stream=True)
                    if response.status_code == 200:
                        with open(image_path, 'wb') as f:
                            for chunk in response.iter_content(chunk_size=8192):
                                f.write(chunk)
                        downloaded_count += 1
                        print(f"âœ… Downloaded: {image_filename}")
                    else:
                        print(f"âŒ Failed to download {image_filename}: HTTP {response.status_code}")
                        
                except Exception as e:
                    print(f"âš ï¸ Error downloading {image_filename}: {e}")
                    # å¦‚æœä¸‹è¼‰å¤±æ•—ï¼Œå‰µå»ºä½”ä½ç¬¦åœ–åƒ
                    placeholder_image = Image.new('RGB', (640, 480), (128, 128, 128))
                    placeholder_image.save(image_path, 'JPEG')
                    print(f"ğŸ”„ Created placeholder for {image_filename}")
        
        print(f"âœ… COCO sample setup complete: {downloaded_count} downloaded, {len(list(sample_dir.glob('*.jpg')))} total images")
    
    def _ensure_coco_images_for_questions(self, questions: List[Dict]):
        """ç‚ºå•é¡Œåˆ—è¡¨ä¸‹è¼‰å°æ‡‰çš„COCOåœ–åƒ"""
        sample_dir = self.images_dir / "val2014_sample"
        sample_dir.mkdir(exist_ok=True)
        
        # æ”¶é›†æ‰€æœ‰éœ€è¦çš„åœ–åƒID
        image_ids = set()
        for question in questions:
            image_ids.add(question['image_id'])
        
        print(f"ğŸ“¥ Ensuring COCO images for {len(image_ids)} unique images...")
        
        # COCO val2014 ä¸‹è¼‰åŸºç¤URL
        base_url = "http://images.cocodataset.org/val2014"
        
        downloaded_count = 0
        existing_count = 0
        
        for image_id in image_ids:
            image_filename = f"COCO_val2014_{image_id:012d}.jpg"
            image_path = sample_dir / image_filename
            
            if image_path.exists():
                existing_count += 1
                continue
                
            try:
                image_url = f"{base_url}/{image_filename}"
                print(f"ğŸ“¥ Downloading {image_filename}...")
                
                response = requests.get(image_url, stream=True, timeout=10)
                if response.status_code == 200:
                    with open(image_path, 'wb') as f:
                        for chunk in response.iter_content(chunk_size=8192):
                            f.write(chunk)
                    downloaded_count += 1
                    print(f"âœ… Downloaded: {image_filename}")
                else:
                    print(f"âŒ Failed to download {image_filename}: HTTP {response.status_code}")
                    
            except Exception as e:
                print(f"âš ï¸ Error downloading {image_filename}: {e}")
                # å¦‚æœä¸‹è¼‰å¤±æ•—ï¼Œå‰µå»ºä½”ä½ç¬¦åœ–åƒ
                try:
                    placeholder_image = Image.new('RGB', (640, 480), (128, 128, 128))
                    placeholder_image.save(image_path, 'JPEG')
                    print(f"ğŸ”„ Created placeholder for {image_filename}")
                except Exception as e2:
                    print(f"âŒ Failed to create placeholder: {e2}")
        
        print(f"âœ… COCO images ready: {existing_count} existing, {downloaded_count} downloaded")
    
    def check_image_availability(self, questions: List[Dict]) -> Dict:
        """æª¢æŸ¥åœ–åƒå¯ç”¨æ€§"""
        available_count = 0
        total_count = len(questions)
        
        for question in questions:
            image_id = question['image_id']
            if self._load_image(image_id) is not None:
                available_count += 1
        
        return {
            'available': available_count,
            'total': total_count,
            'rate': available_count / total_count if total_count > 0 else 0
        }
    
    def _load_image(self, image_id: int, split: str = "val2014") -> Optional[Image.Image]:
        """åŠ è¼‰åœ–åƒ"""
        # æª¢æŸ¥ç·©å­˜
        cache_key = f"{split}_{image_id}"
        if cache_key in self.image_cache:
            return self.image_cache[cache_key]
        
        # å˜—è©¦ä¸åŒçš„åœ–åƒè·¯å¾‘æ ¼å¼
        possible_paths = [
            # çœŸå¯¦COCOæ ¼å¼
            self.images_dir / f"{split}_sample" / f"COCO_{split}_{image_id:012d}.jpg",
            self.images_dir / f"{split}" / f"COCO_{split}_{image_id:012d}.jpg",
            # å‚™ç”¨æ ¼å¼
            self.images_dir / f"{split}_sample" / f"COCO_{split}_{image_id:06d}.jpg",
        ]
        
        for image_path in possible_paths:
            if image_path.exists():
                try:
                    image = Image.open(image_path).convert('RGB')
                    # ç·©å­˜åœ–åƒï¼ˆé™åˆ¶ç·©å­˜å¤§å°ï¼‰
                    if len(self.image_cache) < 100:
                        self.image_cache[cache_key] = image
                    return image
                except Exception as e:
                    print(f"âš ï¸ Error loading image {image_path}: {e}")
                    continue
        
        return None
    
    def evaluate_model(self, model_name: str, questions: List[Dict], 
                      annotations: Dict, max_questions: int = None,
                      verbose: bool = False) -> Dict:
        """è©•ä¼°æ¨¡å‹"""
        print(f"ğŸ¤– Evaluating {model_name} on VQA 2.0...")
        
        if model_name not in self.models_config:
            return {"error": f"Unknown model: {model_name}"}
        
        # é™åˆ¶å•é¡Œæ•¸é‡
        if max_questions:
            questions = questions[:max_questions]
        
        try:
            # åŠ è¼‰æ¨¡å‹
            print("ğŸ“¥ Loading model...")
            model_loader = self.models_config[model_name]["loader"]
            load_start = time.time()
            model, processor = model_loader()
            load_time = time.time() - load_start
            print(f"âœ… Model loaded in {load_time:.2f}s")
            
            # è¨˜éŒ„è¨˜æ†¶é«”ä½¿ç”¨
            memory_info = get_memory_usage()
            print(f"ğŸ’¾ Memory usage: {memory_info}")
            
            # è©•ä¼°
            print(f"ğŸ“Š Evaluating {len(questions)} questions...")
            results = self._evaluate_questions(
                model, processor, model_name, questions, 
                annotations, verbose
            )
            
            # æ¸…ç†æ¨¡å‹è¨˜æ†¶é«”
            print("æ¸…ç†æ¨¡å‹è¨˜æ†¶é«”...")
            clear_model_memory(model, processor)
            
            return results
            
        except Exception as e:
            print(f"âŒ Model evaluation failed: {e}")
            if verbose:
                import traceback
                traceback.print_exc()
            return {"error": str(e)}
    
    def _evaluate_questions(self, model, processor, model_name: str,
                           questions: List[Dict], annotations: Dict,
                           verbose: bool = False) -> Dict:
        """è©•ä¼°å•é¡Œåˆ—è¡¨"""
        correct_answers = 0
        total_vqa_accuracy = 0.0
        total_inference_time = 0.0
        question_results = []
        
        for i, question in enumerate(questions):
            if i % 10 == 0:
                print(f"Progress: {i}/{len(questions)}")
            
            question_id = question['question_id']
            question_text = question['question']
            image_id = question['image_id']
            
            # ç²å–æ¨¡å‹ç­”æ¡ˆ
            inference_start = time.time()
            try:
                model_answer = self._get_model_answer(
                    model, processor, model_name, question_text, image_id
                )
            except Exception as e:
                if verbose:
                    print(f"âš ï¸ Error getting answer for question {question_id}: {e}")
                model_answer = "error"
            
            inference_time = time.time() - inference_start
            total_inference_time += inference_time
            
            # è©•ä¼°ç­”æ¡ˆ
            if question_id in annotations:
                annotation = annotations[question_id]
                gt_answer = annotation['multiple_choice_answer']
                gt_answers = [ans['answer'] for ans in annotation['answers']]
                
                # ç°¡å–®æº–ç¢ºåº¦ - æª¢æŸ¥æ¨™æº–ç­”æ¡ˆæ˜¯å¦åœ¨æ¨¡å‹å›ç­”ä¸­
                model_answer_lower = self._preprocess_answer(model_answer)
                gt_answer_lower = self._preprocess_answer(gt_answer)
                is_correct = gt_answer_lower in model_answer_lower
                if is_correct:
                    correct_answers += 1
                
                # VQAæº–ç¢ºåº¦
                vqa_accuracy = self._calculate_vqa_accuracy(model_answer, gt_answers)
                total_vqa_accuracy += vqa_accuracy
                
                # ç”Ÿæˆå°æ‡‰çš„åœ–åƒæ–‡ä»¶å
                image_filename = f"COCO_val2014_{image_id:012d}.jpg"
                
                question_results.append({
                    'question_id': question_id,
                    'image_id': image_id,
                    'image_filename': image_filename,
                    'question': question_text,
                    'model_answer': model_answer,
                    'ground_truth': gt_answer,
                    'is_correct': is_correct,
                    'vqa_accuracy': vqa_accuracy,
                    'inference_time': inference_time
                })
            
        # è¨ˆç®—æœ€çµ‚çµæœ
        total_questions = len(questions)
        accuracy = correct_answers / total_questions if total_questions > 0 else 0
        avg_vqa_accuracy = total_vqa_accuracy / total_questions if total_questions > 0 else 0
        avg_inference_time = total_inference_time / total_questions if total_questions > 0 else 0
        
        print(f"âœ… Evaluation completed:")
        print(f"   ğŸ“Š Questions evaluated: {total_questions}")
        print(f"   âœ… Correct answers: {correct_answers}")
        print(f"   ğŸ¯ Accuracy: {accuracy:.3f}")
        print(f"   â±ï¸ Avg inference time: {avg_inference_time:.2f}s")
        
        return {
            'accuracy': accuracy,
            'vqa_accuracy': avg_vqa_accuracy,
            'correct': correct_answers,
            'total': total_questions,
            'avg_time': avg_inference_time,
            'question_results': question_results
        }
    
    def _get_model_answer(self, model, processor, model_name: str, 
                         question: str, image_id: int) -> str:
        """ç²å–æ¨¡å‹ç­”æ¡ˆ"""
        # åŠ è¼‰åœ–åƒ
        image = self._load_image(image_id)
        if image is None:
            return "no image available"
        
        try:
            if "smolvlm" in model_name.lower():
                # SmolVLMè™•ç† - ä½¿ç”¨æ­£ç¢ºçš„æ ¼å¼
                messages = [
                    {
                        "role": "user", 
                        "content": [
                            {"type": "image", "image": image},
                            {"type": "text", "text": question}
                        ]
                    }
                ]
                input_text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
                inputs = processor(text=input_text, images=image, return_tensors="pt")
                
                with torch.no_grad():
                    generated_ids = model.generate(**inputs, **self.generation_params)
                    response = processor.decode(generated_ids[0], skip_special_tokens=True)
                
                # æå–ç­”æ¡ˆ - ç§»é™¤è¼¸å…¥æ–‡æœ¬
                answer = response.replace(input_text, "").strip()
                return self._extract_answer(answer, model_name, question)
                
            elif "moondream" in model_name.lower():
                # Moondreamè™•ç†  
                answer = model.answer_question(image, question, processor)
                return answer
                
            else:
                return "model not supported"
                
        except Exception as e:
            return f"error: {str(e)}"
    
    def _extract_answer(self, response: str, model_name: str, question: str) -> str:
        """å¾æ¨¡å‹éŸ¿æ‡‰ä¸­æå–ç­”æ¡ˆ - ç°¡åŒ–ç‰ˆæœ¬ï¼Œç›´æ¥è¿”å›åŸå§‹å›ç­”"""
        if not response:
            return "no response"
        
        # ç°¡å–®æ¸…ç†ï¼Œç›´æ¥è¿”å›åŸå§‹å›ç­”
        return response.strip()
    
    def _preprocess_answer(self, answer: str) -> str:
        """é è™•ç†ç­”æ¡ˆ - ç°¡åŒ–ç‰ˆæœ¬"""
        if not answer:
            return ""
        
        # è½‰å°å¯«ï¼Œä¿æŒç°¡å–®
        return answer.lower().strip()
    
    def _calculate_vqa_accuracy(self, prediction: str, ground_truth_answers: List[str]) -> float:
        """è¨ˆç®—VQAæº–ç¢ºåº¦ - æª¢æŸ¥ä»»ä½•æ¨™æº–ç­”æ¡ˆæ˜¯å¦åœ¨é æ¸¬ä¸­"""
        if not prediction or not ground_truth_answers:
            return 0.0
        
        prediction_lower = self._preprocess_answer(prediction)
        
        # è¨ˆç®—æœ‰å¤šå°‘å€‹æ¨™æº–ç­”æ¡ˆåœ¨é æ¸¬ä¸­å‡ºç¾
        matching_count = 0
        for gt_answer in ground_truth_answers:
            gt_answer_lower = self._preprocess_answer(gt_answer)
            if gt_answer_lower in prediction_lower:
                matching_count += 1
        
        # VQAæº–ç¢ºåº¦ï¼šmin(åŒ¹é…æ•¸/3, 1.0)
        accuracy = min(matching_count / 3.0, 1.0)
        return accuracy
    
    def save_results(self, results: Dict, test_mode: str, num_questions: int) -> Path:
        """ä¿å­˜æ¸¬è©¦çµæœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = self.results_dir / f"vqa2_results_{test_mode}_{timestamp}.json"
        
        # æº–å‚™ä¿å­˜çš„æ•¸æ“š
        save_data = {
            "test_metadata": {
                "test_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "test_mode": test_mode,
                "num_questions": num_questions,
                "framework_version": "unified_v1.1",
                "image_reference_note": "æ¯å€‹å•é¡Œçš„ image_id å°æ‡‰ image_filenameï¼Œåœ–åƒæ–‡ä»¶ä½æ–¼ testing_material/vqa2/images/val2014_sample/ ç›®éŒ„"
            },
            "results": results
        }
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, indent=2, ensure_ascii=False)
        
        return results_file
