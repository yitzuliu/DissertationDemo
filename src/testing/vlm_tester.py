#!/usr/bin/env python3
"""
VLM æ¨¡å‹æ¸¬è©¦ç¨‹å¼
MacBook Air M3 (16GB) å„ªåŒ–ç‰ˆæœ¬ - é€ä¸€è¼‰å…¥æ¨¡å‹é¿å…è¨˜æ†¶é«”æº¢å‡º
"""

import os
import sys
import time
import json
import gc
import signal
import threading
import psutil
from datetime import datetime
from pathlib import Path
from PIL import Image
import torch
from transformers import AutoProcessor, AutoModelForVision2Seq, AutoModelForCausalLM, AutoModelForImageTextToText
from transformers.pipelines import pipeline

# è¨˜æ†¶é«”ç›£æ§ [[memory:2405482]]
def activate_virtual_env():
    """ç¢ºä¿è™›æ“¬ç’°å¢ƒå·²å•Ÿå‹•"""
    print("è«‹ç¢ºä¿å·²å•Ÿå‹•è™›æ“¬ç’°å¢ƒ: source ai_vision_env/bin/activate")

def get_memory_usage():
    """ç²å–ç•¶å‰è¨˜æ†¶é«”ä½¿ç”¨é‡ï¼ˆGBï¼‰"""
    process = psutil.Process()
    memory_info = process.memory_info()
    return memory_info.rss / (1024 ** 3)  # è½‰æ›ç‚º GB

def clear_model_memory(model, processor):
    """æ¸…ç†æ¨¡å‹è¨˜æ†¶é«”"""
    print("æ¸…ç†æ¨¡å‹è¨˜æ†¶é«”...")
    del model, processor
    gc.collect()
    if torch.backends.mps.is_available():
        torch.mps.empty_cache()
    time.sleep(2)  # è®“ç³»çµ±æœ‰æ™‚é–“æ¸…ç†è¨˜æ†¶é«”

class TimeoutError(Exception):
    """è¶…æ™‚éŒ¯èª¤"""
    pass

def run_with_timeout(func, timeout_seconds=120):
    """åœ¨æŒ‡å®šæ™‚é–“å…§åŸ·è¡Œå‡½æ•¸ï¼Œè¶…æ™‚å‰‡æ‹‹å‡ºç•°å¸¸"""
    result = []
    exception = []
    
    def target():
        try:
            result.append(func())
        except Exception as e:
            exception.append(e)
    
    thread = threading.Thread(target=target)
    thread.daemon = True
    thread.start()
    thread.join(timeout_seconds)
    
    if thread.is_alive():
        # è¶…æ™‚äº†ï¼Œä½† Python ç„¡æ³•å¼·åˆ¶çµ‚æ­¢ç·šç¨‹ï¼Œåªèƒ½æ‹‹å‡ºç•°å¸¸
        raise TimeoutError(f"æ¨ç†è¶…æ™‚ï¼ˆ{timeout_seconds}ç§’ï¼‰")
    
    if exception:
        raise exception[0]
    
    return result[0] if result else None

# æ¨¡å‹è¼‰å…¥å™¨é¡
class VLMModelLoader:
    """VLM æ¨¡å‹è¼‰å…¥å™¨ - æ ¹æ“š active_model.md å¯¦ç¾"""
    
    @staticmethod
    def load_smolvlm2_video(model_id="mlx-community/SmolVLM2-500M-Video-Instruct-mlx"):
        """è¼‰å…¥ SmolVLM2-500M-Video-Instruct (å„ªå…ˆä½¿ç”¨ MLX ç‰ˆæœ¬)"""
        print(f"è¼‰å…¥ SmolVLM2-500M-Video-Instruct (å„ªå…ˆä½¿ç”¨ MLX ç‰ˆæœ¬)...")
        
        try:
            # é¦–å…ˆå˜—è©¦ä½¿ç”¨ MLX-VLM æ¡†æ¶ï¼ˆèˆ‡ vlm_context_tester.py ç›¸åŒçš„æ–¹æ³•ï¼‰
            from mlx_vlm import load
            print("æ­£åœ¨è¼‰å…¥ MLX-VLM å„ªåŒ–çš„ SmolVLM2 æ¨¡å‹...")
            model, processor = load(model_id)
            print("MLX-VLM SmolVLM2 è¼‰å…¥æˆåŠŸ!")
            
            # æ¨™è¨˜ç‚º MLX æ¨¡å‹ï¼Œä½¿ç”¨ç‰¹æ®Šçš„æ¨ç†æ–¹å¼
            model._is_mlx_model = True
            
            return model, processor
            
        except ImportError as e:
            print("MLX-VLM æœªå®‰è£ï¼Œä½¿ç”¨åŸå§‹ SmolVLM2 æ¨¡å‹...")
            print("è«‹é‹è¡Œ: pip install mlx-vlm")
            # å›é€€åˆ°åŸå§‹ SmolVLM2 æ¨¡å‹
            fallback_model_id = "HuggingFaceTB/SmolVLM2-500M-Video-Instruct"
            processor = AutoProcessor.from_pretrained(fallback_model_id)
            model = AutoModelForImageTextToText.from_pretrained(fallback_model_id)
            return model, processor
            
        except Exception as e:
            print(f"MLX-VLM è¼‰å…¥å¤±æ•—: {str(e)}")
            print("ä½¿ç”¨åŸå§‹ SmolVLM2 æ¨¡å‹ä½œç‚ºå›é€€...")
            # å›é€€åˆ°åŸå§‹ SmolVLM2 æ¨¡å‹
            fallback_model_id = "HuggingFaceTB/SmolVLM2-500M-Video-Instruct"
            processor = AutoProcessor.from_pretrained(fallback_model_id)
            model = AutoModelForImageTextToText.from_pretrained(fallback_model_id)
            return model, processor
    
    @staticmethod
    def load_smolvlm2_video_mlx(model_id="mlx-community/SmolVLM2-500M-Video-Instruct-mlx"):
        """è¼‰å…¥ MLX å„ªåŒ–çš„ SmolVLM2-500M-Video-Instruct"""
        print(f"è¼‰å…¥ MLX å„ªåŒ–çš„ {model_id}...")
        try:
            # é¦–å…ˆå˜—è©¦ä½¿ç”¨ MLX-VLM æ¡†æ¶
            from mlx_vlm import load
            print("æ­£åœ¨è¼‰å…¥ MLX-VLM å„ªåŒ–çš„ SmolVLM2 æ¨¡å‹...")
            model, processor = load(model_id)
            print("MLX-VLM SmolVLM2 è¼‰å…¥æˆåŠŸ!")
            
            # æ¨™è¨˜ç‚º MLX æ¨¡å‹ï¼Œä½¿ç”¨ç‰¹æ®Šçš„æ¨ç†æ–¹å¼
            model._is_mlx_model = True
            
            return model, processor
            
        except ImportError as e:
            print("MLX-VLM æœªå®‰è£ï¼Œä½¿ç”¨åŸå§‹ SmolVLM2 æ¨¡å‹...")
            print("è«‹é‹è¡Œ: pip install mlx-vlm")
            # å›é€€åˆ°åŸå§‹ SmolVLM2 æ¨¡å‹
            fallback_model_id = "HuggingFaceTB/SmolVLM2-500M-Video-Instruct"
            processor = AutoProcessor.from_pretrained(fallback_model_id)
            model = AutoModelForImageTextToText.from_pretrained(fallback_model_id)
            return model, processor
            
        except Exception as e:
            print(f"MLX-VLM è¼‰å…¥å¤±æ•—: {str(e)}")
            print("ä½¿ç”¨åŸå§‹ SmolVLM2 æ¨¡å‹ä½œç‚ºå›é€€...")
            # å›é€€åˆ°åŸå§‹ SmolVLM2 æ¨¡å‹
            fallback_model_id = "HuggingFaceTB/SmolVLM2-500M-Video-Instruct"
            processor = AutoProcessor.from_pretrained(fallback_model_id)
            model = AutoModelForImageTextToText.from_pretrained(fallback_model_id)
            return model, processor
    
    @staticmethod
    def load_smolvlm_instruct(model_id="HuggingFaceTB/SmolVLM-500M-Instruct"):
        """è¼‰å…¥ SmolVLM-500M-Instruct"""
        print(f"è¼‰å…¥ {model_id}...")
        processor = AutoProcessor.from_pretrained(model_id)
        model = AutoModelForVision2Seq.from_pretrained(model_id)
        return model, processor
    
    @staticmethod
    def load_moondream2(model_id="vikhyatk/moondream2"):
        """è¼‰å…¥ Moondream2 - ä½¿ç”¨ç‰¹æ®Š APIï¼ˆæ¨¡å‹ä¸æ”¯æŒæ¨™æº– pipelineï¼‰"""
        print(f"è¼‰å…¥ {model_id}...")
        # Moondream2 æœ‰è‡ªå®šç¾©é…ç½®ï¼Œç„¡æ³•ä½¿ç”¨æ¨™æº– pipelineï¼Œéœ€ä½¿ç”¨åŸå§‹æ–¹å¼
        from transformers import AutoTokenizer
        model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)
        tokenizer = AutoTokenizer.from_pretrained(model_id)
        
        # å°‡æ¨¡å‹ç§»å‹•åˆ°é©ç•¶çš„è¨­å‚™
        if torch.backends.mps.is_available():
            model = model.to('mps')
        
        return model, tokenizer
    
    @staticmethod
    def load_llava_mlx(model_id="mlx-community/llava-v1.6-mistral-7b-4bit"):
        """è¼‰å…¥ MLX-LLaVA (Apple Silicon optimized)"""
        print(f"è¼‰å…¥ MLX-LLaVA {model_id}...")
        try:
            from mlx_vlm import load
            print("æ­£åœ¨è¼‰å…¥ MLX å„ªåŒ–çš„ LLaVA æ¨¡å‹...")
            model, processor = load(model_id)
            print("MLX-LLaVA è¼‰å…¥æˆåŠŸ!")
            return model, processor
        except ImportError as e:
            print("MLX-VLM æœªå®‰è£ã€‚è«‹é‹è¡Œ: pip install mlx-vlm")
            print("å›é€€åˆ°åŸå§‹ transformers æ–¹æ³•...")
            raise RuntimeError("MLX-VLM å¥—ä»¶æœªå®‰è£ï¼Œç„¡æ³•ä½¿ç”¨ MLX å„ªåŒ–")
        except Exception as e:
            print(f"MLX-LLaVA è¼‰å…¥å¤±æ•—: {str(e)}")
            raise RuntimeError(f"MLX-LLaVA æ¨¡å‹è¼‰å…¥å¤±æ•—: {str(e)}")
    
    @staticmethod
    def load_phi3_vision(model_id="mlx-community/Phi-3.5-vision-instruct-4bit"):
        """Load Phi-3.5-Vision-Instruct using MLX-VLM framework (Apple Silicon optimized)"""
        print(f"Loading {model_id} with MLX-VLM framework...")
        try:
            # Use MLX-VLM for Apple Silicon optimization (supports vision)
            from mlx_vlm import load
            
            print("Loading MLX-VLM optimized Phi-3.5-Vision-Instruct model...")
            model, processor = load(model_id, trust_remote_code=True)
            print("MLX-VLM model loaded successfully!")
            
            return model, processor
            
        except ImportError as e:
            print("MLX-VLM not installed. Please install MLX-VLM...")
            print("Please run: pip install mlx-vlm")
            print("Falling back to original transformers approach...")
            
            # Fallback to original approach if MLX-VLM not available
            from transformers import AutoModelForCausalLM, AutoProcessor
            print("Using memory-optimized loading for Apple Silicon...")
            model = AutoModelForCausalLM.from_pretrained(
                "microsoft/Phi-3.5-vision-instruct", 
                trust_remote_code=True,
                torch_dtype=torch.float16,
                _attn_implementation="eager",  # Disable FlashAttention2
                device_map="cpu",  # Force CPU to avoid memory issues
                low_cpu_mem_usage=True  # Use less CPU memory
            )
            processor = AutoProcessor.from_pretrained(
                "microsoft/Phi-3.5-vision-instruct", 
                trust_remote_code=True,
                num_crops=4  # For single-frame images
            )
            return model, processor
            
        except Exception as e:
            print(f"MLX-VLM loading failed: {str(e)}")
            print("Falling back to original transformers approach...")
            
            # Fallback to original approach
            from transformers import AutoModelForCausalLM, AutoProcessor
            print("Using memory-optimized loading for Apple Silicon...")
            model = AutoModelForCausalLM.from_pretrained(
                "microsoft/Phi-3.5-vision-instruct", 
                trust_remote_code=True,
                torch_dtype=torch.float16,
                _attn_implementation="eager",  # Disable FlashAttention2
                device_map="cpu",  # Force CPU to avoid memory issues
                low_cpu_mem_usage=True  # Use less CPU memory
            )
            processor = AutoProcessor.from_pretrained(
                "microsoft/Phi-3.5-vision-instruct", 
                trust_remote_code=True,
                num_crops=4  # For single-frame images
            )
            return model, processor

class VLMTester:
    """VLM æ¸¬è©¦å™¨"""
    
    def __init__(self):
        self.results = {
            "test_timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "system_info": {
                "device": "MacBook Air M3",
                "memory": "16GB",
                "mps_available": torch.backends.mps.is_available()
            },
            "models": {}
        }
        
        # æ¸¬è©¦æ¨¡å‹é…ç½®
        self.models_config = {
            "SmolVLM2-500M-Video-Instruct": {
                "loader": VLMModelLoader.load_smolvlm2_video,
                "model_id": "mlx-community/SmolVLM2-500M-Video-Instruct-mlx",
                "note": "MLX-optimized for Apple Silicon (M1/M2/M3), falls back to original SmolVLM2 if MLX not available or incompatible"
            },
            "SmolVLM2-500M-Video-Instruct-MLX": {
                "loader": VLMModelLoader.load_smolvlm2_video_mlx,
                "model_id": "mlx-community/SmolVLM2-500M-Video-Instruct-mlx",
                "note": "MLX-optimized for Apple Silicon (M1/M2/M3), falls back to original SmolVLM2 if MLX-VLM not available or incompatible"
            },
            "SmolVLM-500M-Instruct": {
                "loader": VLMModelLoader.load_smolvlm_instruct,
                "model_id": "HuggingFaceTB/SmolVLM-500M-Instruct"
            },
            "Moondream2": {
                "loader": VLMModelLoader.load_moondream2,
                "model_id": "vikhyatk/moondream2"
            },
            "LLaVA-v1.6-Mistral-7B-MLX": {
                "loader": VLMModelLoader.load_llava_mlx,
                "model_id": "mlx-community/llava-v1.6-mistral-7b-4bit",
                "note": "MLX-optimized for Apple Silicon (M1/M2/M3), requires 'pip install mlx-vlm'"
            },
            "Phi-3.5-Vision-Instruct": {
                "loader": VLMModelLoader.load_phi3_vision,
                "model_id": "mlx-community/Phi-3.5-vision-instruct-4bit",
                "note": "MLX-optimized for Apple Silicon (M1/M2/M3), requires 'pip install mlx-vlm'"
            }
        }
        
        # ğŸ“ çµ±ä¸€æ¸¬è©¦æ¢ä»¶
        self.prompt = "Describe what you see in this image in detail."
        self.unified_max_tokens = 100  # çµ±ä¸€ç”Ÿæˆé•·åº¦
        self.unified_image_size = 1024  # çµ±ä¸€åœ–åƒæœ€å¤§å°ºå¯¸
        
        # ğŸ’¡ ç´”æ–‡å­—æ¸¬è©¦é…ç½®
        self.enable_text_only_test = True  # æ˜¯å¦å•Ÿç”¨ç´”æ–‡å­—æ¸¬è©¦
    
    def get_test_images(self):
        """ç²å–æ¸¬è©¦åœ–åƒåˆ—è¡¨"""
        # æ”¯æ´å¾ä¸åŒç›®éŒ„åŸ·è¡Œç¨‹å¼
        possible_paths = [
            Path("src/testing/testing_material/images"),  # å¾å°ˆæ¡ˆæ ¹ç›®éŒ„åŸ·è¡Œ
            Path("testing_material/images"),              # å¾ src/testing ç›®éŒ„åŸ·è¡Œ
            Path("./testing_material/images")             # ç•¶å‰ç›®éŒ„
        ]
        
        images_dir = None
        for path in possible_paths:
            if path.exists():
                images_dir = path
                break
        
        if images_dir is None:
            print(f"è­¦å‘Šï¼šåœ–åƒè³‡æ–™å¤¾ä¸å­˜åœ¨ï¼Œå˜—è©¦äº†ä»¥ä¸‹è·¯å¾‘ï¼š")
            for path in possible_paths:
                print(f"  {path}")
            return []
        
        image_files = []
        for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:
            image_files.extend(images_dir.glob(ext))
            image_files.extend(images_dir.glob(ext.upper()))
        
        return sorted(image_files)
    
    def test_single_model(self, model_name, config):
        """æ¸¬è©¦å–®ä¸€æ¨¡å‹"""
        print(f"\n{'='*50}")
        print(f"é–‹å§‹æ¸¬è©¦æ¨¡å‹: {model_name}")
        print(f"{'='*50}")
        
        # è¨˜éŒ„è¼‰å…¥å‰è¨˜æ†¶é«”
        memory_before = get_memory_usage()
        print(f"è¼‰å…¥å‰è¨˜æ†¶é«”ä½¿ç”¨: {memory_before:.2f} GB")
        
        # è¼‰å…¥æ¨¡å‹
        start_time = time.time()
        try:
            model, processor = config["loader"]()
            load_time = time.time() - start_time
            
            # è¨˜éŒ„è¼‰å…¥å¾Œè¨˜æ†¶é«”
            memory_after = get_memory_usage()
            print(f"è¼‰å…¥å¾Œè¨˜æ†¶é«”ä½¿ç”¨: {memory_after:.2f} GB")
            print(f"æ¨¡å‹è¼‰å…¥æ™‚é–“: {load_time:.2f} ç§’")
            
            # åˆå§‹åŒ–æ¨¡å‹çµæœ
            model_results = {
                "model_id": config["model_id"],
                "load_time": load_time,
                "memory_before": memory_before,
                "memory_after": memory_after,
                "memory_diff": memory_after - memory_before,
                "images": {},
                "total_inference_time": 0,
                "successful_inferences": 0,
                "failed_inferences": 0
            }
            
            # ç²å–æ¸¬è©¦åœ–åƒ
            test_images = self.get_test_images()
            if not test_images:
                print("è­¦å‘Šï¼šæ²’æœ‰æ‰¾åˆ°æ¸¬è©¦åœ–åƒ")
                model_results["error"] = "No test images found"
                return model_results
            
            print(f"æ‰¾åˆ° {len(test_images)} å¼µæ¸¬è©¦åœ–åƒ")
            
            # æ¸¬è©¦æ¯å¼µåœ–åƒ
            for image_path in test_images:
                try:
                    # For LLaVA-MLX, reload the model for each image to avoid state bug
                    if "LLaVA-v1.6-Mistral-7B-MLX" in model_name:
                        print("  >> LLaVA-MLX: Reloading model to clear state...")
                        clear_model_memory(model, processor)
                        model, processor = config["loader"]()

                    image_result = self.test_single_image(model, processor, image_path, model_name)
                    model_results["images"][image_path.name] = image_result
                    
                    # ğŸ’¡ FIX: Improved failure detection for more accurate reporting
                    is_failure = image_result.get("error") is not None or \
                                 ("inference failed" in image_result.get("response", "").lower())
                    
                    if not is_failure:
                        model_results["successful_inferences"] += 1
                        model_results["total_inference_time"] += image_result["inference_time"]
                    else:
                        model_results["failed_inferences"] += 1
                        
                except Exception as e:
                    print(f"æ¸¬è©¦åœ–åƒ {image_path.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                    model_results["images"][image_path.name] = {
                        "error": str(e),
                        "inference_time": 0
                    }
                    model_results["failed_inferences"] += 1
            
            # è¨ˆç®—å¹³å‡æ¨ç†æ™‚é–“
            if model_results["successful_inferences"] > 0:
                model_results["avg_inference_time"] = model_results["total_inference_time"] / model_results["successful_inferences"]
            else:
                model_results["avg_inference_time"] = 0
                
            print(f"æ¨¡å‹ {model_name} åœ–åƒæ¸¬è©¦å®Œæˆ")
            print(f"æˆåŠŸ: {model_results['successful_inferences']}, å¤±æ•—: {model_results['failed_inferences']}")
            print(f"å¹³å‡æ¨ç†æ™‚é–“: {model_results['avg_inference_time']:.2f} ç§’")
            
            # ğŸ’¡ æ–°å¢ï¼šç´”æ–‡å­—èƒ½åŠ›æ¸¬è©¦ï¼ˆå¯é¸ï¼‰
            if self.enable_text_only_test:
                print(f"\né–‹å§‹æ¸¬è©¦ {model_name} ç´”æ–‡å­—èƒ½åŠ›...")
                try:
                    text_only_results = self.test_text_only_capability(model, processor, model_name)
                    model_results["text_only_capability"] = text_only_results
                    
                    if text_only_results["text_only_supported"]:
                        print(f"âœ… {model_name} æ”¯æ´ç´”æ–‡å­—è¼¸å…¥!")
                    else:
                        print(f"âŒ {model_name} ä¸æ”¯æ´ç´”æ–‡å­—è¼¸å…¥")
                        
                except Exception as e:
                    print(f"âš ï¸ ç´”æ–‡å­—æ¸¬è©¦ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                    model_results["text_only_capability"] = {
                        "text_only_supported": False,
                        "error": str(e)
                    }
            else:
                print(f"\nè·³éç´”æ–‡å­—æ¸¬è©¦ï¼ˆå·²åœç”¨ï¼‰")
                model_results["text_only_capability"] = {
                    "text_only_supported": "æœªæ¸¬è©¦",
                    "reason": "ç´”æ–‡å­—æ¸¬è©¦å·²åœç”¨"
                }
            
            print(f"\næ¨¡å‹ {model_name} æ‰€æœ‰æ¸¬è©¦å®Œæˆ")
            
        except Exception as e:
            print(f"è¼‰å…¥æ¨¡å‹ {model_name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            model_results = {
                "model_id": config["model_id"],
                "load_error": str(e),
                "memory_before": memory_before,
                "memory_after": memory_before,  # è¼‰å…¥å¤±æ•—ï¼Œè¨˜æ†¶é«”ç„¡è®ŠåŒ–
                "memory_diff": 0
            }
            
            # æ¸…ç†è¨˜æ†¶é«”ï¼ˆå³ä½¿è¼‰å…¥å¤±æ•—ä¹Ÿè¦æ¸…ç†ï¼‰
            gc.collect()
            if torch.backends.mps.is_available():
                torch.mps.empty_cache()
            
            return model_results
        
        # æ¸…ç†æ¨¡å‹è¨˜æ†¶é«”
        if model_name != "LLaVA-v1.6-Mistral-7B-MLX": # Already cleaned inside loop
            clear_model_memory(model, processor)
        
        # æª¢æŸ¥è¨˜æ†¶é«”æ¸…ç†æ•ˆæœ
        memory_after_cleanup = get_memory_usage()
        print(f"æ¸…ç†å¾Œè¨˜æ†¶é«”ä½¿ç”¨: {memory_after_cleanup:.2f} GB")
        model_results["memory_after_cleanup"] = memory_after_cleanup
        
        return model_results
    
    def test_single_image(self, model, processor, image_path, model_name):
        """æ¸¬è©¦å–®å¼µåœ–åƒï¼ˆåŒ…å«å„ªåŒ–çš„è¶…æ™‚æ©Ÿåˆ¶ï¼‰"""
        print(f"æ¸¬è©¦åœ–åƒ: {image_path.name}")
        
        temp_image_path_for_fix = None
        try:
            # è¼‰å…¥ä¸¦å„ªåŒ–åœ–åƒ
            image = Image.open(image_path).convert('RGB')
            
            # æ ¹æ“šéœ€è¦ï¼Œè¨­å®šç•¶å‰æ¨ç†æ‡‰ä½¿ç”¨çš„åœ–åƒè·¯å¾‘
            current_image_path = str(image_path)
            
            # ğŸ“ çµ±ä¸€åœ–åƒé è™•ç†ï¼ˆæ‰€æœ‰æ¨¡å‹ä¸€è‡´ï¼‰
            original_size = image.size
            if max(image.size) > self.unified_image_size:
                ratio = self.unified_image_size / max(image.size)
                new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))
                image = image.resize(new_size, Image.Resampling.LANCZOS)
                print(f"  ğŸ“ çµ±ä¸€ç¸®æ”¾: {original_size} â†’ {image.size}")
            
            image_info = {
                "original_size": original_size,
                "processed_size": image.size,
                "mode": image.mode,
                "file_size": os.path.getsize(current_image_path)
            }
            
            # ğŸ“ çµ±ä¸€ç”Ÿæˆåƒæ•¸ï¼ˆæ‰€æœ‰æ¨¡å‹ä¸€è‡´ï¼‰
            unified_generation_params = {
                "max_new_tokens": self.unified_max_tokens,
                "do_sample": False
            }
            
            # â±ï¸ Adjust timeout based on model technical characteristics (does not affect comparison fairness)
            def get_timeout(model_name):
                if "LLaVA" in model_name:
                    return 180  # CPU inference needs more time
                elif "Phi-3.5" in model_name:
                    return 180  # Give more time for both MLX and fallback transformers
                else:
                    return 60   # Small models fast inference
            
            timeout_seconds = get_timeout(model_name)
            print(f"  ğŸ“ çµ±ä¸€åƒæ•¸: {unified_generation_params}")
            print(f"  â±ï¸ è¶…æ™‚è¨­å®š: {timeout_seconds}ç§’")
            
            # å®šç¾©æ¨ç†å‡½æ•¸
            def do_inference():
                # æ ¹æ“šæ¨¡å‹é¡å‹ä½¿ç”¨ä¸åŒçš„æ¨ç†æ–¹å¼
                if "Moondream2" in model_name:
                    # Moondream2 ç‰¹æ®Š APIï¼ˆæ¨¡å‹é™åˆ¶ï¼Œä½†ä¿æŒçµ±ä¸€æ¸¬è©¦æ¢ä»¶ï¼‰
                    # å…ˆå°‡åœ–åƒç§»å‹•åˆ°æ­£ç¢ºçš„è¨­å‚™
                    device = next(model.parameters()).device
                    enc_image = model.encode_image(image)
                    if hasattr(enc_image, 'to'):
                        enc_image = enc_image.to(device)
                    # ä½¿ç”¨çµ±ä¸€æç¤ºè©ï¼Œä½†ç„¡æ³•æ§åˆ¶ max_tokensï¼ˆAPI é™åˆ¶ï¼‰
                    return model.answer_question(enc_image, self.prompt, processor)
                elif "Phi-3.5" in model_name:
                    # Check if this is an MLX-VLM model or transformers model
                    try:
                        # Use MLX-VLM inference for vision model (official way)
                        from mlx_vlm import generate
                        print("  ğŸš€ Using MLX-VLM inference for Phi-3.5-Vision-Instruct...")
                        
                        # Save image to temporary file for MLX-VLM
                        temp_image_path = "temp_mlx_image.jpg"
                        image.save(temp_image_path)
                        
                        try:
                            # Use simple prompt format for MLX-VLM (same as vlm_context_tester.py)
                            mlx_prompt = f"<|image_1|>\nUser: {self.prompt}\nAssistant:"
                            
                            response = generate(
                                model, 
                                processor, 
                                mlx_prompt,
                                image=temp_image_path,
                                max_tokens=unified_generation_params["max_new_tokens"],
                                temp=0.0,  # Use 0.0 for deterministic output
                                verbose=False
                            )
                        finally:
                            # Clean up temporary file
                            if os.path.exists(temp_image_path):
                                os.remove(temp_image_path)
                        
                        # MLX-VLM returns string directly, not tuple
                        text_response = str(response)
                        
                        # Clean up response
                        text_response = text_response.replace("<|end|><|endoftext|>", " ").replace("<|end|>", " ").replace("<|endoftext|>", " ")
                        if "1. What is meant by" in text_response:
                            text_response = text_response.split("1. What is meant by")[0].strip()
                        text_response = ' '.join(text_response.split())
                        
                        return text_response
                        
                    except (ImportError, AttributeError, TypeError, Exception) as e:
                        print(f"  âš ï¸ MLX-VLM inference failed ({e}), loading transformers model...")
                        
                        # Load transformers model for fallback (MLX model can't be used with transformers)
                        from transformers import AutoModelForCausalLM, AutoProcessor
                        print("  ğŸ“¥ Loading transformers Phi-3.5-Vision for fallback...")
                        
                        fallback_model = AutoModelForCausalLM.from_pretrained(
                            "microsoft/Phi-3.5-vision-instruct", 
                            trust_remote_code=True,
                            torch_dtype=torch.float16,
                            _attn_implementation="eager",  # Disable FlashAttention2
                            device_map="cpu",  # Force CPU to avoid memory issues
                            low_cpu_mem_usage=True  # Use less CPU memory
                        )
                        fallback_processor = AutoProcessor.from_pretrained(
                            "microsoft/Phi-3.5-vision-instruct", 
                            trust_remote_code=True,
                            num_crops=4  # For single-frame images
                        )
                        
                        # Phi-3.5 Vision special format (model compatibility requirement)
                        messages = [
                            {"role": "user", "content": f"<|image_1|>\\n{self.prompt}"}
                        ]
                        
                        prompt = fallback_processor.tokenizer.apply_chat_template(
                            messages, 
                            tokenize=False, 
                            add_generation_prompt=True
                        )
                        
                        inputs = fallback_processor(prompt, [image], return_tensors="pt")
                        
                        # Move to correct device
                        device = next(fallback_model.parameters()).device
                        inputs = {k: v.to(device) for k, v in inputs.items()}
                        
                        # Technical fix: avoid DynamicCache error
                        with torch.no_grad():
                            outputs = fallback_model.generate(
                                **inputs, 
                                **unified_generation_params,  # Use unified parameters
                                use_cache=False,  # Disable cache to avoid DynamicCache error
                                pad_token_id=fallback_processor.tokenizer.eos_token_id
                            )
                        
                        result = fallback_processor.decode(outputs[0], skip_special_tokens=True)
                        
                        # Clean up fallback model
                        del fallback_model, fallback_processor
                        gc.collect()
                        if torch.backends.mps.is_available():
                            torch.mps.empty_cache()
                        
                        return result
                elif "LLaVA" in model_name:
                    # Check if this is MLX-LLaVA or standard LLaVA
                    if "MLX" in model_name:
                        # MLX-LLaVA inference
                        try:
                            from mlx_vlm import generate
                            print("  ğŸš€ Using MLX-VLM for LLaVA...")
                            # Simple prompt for MLX-LLaVA
                            response = generate(
                                model, 
                                processor, 
                                self.prompt, 
                                image=current_image_path,
                                max_tokens=unified_generation_params["max_new_tokens"],
                                verbose=False
                            )
                            
                            # MLX-VLM returns string directly, not tuple
                            text_response = str(response)
                            
                            return text_response
                        except Exception as e:
                            print(f"  âš ï¸ MLX-VLM failed: {e}")
                            # Fallback: Return descriptive error but don't crash
                            return f"MLX-VLM inference failed: {str(e)}"
                    else:
                        # Standard LLaVA Pipeline æ–¹å¼
                        messages = [
                            {
                                "role": "user",
                                "content": [
                                    {"type": "image", "image": image},  # ä½¿ç”¨æœ¬åœ°åœ–åƒæ ¼å¼ï¼ˆèˆ‡ SmolVLM ä¸€è‡´ï¼‰
                                    {"type": "text", "text": self.prompt}  # ä½¿ç”¨çµ±ä¸€æç¤ºè©
                                ]
                            },
                        ]
                        # ğŸš€ å„ªåŒ–ï¼šæ·»åŠ ç”Ÿæˆåƒæ•¸æ§åˆ¶
                        response = model(
                            text=messages, 
                            **unified_generation_params,  # ä½¿ç”¨çµ±ä¸€åƒæ•¸
                            return_full_text=False  # åªè¿”å›ç”Ÿæˆéƒ¨åˆ†
                        )
                        if isinstance(response, list) and len(response) > 0:
                            return response[0].get('generated_text', str(response))
                        else:
                            return str(response)
                elif "SmolVLM" in model_name:
                    # SmolVLM å„ªåŒ–æ–¹å¼
                    if "MLX" in model_name or hasattr(model, '_is_mlx_model'):
                        # MLX ç‰ˆæœ¬çš„ SmolVLM2 æ¨ç†
                        try:
                            # ä½¿ç”¨ MLX-VLM çš„å‘½ä»¤è¡Œå·¥å…·é€²è¡Œæ¨ç†
                            import subprocess
                            import tempfile
                            
                            print("  ğŸš€ Using MLX-VLM command line for SmolVLM2...")
                            
                            # å‰µå»ºè‡¨æ™‚åœ–åƒæ–‡ä»¶
                            with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as tmp_file:
                                temp_image_path = tmp_file.name
                                image.save(temp_image_path)
                            
                            try:
                                # ä½¿ç”¨ MLX-VLM å‘½ä»¤è¡Œå·¥å…·
                                cmd = [
                                    sys.executable, '-m', 'mlx_vlm.generate',
                                    '--model', 'mlx-community/SmolVLM2-500M-Video-Instruct-mlx',
                                    '--image', temp_image_path,
                                    '--prompt', self.prompt,
                                    '--max-tokens', str(unified_generation_params["max_new_tokens"]),
                                    '--temperature', '0.0'
                                ]
                                
                                result = subprocess.run(
                                    cmd,
                                    capture_output=True,
                                    text=True,
                                    timeout=timeout_seconds
                                )
                                
                                if result.returncode == 0:
                                    # è§£æè¼¸å‡ºï¼Œæå–ç”Ÿæˆçš„æ–‡æœ¬
                                    output_lines = result.stdout.split('\n')
                                    generated_text = ""
                                    
                                    # ä¿ç•™å®Œæ•´çš„ Assistant å›è¦†
                                    generated_text = ""
                                    assistant_found = False
                                    for i, line in enumerate(output_lines):
                                        line = line.strip()
                                        if line.startswith('Assistant:'):
                                            # æ‰¾åˆ° Assistant è¡Œ
                                            assistant_found = True
                                            generated_text = line
                                            # æª¢æŸ¥ä¸‹ä¸€è¡Œæ˜¯å¦æœ‰å…§å®¹
                                            if i + 1 < len(output_lines):
                                                next_line = output_lines[i + 1].strip()
                                                if next_line and not next_line.startswith('==========') and not next_line.startswith('Files:') and not next_line.startswith('Prompt:') and not next_line.startswith('Generation:') and not next_line.startswith('Peak memory:'):
                                                    # ä¸‹ä¸€è¡Œæœ‰å…§å®¹ï¼Œçµ„åˆå…©è¡Œ
                                                    generated_text = f"{line} {next_line}"
                                            break
                                        elif line and not line.startswith('==========') and not line.startswith('Files:') and not line.startswith('Prompt:') and not line.startswith('Generation:') and not line.startswith('Peak memory:'):
                                            # æ‰¾åˆ°å…¶ä»–éç³»çµ±ä¿¡æ¯çš„å…§å®¹è¡Œ
                                            if not generated_text:
                                                generated_text = line
                                    
                                    return generated_text
                                else:
                                    print(f"  âš ï¸ MLX-VLM command failed: {result.stderr}")
                                    raise Exception(f"MLX-VLM command failed: {result.stderr}")
                                    
                            finally:
                                # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
                                if os.path.exists(temp_image_path):
                                    os.remove(temp_image_path)
                            
                        except Exception as e:
                            print(f"  âš ï¸ MLX-VLM SmolVLM2 inference failed: {e}")
                            # Fallback to standard SmolVLM method
                            print("  ğŸ“¥ Falling back to standard SmolVLM method...")
                            messages = [
                                {
                                    "role": "user",
                                    "content": [
                                        {"type": "image", "image": image},
                                        {"type": "text", "text": self.prompt}
                                    ]
                                }
                            ]
                            input_text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
                            inputs = processor(text=input_text, images=image, return_tensors="pt")
                            with torch.no_grad():
                                outputs = model.generate(**inputs, **unified_generation_params)
                            return processor.decode(outputs[0], skip_special_tokens=True)
                    else:
                        # æ¨™æº– SmolVLM æ¨ç†æ–¹å¼
                        messages = [
                            {
                                "role": "user",
                                "content": [
                                    {"type": "image", "image": image},
                                    {"type": "text", "text": self.prompt}
                                ]
                            }
                        ]
                        input_text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
                        inputs = processor(text=input_text, images=image, return_tensors="pt")
                        with torch.no_grad():
                            outputs = model.generate(**inputs, **unified_generation_params)  # ä½¿ç”¨çµ±ä¸€åƒæ•¸
                        return processor.decode(outputs[0], skip_special_tokens=True)
                else:
                    # å‚³çµ±æ–¹å¼
                    inputs = processor(text=self.prompt, images=image, return_tensors="pt")
                    with torch.no_grad():
                        outputs = model.generate(**inputs, **unified_generation_params)  # ä½¿ç”¨çµ±ä¸€åƒæ•¸
                    return processor.decode(outputs[0], skip_special_tokens=True)
            
            # â±ï¸ åŸ·è¡Œå¸¶è¶…æ™‚çš„æ¨ç†
            start_time = time.time()
            try:
                response_text = run_with_timeout(do_inference, timeout_seconds=timeout_seconds)
                if response_text is None:
                    response_text = ""
                    
                inference_time = time.time() - start_time
                
                # ğŸ’¡ FIX: Properly separate successful responses from error messages
                error_message = None
                final_response = response_text
                if "inference failed" in response_text.lower():
                    error_message = response_text
                    final_response = ""  # Response should be empty on error
                    print(f"  âŒ Detected inference failure: {error_message}")

                result = {
                    "inference_time": inference_time,
                    "response": final_response,
                    "image_info": image_info,
                    "error": error_message,  # Correctly populate the error field
                    "unified_test": True,
                    "generation_params": unified_generation_params,
                    "timeout_used": timeout_seconds
                }
                
                print(f"  âœ… æ¨ç†æ™‚é–“: {inference_time:.2f} ç§’")
                print(f"  ğŸ“ å›æ‡‰é•·åº¦: {len(final_response)} å­—å…ƒ")
                
                return result
                
            except TimeoutError as e:
                inference_time = time.time() - start_time
                print(f"  âš ï¸ {str(e)}")
                return {
                    "inference_time": inference_time,
                    "response": "",
                    "image_info": image_info,
                    "error": str(e),
                    "unified_test": True  # æ¨™è¨˜ä½¿ç”¨çµ±ä¸€æ¸¬è©¦æ¢ä»¶
                }
            
        except Exception as e:
            print(f"  âŒ éŒ¯èª¤: {str(e)}")
            return {
                "inference_time": 0,
                "response": "",
                "image_info": {},
                "error": str(e),
                "unified_test": True  # æ¨™è¨˜ä½¿ç”¨çµ±ä¸€æ¸¬è©¦æ¢ä»¶ï¼ˆå³ä½¿å¤±æ•—ï¼‰
            }
        finally:
            # æ¸…ç†è‡¨æ™‚æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if temp_image_path_for_fix:
                os.remove(temp_image_path_for_fix)
    
    def test_text_only_capability(self, model, processor, model_name):
        """æ¸¬è©¦æ¨¡å‹æ˜¯å¦æ”¯æ´ç´”æ–‡å­—è¼¸å…¥ï¼ˆä¸éœ€è¦åœ–ç‰‡ï¼‰- æ”¯æ´æ‰€æœ‰æ¨¡å‹"""
        print(f"æ¸¬è©¦ {model_name} ç´”æ–‡å­—èƒ½åŠ›...")
        
        # ç´”æ–‡å­—æ¸¬è©¦æç¤º
        text_only_prompts = [
            "What is the capital of France?",
            "Explain the concept of machine learning in simple terms.",
            "Write a short poem about technology."
        ]
        
        results = {}
        
        for i, prompt in enumerate(text_only_prompts):
            print(f"  æ¸¬è©¦æç¤º {i+1}: {prompt}")
            
            try:
                start_time = time.time()
                response = ""
                
                # ğŸ”§ æ ¹æ“šæ¨¡å‹é¡å‹ä½¿ç”¨å°ˆé–€çš„ç´”æ–‡å­—æ¨ç†æ–¹å¼
                if "Moondream2" in model_name:
                    response = self._test_moondream2_text_only(model, processor, prompt)
                elif "Phi-3.5" in model_name:
                    response = self._test_phi35_text_only(model, processor, prompt)
                elif "SmolVLM2" in model_name:
                    response = self._test_smolvlm2_text_only(model, processor, prompt)
                elif "SmolVLM" in model_name:
                    response = self._test_smolvlm_text_only(model, processor, prompt)
                elif "LLaVA" in model_name:
                    response = self._test_llava_text_only(model, processor, prompt)
                else:
                    response = self._test_generic_text_only(model, processor, prompt)
                
                inference_time = time.time() - start_time
                
                # åˆ¤æ–·æˆåŠŸèˆ‡å¦
                is_success = (
                    response and 
                    len(response.strip()) > 0 and 
                    "å¤±æ•—" not in response and 
                    "failed" not in response.lower() and
                    "error" not in response.lower() and
                    "ä¸æ”¯æ´" not in response
                )
                
                results[f"prompt_{i+1}"] = {
                    "prompt": prompt,
                    "response": response,
                    "inference_time": inference_time,
                    "success": is_success
                }
                
                print(f"    å›æ‡‰: {response[:100]}...")
                print(f"    æ™‚é–“: {inference_time:.2f}ç§’")
                print(f"    ç‹€æ…‹: {'âœ… æˆåŠŸ' if is_success else 'âŒ å¤±æ•—'}")
                
            except Exception as e:
                results[f"prompt_{i+1}"] = {
                    "prompt": prompt,
                    "response": "",
                    "inference_time": 0,
                    "error": str(e),
                    "success": False
                }
                print(f"    éŒ¯èª¤: {str(e)}")
        
        # è¨ˆç®—æˆåŠŸç‡
        successful_tests = sum(1 for r in results.values() if r.get("success", False))
        total_tests = len(results)
        success_rate = successful_tests / total_tests if total_tests > 0 else 0
        
        print(f"  ç´”æ–‡å­—æ¸¬è©¦æˆåŠŸç‡: {successful_tests}/{total_tests} ({success_rate:.1%})")
        
        return {
            "text_only_supported": success_rate > 0,  # ä»»ä½•æˆåŠŸéƒ½è¦–ç‚ºæ”¯æ´
            "success_rate": success_rate,
            "total_tests": total_tests,
            "successful_tests": successful_tests,
            "results": results
        }
    
    def _test_moondream2_text_only(self, model, processor, prompt):
        """Moondream2 ç´”æ–‡å­—æ¸¬è©¦"""
        try:
            # æ–¹æ³•1: ç›´æ¥ä½¿ç”¨ tokenizer é€²è¡Œç´”æ–‡å­—ç”Ÿæˆ
            inputs = processor(prompt, return_tensors="pt")
            device = next(model.parameters()).device
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=self.unified_max_tokens,
                    do_sample=False,
                    pad_token_id=processor.eos_token_id
                )
            
            response = processor.decode(outputs[0], skip_special_tokens=True)
            
            # ç§»é™¤åŸå§‹æç¤ºï¼Œåªä¿ç•™ç”Ÿæˆçš„éƒ¨åˆ†
            if prompt in response:
                response = response.replace(prompt, "").strip()
            
            return response
            
        except Exception as e:
            # æ–¹æ³•2: å˜—è©¦ä½¿ç”¨æ¨¡å‹çš„ chat åŠŸèƒ½ï¼ˆå¦‚æœæœ‰ï¼‰
            try:
                # æŸäº›ç‰ˆæœ¬çš„ Moondream2 å¯èƒ½æ”¯æ´ç´”æ–‡å­—å°è©±
                device = next(model.parameters()).device
                inputs = processor(prompt, return_tensors="pt").to(device)
                
                with torch.no_grad():
                    outputs = model.generate(
                        inputs.input_ids,
                        max_new_tokens=self.unified_max_tokens,
                        do_sample=False,
                        pad_token_id=processor.eos_token_id
                    )
                
                response = processor.decode(outputs[0], skip_special_tokens=True)
                return response.replace(prompt, "").strip()
                
            except Exception as e2:
                return f"Moondream2 ç´”æ–‡å­—æ¨ç†å¤±æ•—: {str(e)} | å‚™ç”¨æ–¹æ³•: {str(e2)}"
    
    def _test_phi35_text_only(self, model, processor, prompt):
        """Phi-3.5-Vision-Instruct ç´”æ–‡å­—æ¸¬è©¦"""
        try:
            # ä½¿ç”¨ MLX-VLM é€²è¡Œç´”æ–‡å­—æ¨ç†ï¼ˆç°¡åŒ–æ–¹å¼ï¼‰
            try:
                from mlx_vlm import generate
                print("  ğŸš€ Using MLX-VLM for Phi-3.5-Vision-Instruct text-only...")
                
                # Generate text output - use simple prompt for text-only
                response = generate(
                    model,
                    processor,
                    prompt,  # Use simple string prompt
                    max_tokens=self.unified_max_tokens,
                    temp=0.0,
                    verbose=False
                )
                return str(response)
                
            except Exception as mlx_e:
                return f"MLX-VLM ç´”æ–‡å­—æ¨ç†å¤±æ•—: {str(mlx_e)}"
            
        except Exception as e:
            return f"Phi-3.5-Vision-Instruct ç´”æ–‡å­—æ¨ç†å¤±æ•—: {str(e)}"
    
    def _test_smolvlm2_text_only(self, model, processor, prompt):
        """SmolVLM2-500M-Video ç´”æ–‡å­—æ¸¬è©¦"""
        try:
            # æª¢æŸ¥æ˜¯å¦ç‚º MLX ç‰ˆæœ¬
            if hasattr(model, '_is_mlx_model'):
                # MLX ç‰ˆæœ¬çš„ç´”æ–‡å­—æ¸¬è©¦
                try:
                    import subprocess
                    import tempfile
                    
                    print("  ğŸš€ Using MLX-VLM command line for SmolVLM2 text-only...")
                    
                    # å‰µå»ºä¸€å€‹ç°¡å–®çš„æ¸¬è©¦åœ–åƒï¼ˆMLX-VLM éœ€è¦åœ–åƒè¼¸å…¥ï¼‰
                    from PIL import Image
                    test_image = Image.new('RGB', (224, 224), color='white')
                    
                    with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as tmp_file:
                        temp_image_path = tmp_file.name
                        test_image.save(temp_image_path)
                    
                    try:
                        # ä½¿ç”¨ MLX-VLM å‘½ä»¤è¡Œå·¥å…·é€²è¡Œç´”æ–‡å­—æ¸¬è©¦
                        cmd = [
                            sys.executable, '-m', 'mlx_vlm.generate',
                            '--model', 'mlx-community/SmolVLM2-500M-Video-Instruct-mlx',
                            '--image', temp_image_path,
                            '--prompt', prompt,
                            '--max-tokens', str(self.unified_max_tokens),
                            '--temperature', '0.0'
                        ]
                        
                        result = subprocess.run(
                            cmd,
                            capture_output=True,
                            text=True,
                            timeout=60
                        )
                        
                        if result.returncode == 0:
                            # ä¿ç•™å®Œæ•´çš„ Assistant å›è¦†
                            output_lines = result.stdout.split('\n')
                            generated_text = ""
                            
                            for i, line in enumerate(output_lines):
                                line = line.strip()
                                if line.startswith('Assistant:'):
                                    # æ‰¾åˆ° Assistant è¡Œ
                                    generated_text = line
                                    # æª¢æŸ¥ä¸‹ä¸€è¡Œæ˜¯å¦æœ‰å…§å®¹
                                    if i + 1 < len(output_lines):
                                        next_line = output_lines[i + 1].strip()
                                        if next_line and not next_line.startswith('==========') and not next_line.startswith('Files:') and not next_line.startswith('Prompt:') and not next_line.startswith('Generation:') and not next_line.startswith('Peak memory:'):
                                            # ä¸‹ä¸€è¡Œæœ‰å…§å®¹ï¼Œçµ„åˆå…©è¡Œ
                                            generated_text = f"{line} {next_line}"
                                    break
                                elif line and not line.startswith('==========') and not line.startswith('Files:') and not line.startswith('Prompt:') and not line.startswith('Generation:') and not line.startswith('Peak memory:'):
                                    # æ‰¾åˆ°å…¶ä»–éç³»çµ±ä¿¡æ¯çš„å…§å®¹è¡Œ
                                    if not generated_text:
                                        generated_text = line
                            
                            return generated_text
                        else:
                            return f"MLX-VLM text-only command failed: {result.stderr}"
                            
                    finally:
                        # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
                        if os.path.exists(temp_image_path):
                            os.remove(temp_image_path)
                    
                except Exception as mlx_e:
                    print(f"  âš ï¸ MLX-VLM text-only failed: {mlx_e}")
                    return f"MLX-VLM ç´”æ–‡å­—æ¨ç†å¤±æ•—: {str(mlx_e)}"
            
            # æ¨™æº– SmolVLM2 ç´”æ–‡å­—æ¸¬è©¦
            # æ–¹æ³•1: å˜—è©¦ç´”æ–‡å­—æ¶ˆæ¯æ ¼å¼
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt}  # åªæœ‰æ–‡å­—ï¼Œæ²’æœ‰åœ–åƒ
                    ]
                }
            ]
            
            input_text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = processor(text=input_text, return_tensors="pt")
            
            with torch.no_grad():
                outputs = model.generate(**inputs, max_new_tokens=self.unified_max_tokens, do_sample=False)
            
            response = processor.decode(outputs[0], skip_special_tokens=True)
            return response.replace(input_text, "").strip()
            
        except Exception as e:
            # æ–¹æ³•2: å˜—è©¦ç›´æ¥æ–‡å­—è¼¸å…¥
            try:
                inputs = processor(text=prompt, return_tensors="pt")
                with torch.no_grad():
                    outputs = model.generate(**inputs, max_new_tokens=self.unified_max_tokens, do_sample=False)
                response = processor.decode(outputs[0], skip_special_tokens=True)
                return response.replace(prompt, "").strip()
            except Exception as e2:
                return f"SmolVLM2 ç´”æ–‡å­—æ¨ç†å¤±æ•—: {str(e)} | å‚™ç”¨æ–¹æ³•: {str(e2)}"
    
    def _test_smolvlm_text_only(self, model, processor, prompt):
        """SmolVLM-500M-Instruct ç´”æ–‡å­—æ¸¬è©¦"""
        try:
            # æ–¹æ³•1: å˜—è©¦ç´”æ–‡å­—æ¶ˆæ¯æ ¼å¼
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt}  # åªæœ‰æ–‡å­—ï¼Œæ²’æœ‰åœ–åƒ
                    ]
                }
            ]
            
            input_text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = processor(text=input_text, return_tensors="pt")
            
            with torch.no_grad():
                outputs = model.generate(**inputs, max_new_tokens=self.unified_max_tokens, do_sample=False)
            
            response = processor.decode(outputs[0], skip_special_tokens=True)
            return response.replace(input_text, "").strip()
            
        except Exception as e:
            # æ–¹æ³•2: å˜—è©¦ç›´æ¥æ–‡å­—è¼¸å…¥
            try:
                inputs = processor(text=prompt, return_tensors="pt")
                with torch.no_grad():
                    outputs = model.generate(**inputs, max_new_tokens=self.unified_max_tokens, do_sample=False)
                response = processor.decode(outputs[0], skip_special_tokens=True)
                return response.replace(prompt, "").strip()
            except Exception as e2:
                return f"SmolVLM ç´”æ–‡å­—æ¨ç†å¤±æ•—: {str(e)} | å‚™ç”¨æ–¹æ³•: {str(e2)}"
    
    def _test_llava_text_only(self, model, processor, prompt):
        """LLaVA-MLX ç´”æ–‡å­—æ¸¬è©¦"""
        try:
            # æ–¹æ³•1: MLX-VLM ç´”æ–‡å­—æ¨ç†
            from mlx_vlm import generate
            response = generate(
                model=model,
                processor=processor,
                prompt=prompt,
                max_tokens=self.unified_max_tokens,
                verbose=False
            )
            
            if isinstance(response, tuple) and len(response) >= 1:
                text_response = response[0] if response[0] else ""
            else:
                text_response = str(response) if response else ""
            
            return text_response
            
        except Exception as e:
            # æ–¹æ³•2: å˜—è©¦ pipeline æ–¹å¼ï¼ˆå¦‚æœæ”¯æ´ï¼‰
            try:
                # å˜—è©¦ç´”æ–‡å­—å°è©±æ ¼å¼
                messages = [
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt}
                        ]
                    },
                ]
                
                response = model(text=messages, max_new_tokens=self.unified_max_tokens, return_full_text=False)
                if isinstance(response, list) and len(response) > 0:
                    return response[0].get('generated_text', str(response))
                else:
                    return str(response)
                    
            except Exception as e2:
                return f"LLaVA ç´”æ–‡å­—æ¨ç†å¤±æ•—: {str(e)} | å‚™ç”¨æ–¹æ³•: {str(e2)}"
    
    def _test_generic_text_only(self, model, processor, prompt):
        """é€šç”¨ç´”æ–‡å­—æ¸¬è©¦æ–¹æ³•"""
        try:
            # æ–¹æ³•1: å˜—è©¦ä½¿ç”¨ processor é€²è¡Œç´”æ–‡å­—è™•ç†
            inputs = processor(text=prompt, return_tensors="pt")
            device = next(model.parameters()).device
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=self.unified_max_tokens,
                    do_sample=False
                )
            
            response = processor.decode(outputs[0], skip_special_tokens=True)
            return response.replace(prompt, "").strip()
            
        except Exception as e:
            # æ–¹æ³•2: å˜—è©¦ç›´æ¥ tokenizer æ–¹å¼
            try:
                if hasattr(processor, 'tokenizer'):
                    inputs = processor.tokenizer(prompt, return_tensors="pt")
                    device = next(model.parameters()).device
                    inputs = {k: v.to(device) for k, v in inputs.items()}
                    
                    with torch.no_grad():
                        outputs = model.generate(
                            **inputs,
                            max_new_tokens=self.unified_max_tokens,
                            do_sample=False
                        )
                    
                    response = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)
                    return response.replace(prompt, "").strip()
                else:
                    return f"é€šç”¨ç´”æ–‡å­—æ¨ç†å¤±æ•—: ç„¡æ³•æ‰¾åˆ°é©ç•¶çš„ tokenizer"
                    
            except Exception as e2:
                return f"é€šç”¨ç´”æ–‡å­—æ¨ç†å¤±æ•—: {str(e)} | å‚™ç”¨æ–¹æ³•: {str(e2)}"
    
    def run_all_tests(self):
        """åŸ·è¡Œæ‰€æœ‰æ¨¡å‹çš„æ¸¬è©¦"""
        print("é–‹å§‹ VLM æ¨¡å‹æ¸¬è©¦")
        print(f"ç³»çµ±è³‡è¨Š: MacBook Air M3, 16GB")
        print(f"MPS å¯ç”¨: {torch.backends.mps.is_available()}")
        
        # æª¢æŸ¥æ¸¬è©¦åœ–åƒ
        test_images = self.get_test_images()
        print(f"æ‰¾åˆ° {len(test_images)} å¼µæ¸¬è©¦åœ–åƒ")
        
        if not test_images:
            print("éŒ¯èª¤ï¼šæ²’æœ‰æ‰¾åˆ°æ¸¬è©¦åœ–åƒï¼Œè«‹å°‡åœ–åƒæ”¾ç½®æ–¼ src/testing/testing_material/images/")
            return
        
        total_start_time = time.time()
        
        # é€ä¸€æ¸¬è©¦æ¯å€‹æ¨¡å‹
        for model_name, config in self.models_config.items():
            try:
                model_results = self.test_single_model(model_name, config)
                self.results["models"][model_name] = model_results
                
                # å„²å­˜ä¸­é–“çµæœï¼ˆé˜²æ­¢æ¸¬è©¦ä¸­æ–·ä¸Ÿå¤±æ•¸æ“šï¼‰
                self.save_results(f"intermediate_{model_name}")
                
            except Exception as e:
                print(f"æ¸¬è©¦æ¨¡å‹ {model_name} æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {str(e)}")
                self.results["models"][model_name] = {
                    "severe_error": str(e)
                }
        
        # è¨˜éŒ„ç¸½æ¸¬è©¦æ™‚é–“
        total_time = time.time() - total_start_time
        self.results["total_test_time"] = total_time
        print(f"\næ‰€æœ‰æ¸¬è©¦å®Œæˆï¼Œç¸½æ™‚é–“: {total_time:.2f} ç§’")
        
        # å„²å­˜æœ€çµ‚çµæœ
        self.save_results()
    
    def save_results(self, suffix=""):
        """å„²å­˜æ¸¬è©¦çµæœ"""
        # æ”¯æ´å¾ä¸åŒç›®éŒ„åŸ·è¡Œç¨‹å¼
        possible_results_dirs = [
            Path("src/testing/results"),  # å¾å°ˆæ¡ˆæ ¹ç›®éŒ„åŸ·è¡Œ
            Path("results"),              # å¾ src/testing ç›®éŒ„åŸ·è¡Œ
            Path("./results")             # ç•¶å‰ç›®éŒ„
        ]
        
        # ä½¿ç”¨ç¬¬ä¸€å€‹å¯è¡Œçš„è·¯å¾‘ï¼Œå¦‚æœéƒ½ä¸å­˜åœ¨å‰‡å‰µå»ºç¬¬äºŒå€‹
        results_dir = possible_results_dirs[1] if Path("testing_material").exists() else possible_results_dirs[0]
        results_dir.mkdir(parents=True, exist_ok=True)
        
        if suffix:
            filename = f"test_results_{suffix}.json"
        else:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"test_results_{timestamp}.json"
        
        filepath = results_dir / filename
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2)
            print(f"çµæœå·²å„²å­˜è‡³: {filepath}")
        except Exception as e:
            print(f"å„²å­˜çµæœæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")

def main():
    """ä¸»å‡½æ•¸"""
    print("VLM æ¨¡å‹æ¸¬è©¦ç¨‹å¼")
    print("="*50)
    
    # æª¢æŸ¥è™›æ“¬ç’°å¢ƒ
    activate_virtual_env()
    
    # å»ºç«‹æ¸¬è©¦å™¨
    tester = VLMTester()
    
    # æª¢æŸ¥å‘½ä»¤è¡Œåƒæ•¸æ˜¯å¦è¦æ¸¬è©¦å–®ä¸€æ¨¡å‹
    import sys
    if len(sys.argv) > 1:
        model_name = sys.argv[1]
        if model_name in tester.models_config:
            print(f"æ¸¬è©¦å–®ä¸€æ¨¡å‹: {model_name}")
            model_results = tester.test_single_model(model_name, tester.models_config[model_name])
            tester.results["models"][model_name] = model_results
            tester.save_results(f"single_{model_name}")
        else:
            print(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æ¨¡å‹ {model_name}")
            print(f"å¯ç”¨çš„æ¨¡å‹ï¼š{list(tester.models_config.keys())}")
            return
    else:
        # åŸ·è¡Œæ‰€æœ‰æ¨¡å‹æ¸¬è©¦
        tester.run_all_tests()
    
    print("\næ¸¬è©¦å®Œæˆï¼")
    print("çµæœæ–‡ä»¶ä½æ–¼: src/testing/results/")

if __name__ == "__main__":
    main() 