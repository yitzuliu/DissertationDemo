#!/usr/bin/env python3
"""
VLM æ¨¡å‹æ¸¬è©¦ç¨‹å¼
MacBook Air M3 (16GB) å„ªåŒ–ç‰ˆæœ¬ - é€ä¸€è¼‰å…¥æ¨¡å‹é¿å…è¨˜æ†¶é«”æº¢å‡º
"""

import os
import time
import json
import gc
import signal
import threading
import psutil
from datetime import datetime
from pathlib import Path
from PIL import Image
import torch
from transformers import AutoProcessor, AutoModelForVision2Seq, AutoModelForCausalLM, AutoModelForImageTextToText
from transformers.pipelines import pipeline

# è¨˜æ†¶é«”ç›£æ§ [[memory:2405482]]
def activate_virtual_env():
    """ç¢ºä¿è™›æ“¬ç’°å¢ƒå·²å•Ÿå‹•"""
    print("è«‹ç¢ºä¿å·²å•Ÿå‹•è™›æ“¬ç’°å¢ƒ: source ai_vision_env/bin/activate")

def get_memory_usage():
    """ç²å–ç•¶å‰è¨˜æ†¶é«”ä½¿ç”¨é‡ï¼ˆGBï¼‰"""
    process = psutil.Process()
    memory_info = process.memory_info()
    return memory_info.rss / (1024 ** 3)  # è½‰æ›ç‚º GB

def clear_model_memory(model, processor):
    """æ¸…ç†æ¨¡å‹è¨˜æ†¶é«”"""
    print("æ¸…ç†æ¨¡å‹è¨˜æ†¶é«”...")
    del model, processor
    gc.collect()
    if torch.backends.mps.is_available():
        torch.mps.empty_cache()
    time.sleep(2)  # è®“ç³»çµ±æœ‰æ™‚é–“æ¸…ç†è¨˜æ†¶é«”

class TimeoutError(Exception):
    """è¶…æ™‚éŒ¯èª¤"""
    pass

def run_with_timeout(func, timeout_seconds=120):
    """åœ¨æŒ‡å®šæ™‚é–“å…§åŸ·è¡Œå‡½æ•¸ï¼Œè¶…æ™‚å‰‡æ‹‹å‡ºç•°å¸¸"""
    result = []
    exception = []
    
    def target():
        try:
            result.append(func())
        except Exception as e:
            exception.append(e)
    
    thread = threading.Thread(target=target)
    thread.daemon = True
    thread.start()
    thread.join(timeout_seconds)
    
    if thread.is_alive():
        # è¶…æ™‚äº†ï¼Œä½† Python ç„¡æ³•å¼·åˆ¶çµ‚æ­¢ç·šç¨‹ï¼Œåªèƒ½æ‹‹å‡ºç•°å¸¸
        raise TimeoutError(f"æ¨ç†è¶…æ™‚ï¼ˆ{timeout_seconds}ç§’ï¼‰")
    
    if exception:
        raise exception[0]
    
    return result[0] if result else None

# æ¨¡å‹è¼‰å…¥å™¨é¡
class VLMModelLoader:
    """VLM æ¨¡å‹è¼‰å…¥å™¨ - æ ¹æ“š active_model.md å¯¦ç¾"""
    
    @staticmethod
    def load_smolvlm2_video(model_id="HuggingFaceTB/SmolVLM2-500M-Video-Instruct"):
        """è¼‰å…¥ SmolVLM2-500M-Video-Instruct"""
        print(f"è¼‰å…¥ {model_id}...")
        processor = AutoProcessor.from_pretrained(model_id)
        model = AutoModelForImageTextToText.from_pretrained(model_id)
        return model, processor
    
    @staticmethod
    def load_smolvlm_instruct(model_id="HuggingFaceTB/SmolVLM-500M-Instruct"):
        """è¼‰å…¥ SmolVLM-500M-Instruct"""
        print(f"è¼‰å…¥ {model_id}...")
        processor = AutoProcessor.from_pretrained(model_id)
        model = AutoModelForVision2Seq.from_pretrained(model_id)
        return model, processor
    
    @staticmethod
    def load_moondream2(model_id="vikhyatk/moondream2"):
        """è¼‰å…¥ Moondream2 - ä½¿ç”¨ç‰¹æ®Š APIï¼ˆæ¨¡å‹ä¸æ”¯æŒæ¨™æº– pipelineï¼‰"""
        print(f"è¼‰å…¥ {model_id}...")
        # Moondream2 æœ‰è‡ªå®šç¾©é…ç½®ï¼Œç„¡æ³•ä½¿ç”¨æ¨™æº– pipelineï¼Œéœ€ä½¿ç”¨åŸå§‹æ–¹å¼
        from transformers import AutoTokenizer
        model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)
        tokenizer = AutoTokenizer.from_pretrained(model_id)
        
        # å°‡æ¨¡å‹ç§»å‹•åˆ°é©ç•¶çš„è¨­å‚™
        if torch.backends.mps.is_available():
            model = model.to('mps')
        
        return model, tokenizer
    
    @staticmethod
    def load_llava_mlx(model_id="mlx-community/llava-v1.6-mistral-7b-4bit"):
        """è¼‰å…¥ MLX-LLaVA (Apple Silicon optimized)"""
        print(f"è¼‰å…¥ MLX-LLaVA {model_id}...")
        try:
            from mlx_vlm import load
            print("æ­£åœ¨è¼‰å…¥ MLX å„ªåŒ–çš„ LLaVA æ¨¡å‹...")
            model, processor = load(model_id)
            print("MLX-LLaVA è¼‰å…¥æˆåŠŸ!")
            return model, processor
        except ImportError as e:
            print("MLX-VLM æœªå®‰è£ã€‚è«‹é‹è¡Œ: pip install mlx-vlm")
            print("å›é€€åˆ°åŸå§‹ transformers æ–¹æ³•...")
            raise RuntimeError("MLX-VLM å¥—ä»¶æœªå®‰è£ï¼Œç„¡æ³•ä½¿ç”¨ MLX å„ªåŒ–")
        except Exception as e:
            print(f"MLX-LLaVA è¼‰å…¥å¤±æ•—: {str(e)}")
            raise RuntimeError(f"MLX-LLaVA æ¨¡å‹è¼‰å…¥å¤±æ•—: {str(e)}")
    
    @staticmethod
    def load_phi3_vision(model_id="lokinfey/Phi-3.5-vision-mlx-int4"):
        """Load Phi-3.5-Vision-Instruct using MLX (Apple Silicon optimized)"""
        print(f"Loading {model_id} with MLX framework...")
        try:
            # Use MLX-VLM for Apple Silicon optimization
            import mlx.core as mx
            from mlx_vlm import load, generate
            from mlx_vlm.utils import load_config
            
            print("Loading MLX-optimized Phi-3.5-Vision model...")
            model, processor = load(model_id, trust_remote_code=True)
            print("MLX model loaded successfully!")
            
            return model, processor
            
        except ImportError as e:
            print("MLX-VLM not installed. Installing MLX-VLM...")
            print("Please run: pip install mlx-vlm")
            print("Falling back to original transformers approach...")
            
            # Fallback to original approach if MLX not available
            from transformers import AutoModelForCausalLM, AutoProcessor
            print("Using memory-optimized loading for Apple Silicon...")
            model = AutoModelForCausalLM.from_pretrained(
                "microsoft/Phi-3.5-vision-instruct", 
                trust_remote_code=True,
                torch_dtype=torch.float16,
                _attn_implementation="eager",  # Disable FlashAttention2
                device_map="cpu",  # Force CPU to avoid memory issues
                low_cpu_mem_usage=True  # Use less CPU memory
            )
            processor = AutoProcessor.from_pretrained("microsoft/Phi-3.5-vision-instruct", trust_remote_code=True)
            return model, processor
            
        except Exception as e:
            print(f"MLX loading failed: {str(e)}")
            print("Falling back to original transformers approach...")
            
            # Fallback to original approach
            from transformers import AutoModelForCausalLM, AutoProcessor
            print("Using memory-optimized loading for Apple Silicon...")
            model = AutoModelForCausalLM.from_pretrained(
                "microsoft/Phi-3.5-vision-instruct", 
                trust_remote_code=True,
                torch_dtype=torch.float16,
                _attn_implementation="eager",  # Disable FlashAttention2
                device_map="cpu",  # Force CPU to avoid memory issues
                low_cpu_mem_usage=True  # Use less CPU memory
            )
            processor = AutoProcessor.from_pretrained("microsoft/Phi-3.5-vision-instruct", trust_remote_code=True)
            return model, processor

class VLMTester:
    """VLM æ¸¬è©¦å™¨"""
    
    def __init__(self):
        self.results = {
            "test_timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "system_info": {
                "device": "MacBook Air M3",
                "memory": "16GB",
                "mps_available": torch.backends.mps.is_available()
            },
            "models": {}
        }
        
        # æ¸¬è©¦æ¨¡å‹é…ç½®
        self.models_config = {
            "SmolVLM2-500M-Video-Instruct": {
                "loader": VLMModelLoader.load_smolvlm2_video,
                "model_id": "HuggingFaceTB/SmolVLM2-500M-Video-Instruct"
            },
            "SmolVLM-500M-Instruct": {
                "loader": VLMModelLoader.load_smolvlm_instruct,
                "model_id": "HuggingFaceTB/SmolVLM-500M-Instruct"
            },
            "Moondream2": {
                "loader": VLMModelLoader.load_moondream2,
                "model_id": "vikhyatk/moondream2"
            },
            "LLaVA-v1.6-Mistral-7B-MLX": {
                "loader": VLMModelLoader.load_llava_mlx,
                "model_id": "mlx-community/llava-v1.6-mistral-7b-4bit",
                "note": "MLX-optimized for Apple Silicon (M1/M2/M3), requires 'pip install mlx-vlm'"
            },
            "Phi-3.5-Vision-Instruct": {
                "loader": VLMModelLoader.load_phi3_vision,
                "model_id": "lokinfey/Phi-3.5-vision-mlx-int4",
                "note": "MLX-optimized for Apple Silicon (M1/M2/M3), requires 'pip install mlx-vlm'"
            }
        }
        
        # ğŸ“ çµ±ä¸€æ¸¬è©¦æ¢ä»¶
        self.prompt = "Describe what you see in this image in detail."
        self.unified_max_tokens = 100  # çµ±ä¸€ç”Ÿæˆé•·åº¦
        self.unified_image_size = 1024  # çµ±ä¸€åœ–åƒæœ€å¤§å°ºå¯¸
    
    def get_test_images(self):
        """ç²å–æ¸¬è©¦åœ–åƒåˆ—è¡¨"""
        # æ”¯æ´å¾ä¸åŒç›®éŒ„åŸ·è¡Œç¨‹å¼
        possible_paths = [
            Path("src/testing/testing_material/images"),  # å¾å°ˆæ¡ˆæ ¹ç›®éŒ„åŸ·è¡Œ
            Path("testing_material/images"),              # å¾ src/testing ç›®éŒ„åŸ·è¡Œ
            Path("./testing_material/images")             # ç•¶å‰ç›®éŒ„
        ]
        
        images_dir = None
        for path in possible_paths:
            if path.exists():
                images_dir = path
                break
        
        if images_dir is None:
            print(f"è­¦å‘Šï¼šåœ–åƒè³‡æ–™å¤¾ä¸å­˜åœ¨ï¼Œå˜—è©¦äº†ä»¥ä¸‹è·¯å¾‘ï¼š")
            for path in possible_paths:
                print(f"  {path}")
            return []
        
        image_files = []
        for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:
            image_files.extend(images_dir.glob(ext))
            image_files.extend(images_dir.glob(ext.upper()))
        
        return sorted(image_files)
    
    def test_single_model(self, model_name, config):
        """æ¸¬è©¦å–®ä¸€æ¨¡å‹"""
        print(f"\n{'='*50}")
        print(f"é–‹å§‹æ¸¬è©¦æ¨¡å‹: {model_name}")
        print(f"{'='*50}")
        
        # è¨˜éŒ„è¼‰å…¥å‰è¨˜æ†¶é«”
        memory_before = get_memory_usage()
        print(f"è¼‰å…¥å‰è¨˜æ†¶é«”ä½¿ç”¨: {memory_before:.2f} GB")
        
        # è¼‰å…¥æ¨¡å‹
        start_time = time.time()
        try:
            model, processor = config["loader"]()
            load_time = time.time() - start_time
            
            # è¨˜éŒ„è¼‰å…¥å¾Œè¨˜æ†¶é«”
            memory_after = get_memory_usage()
            print(f"è¼‰å…¥å¾Œè¨˜æ†¶é«”ä½¿ç”¨: {memory_after:.2f} GB")
            print(f"æ¨¡å‹è¼‰å…¥æ™‚é–“: {load_time:.2f} ç§’")
            
            # åˆå§‹åŒ–æ¨¡å‹çµæœ
            model_results = {
                "model_id": config["model_id"],
                "load_time": load_time,
                "memory_before": memory_before,
                "memory_after": memory_after,
                "memory_diff": memory_after - memory_before,
                "images": {},
                "total_inference_time": 0,
                "successful_inferences": 0,
                "failed_inferences": 0
            }
            
            # ç²å–æ¸¬è©¦åœ–åƒ
            test_images = self.get_test_images()
            if not test_images:
                print("è­¦å‘Šï¼šæ²’æœ‰æ‰¾åˆ°æ¸¬è©¦åœ–åƒ")
                model_results["error"] = "No test images found"
                return model_results
            
            print(f"æ‰¾åˆ° {len(test_images)} å¼µæ¸¬è©¦åœ–åƒ")
            
            # æ¸¬è©¦æ¯å¼µåœ–åƒ
            for image_path in test_images:
                try:
                    image_result = self.test_single_image(model, processor, image_path, model_name)
                    model_results["images"][image_path.name] = image_result
                    
                    # ğŸ’¡ FIX: Improved failure detection for more accurate reporting
                    is_failure = image_result.get("error") is not None or \
                                 ("inference failed" in image_result.get("response", "").lower())
                    
                    if not is_failure:
                        model_results["successful_inferences"] += 1
                        model_results["total_inference_time"] += image_result["inference_time"]
                    else:
                        model_results["failed_inferences"] += 1
                        
                except Exception as e:
                    print(f"æ¸¬è©¦åœ–åƒ {image_path.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                    model_results["images"][image_path.name] = {
                        "error": str(e),
                        "inference_time": 0
                    }
                    model_results["failed_inferences"] += 1
            
            # è¨ˆç®—å¹³å‡æ¨ç†æ™‚é–“
            if model_results["successful_inferences"] > 0:
                model_results["avg_inference_time"] = model_results["total_inference_time"] / model_results["successful_inferences"]
            else:
                model_results["avg_inference_time"] = 0
                
            print(f"æ¨¡å‹ {model_name} æ¸¬è©¦å®Œæˆ")
            print(f"æˆåŠŸ: {model_results['successful_inferences']}, å¤±æ•—: {model_results['failed_inferences']}")
            print(f"å¹³å‡æ¨ç†æ™‚é–“: {model_results['avg_inference_time']:.2f} ç§’")
            
        except Exception as e:
            print(f"è¼‰å…¥æ¨¡å‹ {model_name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            model_results = {
                "model_id": config["model_id"],
                "load_error": str(e),
                "memory_before": memory_before,
                "memory_after": memory_before,  # è¼‰å…¥å¤±æ•—ï¼Œè¨˜æ†¶é«”ç„¡è®ŠåŒ–
                "memory_diff": 0
            }
            
            # æ¸…ç†è¨˜æ†¶é«”ï¼ˆå³ä½¿è¼‰å…¥å¤±æ•—ä¹Ÿè¦æ¸…ç†ï¼‰
            gc.collect()
            if torch.backends.mps.is_available():
                torch.mps.empty_cache()
            
            return model_results
        
        # æ¸…ç†æ¨¡å‹è¨˜æ†¶é«”
        clear_model_memory(model, processor)
        
        # æª¢æŸ¥è¨˜æ†¶é«”æ¸…ç†æ•ˆæœ
        memory_after_cleanup = get_memory_usage()
        print(f"æ¸…ç†å¾Œè¨˜æ†¶é«”ä½¿ç”¨: {memory_after_cleanup:.2f} GB")
        model_results["memory_after_cleanup"] = memory_after_cleanup
        
        return model_results
    
    def test_single_image(self, model, processor, image_path, model_name):
        """æ¸¬è©¦å–®å¼µåœ–åƒï¼ˆåŒ…å«å„ªåŒ–çš„è¶…æ™‚æ©Ÿåˆ¶ï¼‰"""
        print(f"æ¸¬è©¦åœ–åƒ: {image_path.name}")
        
        temp_image_path_for_fix = None
        try:
            # è¼‰å…¥ä¸¦å„ªåŒ–åœ–åƒ
            image = Image.open(image_path).convert('RGB')
            
            # æ ¹æ“šéœ€è¦ï¼Œè¨­å®šç•¶å‰æ¨ç†æ‡‰ä½¿ç”¨çš„åœ–åƒè·¯å¾‘
            current_image_path = str(image_path)
            
            # ğŸ“ çµ±ä¸€åœ–åƒé è™•ç†ï¼ˆæ‰€æœ‰æ¨¡å‹ä¸€è‡´ï¼‰
            original_size = image.size
            if max(image.size) > self.unified_image_size:
                ratio = self.unified_image_size / max(image.size)
                new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))
                image = image.resize(new_size, Image.Resampling.LANCZOS)
                print(f"  ğŸ“ çµ±ä¸€ç¸®æ”¾: {original_size} â†’ {image.size}")
            
            image_info = {
                "original_size": original_size,
                "processed_size": image.size,
                "mode": image.mode,
                "file_size": os.path.getsize(current_image_path)
            }
            
            # ğŸ“ çµ±ä¸€ç”Ÿæˆåƒæ•¸ï¼ˆæ‰€æœ‰æ¨¡å‹ä¸€è‡´ï¼‰
            unified_generation_params = {
                "max_new_tokens": self.unified_max_tokens,
                "do_sample": False
            }
            
            # â±ï¸ Adjust timeout based on model technical characteristics (does not affect comparison fairness)
            def get_timeout(model_name):
                if "LLaVA" in model_name:
                    return 180  # CPU inference needs more time
                elif "Phi-3.5" in model_name:
                    return 180  # Give more time for both MLX and fallback transformers
                else:
                    return 60   # Small models fast inference
            
            timeout_seconds = get_timeout(model_name)
            print(f"  ğŸ“ çµ±ä¸€åƒæ•¸: {unified_generation_params}")
            print(f"  â±ï¸ è¶…æ™‚è¨­å®š: {timeout_seconds}ç§’")
            
            # å®šç¾©æ¨ç†å‡½æ•¸
            def do_inference():
                # æ ¹æ“šæ¨¡å‹é¡å‹ä½¿ç”¨ä¸åŒçš„æ¨ç†æ–¹å¼
                if "Moondream2" in model_name:
                    # Moondream2 ç‰¹æ®Š APIï¼ˆæ¨¡å‹é™åˆ¶ï¼Œä½†ä¿æŒçµ±ä¸€æ¸¬è©¦æ¢ä»¶ï¼‰
                    # å…ˆå°‡åœ–åƒç§»å‹•åˆ°æ­£ç¢ºçš„è¨­å‚™
                    device = next(model.parameters()).device
                    enc_image = model.encode_image(image)
                    if hasattr(enc_image, 'to'):
                        enc_image = enc_image.to(device)
                    # ä½¿ç”¨çµ±ä¸€æç¤ºè©ï¼Œä½†ç„¡æ³•æ§åˆ¶ max_tokensï¼ˆAPI é™åˆ¶ï¼‰
                    return model.answer_question(enc_image, self.prompt, processor)
                elif "Phi-3.5" in model_name:
                    # Check if this is an MLX model or transformers model
                    try:
                        # Use MLX inference - it's much faster than transformers
                        
                        # Try MLX inference first
                        from mlx_vlm import generate
                        print("  ğŸš€ Using MLX inference for Phi-3.5-Vision...")
                        
                        # Try simpler prompt format that works better with quantized models
                        mlx_prompt = f"<|image_1|>\\nUser: {self.prompt}\\nAssistant:"
                        response = generate(
                            model=model, 
                            processor=processor, 
                            image=current_image_path, 
                            prompt=mlx_prompt,
                            max_tokens=unified_generation_params["max_new_tokens"],
                            temp=0.7,  # Increase temperature for more diverse output
                            repetition_penalty=1.2,  # Stronger repetition penalty
                            top_p=0.9,  # Add nucleus sampling
                            verbose=False  # Reduce MLX verbosity
                        )
                        
                        # Handle MLX response format (might be tuple with text and metadata)
                        if isinstance(response, tuple) and len(response) >= 2:
                            # MLX returns (text, metadata_dict) - extract just the text
                            text_response = response[0]
                        elif isinstance(response, list) and len(response) > 0:
                            # Extract just the text part if it's a list
                            text_response = response[0] if isinstance(response[0], str) else str(response[0])
                        else:
                            text_response = str(response)
                        
                        # Clean up repetitive tokens and unwanted text
                        text_response = text_response.replace("<|end|><|endoftext|>", " ").replace("<|end|>", " ").replace("<|endoftext|>", " ")
                        # Remove any trailing questions that might be part of the model's training data
                        if "1. What is meant by" in text_response:
                            text_response = text_response.split("1. What is meant by")[0].strip()
                        text_response = ' '.join(text_response.split())  # Clean up whitespace
                        
                        return text_response
                        
                    except (ImportError, AttributeError, TypeError, Exception) as e:
                        print(f"  âš ï¸ MLX inference failed ({e}), loading transformers model...")
                        
                        # Load transformers model for fallback (MLX model can't be used with transformers)
                        from transformers import AutoModelForCausalLM, AutoProcessor
                        print("  ğŸ“¥ Loading transformers Phi-3.5-Vision for fallback...")
                        
                        fallback_model = AutoModelForCausalLM.from_pretrained(
                            "microsoft/Phi-3.5-vision-instruct", 
                            trust_remote_code=True,
                            torch_dtype=torch.float16,
                            _attn_implementation="eager",  # Disable FlashAttention2
                            device_map="cpu",  # Force CPU to avoid memory issues
                            low_cpu_mem_usage=True  # Use less CPU memory
                        )
                        fallback_processor = AutoProcessor.from_pretrained("microsoft/Phi-3.5-vision-instruct", trust_remote_code=True)
                        
                        # Phi-3.5 Vision special format (model compatibility requirement)
                        messages = [
                            {"role": "user", "content": f"<|image_1|>\\n{self.prompt}"}
                        ]
                        
                        prompt = fallback_processor.tokenizer.apply_chat_template(
                            messages, 
                            tokenize=False, 
                            add_generation_prompt=True
                        )
                        
                        inputs = fallback_processor(prompt, [image], return_tensors="pt")
                        
                        # Move to correct device
                        device = next(fallback_model.parameters()).device
                        inputs = {k: v.to(device) for k, v in inputs.items()}
                        
                        # Technical fix: avoid DynamicCache error
                        with torch.no_grad():
                            outputs = fallback_model.generate(
                                **inputs, 
                                **unified_generation_params,  # Use unified parameters
                                use_cache=False,  # Disable cache to avoid DynamicCache error
                                pad_token_id=fallback_processor.tokenizer.eos_token_id
                            )
                        
                        result = fallback_processor.decode(outputs[0], skip_special_tokens=True)
                        
                        # Clean up fallback model
                        del fallback_model, fallback_processor
                        gc.collect()
                        if torch.backends.mps.is_available():
                            torch.mps.empty_cache()
                        
                        return result
                elif "LLaVA" in model_name:
                    # Check if this is MLX-LLaVA or standard LLaVA
                    if "MLX" in model_name:
                        # MLX-LLaVA inference
                        try:
                            from mlx_vlm import generate
                            print("  ğŸš€ Using MLX-VLM for LLaVA...")
                            # Simple prompt for MLX-LLaVA
                            response = generate(
                                model, 
                                processor, 
                                self.prompt, 
                                image=current_image_path,
                                max_tokens=unified_generation_params["max_new_tokens"],
                                verbose=False
                            )
                            
                            # Handle MLX-VLM response format (tuple with text and metadata)
                            if isinstance(response, tuple) and len(response) >= 1:
                                # Extract just the text part
                                text_response = response[0] if response[0] else ""
                            else:
                                text_response = str(response) if response else ""
                            
                            return text_response
                        except Exception as e:
                            print(f"  âš ï¸ MLX-VLM failed: {e}")
                            # Fallback: Return descriptive error but don't crash
                            return f"MLX-VLM inference failed: {str(e)}"
                    else:
                        # Standard LLaVA Pipeline æ–¹å¼
                        messages = [
                            {
                                "role": "user",
                                "content": [
                                    {"type": "image", "image": image},  # ä½¿ç”¨æœ¬åœ°åœ–åƒæ ¼å¼ï¼ˆèˆ‡ SmolVLM ä¸€è‡´ï¼‰
                                    {"type": "text", "text": self.prompt}  # ä½¿ç”¨çµ±ä¸€æç¤ºè©
                                ]
                            },
                        ]
                        # ğŸš€ å„ªåŒ–ï¼šæ·»åŠ ç”Ÿæˆåƒæ•¸æ§åˆ¶
                        response = model(
                            text=messages, 
                            **unified_generation_params,  # ä½¿ç”¨çµ±ä¸€åƒæ•¸
                            return_full_text=False  # åªè¿”å›ç”Ÿæˆéƒ¨åˆ†
                        )
                        if isinstance(response, list) and len(response) > 0:
                            return response[0].get('generated_text', str(response))
                        else:
                            return str(response)
                elif "SmolVLM" in model_name:
                    # SmolVLM å„ªåŒ–æ–¹å¼
                    messages = [
                        {
                            "role": "user",
                            "content": [
                                {"type": "image", "image": image},
                                {"type": "text", "text": self.prompt}
                            ]
                        }
                    ]
                    input_text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
                    inputs = processor(text=input_text, images=image, return_tensors="pt")
                    with torch.no_grad():
                        outputs = model.generate(**inputs, **unified_generation_params)  # ä½¿ç”¨çµ±ä¸€åƒæ•¸
                    return processor.decode(outputs[0], skip_special_tokens=True)
                else:
                    # å‚³çµ±æ–¹å¼
                    inputs = processor(text=self.prompt, images=image, return_tensors="pt")
                    with torch.no_grad():
                        outputs = model.generate(**inputs, **unified_generation_params)  # ä½¿ç”¨çµ±ä¸€åƒæ•¸
                    return processor.decode(outputs[0], skip_special_tokens=True)
            
            # â±ï¸ åŸ·è¡Œå¸¶è¶…æ™‚çš„æ¨ç†
            start_time = time.time()
            try:
                response_text = run_with_timeout(do_inference, timeout_seconds=timeout_seconds)
                if response_text is None:
                    response_text = ""
                    
                inference_time = time.time() - start_time
                
                # ğŸ’¡ FIX: Properly separate successful responses from error messages
                error_message = None
                final_response = response_text
                if "inference failed" in response_text.lower():
                    error_message = response_text
                    final_response = ""  # Response should be empty on error
                    print(f"  âŒ Detected inference failure: {error_message}")

                result = {
                    "inference_time": inference_time,
                    "response": final_response,
                    "image_info": image_info,
                    "error": error_message,  # Correctly populate the error field
                    "unified_test": True,
                    "generation_params": unified_generation_params,
                    "timeout_used": timeout_seconds
                }
                
                print(f"  âœ… æ¨ç†æ™‚é–“: {inference_time:.2f} ç§’")
                print(f"  ğŸ“ å›æ‡‰é•·åº¦: {len(final_response)} å­—å…ƒ")
                
                return result
                
            except TimeoutError as e:
                inference_time = time.time() - start_time
                print(f"  âš ï¸ {str(e)}")
                return {
                    "inference_time": inference_time,
                    "response": "",
                    "image_info": image_info,
                    "error": str(e),
                    "unified_test": True  # æ¨™è¨˜ä½¿ç”¨çµ±ä¸€æ¸¬è©¦æ¢ä»¶
                }
            
        except Exception as e:
            print(f"  âŒ éŒ¯èª¤: {str(e)}")
            return {
                "inference_time": 0,
                "response": "",
                "image_info": {},
                "error": str(e),
                "unified_test": True  # æ¨™è¨˜ä½¿ç”¨çµ±ä¸€æ¸¬è©¦æ¢ä»¶ï¼ˆå³ä½¿å¤±æ•—ï¼‰
            }
        finally:
            # æ¸…ç†è‡¨æ™‚æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if temp_image_path_for_fix:
                os.remove(temp_image_path_for_fix)
    
    def run_all_tests(self):
        """åŸ·è¡Œæ‰€æœ‰æ¨¡å‹çš„æ¸¬è©¦"""
        print("é–‹å§‹ VLM æ¨¡å‹æ¸¬è©¦")
        print(f"ç³»çµ±è³‡è¨Š: MacBook Air M3, 16GB")
        print(f"MPS å¯ç”¨: {torch.backends.mps.is_available()}")
        
        # æª¢æŸ¥æ¸¬è©¦åœ–åƒ
        test_images = self.get_test_images()
        print(f"æ‰¾åˆ° {len(test_images)} å¼µæ¸¬è©¦åœ–åƒ")
        
        if not test_images:
            print("éŒ¯èª¤ï¼šæ²’æœ‰æ‰¾åˆ°æ¸¬è©¦åœ–åƒï¼Œè«‹å°‡åœ–åƒæ”¾ç½®æ–¼ src/testing/testing_material/images/")
            return
        
        total_start_time = time.time()
        
        # é€ä¸€æ¸¬è©¦æ¯å€‹æ¨¡å‹
        for model_name, config in self.models_config.items():
            try:
                model_results = self.test_single_model(model_name, config)
                self.results["models"][model_name] = model_results
                
                # å„²å­˜ä¸­é–“çµæœï¼ˆé˜²æ­¢æ¸¬è©¦ä¸­æ–·ä¸Ÿå¤±æ•¸æ“šï¼‰
                self.save_results(f"intermediate_{model_name}")
                
            except Exception as e:
                print(f"æ¸¬è©¦æ¨¡å‹ {model_name} æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {str(e)}")
                self.results["models"][model_name] = {
                    "severe_error": str(e)
                }
        
        # è¨˜éŒ„ç¸½æ¸¬è©¦æ™‚é–“
        total_time = time.time() - total_start_time
        self.results["total_test_time"] = total_time
        print(f"\næ‰€æœ‰æ¸¬è©¦å®Œæˆï¼Œç¸½æ™‚é–“: {total_time:.2f} ç§’")
        
        # å„²å­˜æœ€çµ‚çµæœ
        self.save_results()
    
    def save_results(self, suffix=""):
        """å„²å­˜æ¸¬è©¦çµæœ"""
        # æ”¯æ´å¾ä¸åŒç›®éŒ„åŸ·è¡Œç¨‹å¼
        possible_results_dirs = [
            Path("src/testing/results"),  # å¾å°ˆæ¡ˆæ ¹ç›®éŒ„åŸ·è¡Œ
            Path("results"),              # å¾ src/testing ç›®éŒ„åŸ·è¡Œ
            Path("./results")             # ç•¶å‰ç›®éŒ„
        ]
        
        # ä½¿ç”¨ç¬¬ä¸€å€‹å¯è¡Œçš„è·¯å¾‘ï¼Œå¦‚æœéƒ½ä¸å­˜åœ¨å‰‡å‰µå»ºç¬¬äºŒå€‹
        results_dir = possible_results_dirs[1] if Path("testing_material").exists() else possible_results_dirs[0]
        results_dir.mkdir(parents=True, exist_ok=True)
        
        if suffix:
            filename = f"test_results_{suffix}.json"
        else:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"test_results_{timestamp}.json"
        
        filepath = results_dir / filename
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2)
            print(f"çµæœå·²å„²å­˜è‡³: {filepath}")
        except Exception as e:
            print(f"å„²å­˜çµæœæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")

def main():
    """ä¸»å‡½æ•¸"""
    print("VLM æ¨¡å‹æ¸¬è©¦ç¨‹å¼")
    print("="*50)
    
    # æª¢æŸ¥è™›æ“¬ç’°å¢ƒ
    activate_virtual_env()
    
    # å»ºç«‹æ¸¬è©¦å™¨
    tester = VLMTester()
    
    # æª¢æŸ¥å‘½ä»¤è¡Œåƒæ•¸æ˜¯å¦è¦æ¸¬è©¦å–®ä¸€æ¨¡å‹
    import sys
    if len(sys.argv) > 1:
        model_name = sys.argv[1]
        if model_name in tester.models_config:
            print(f"æ¸¬è©¦å–®ä¸€æ¨¡å‹: {model_name}")
            model_results = tester.test_single_model(model_name, tester.models_config[model_name])
            tester.results["models"][model_name] = model_results
            tester.save_results(f"single_{model_name}")
        else:
            print(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æ¨¡å‹ {model_name}")
            print(f"å¯ç”¨çš„æ¨¡å‹ï¼š{list(tester.models_config.keys())}")
            return
    else:
        # åŸ·è¡Œæ‰€æœ‰æ¨¡å‹æ¸¬è©¦
        tester.run_all_tests()
    
    print("\næ¸¬è©¦å®Œæˆï¼")
    print("çµæœæ–‡ä»¶ä½æ–¼: src/testing/results/")

if __name__ == "__main__":
    main() 