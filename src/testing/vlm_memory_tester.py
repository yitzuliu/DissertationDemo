#!/usr/bin/env python3
"""
VLM æ¨¡å‹çŸ­æœŸå°è©±è¨˜æ†¶æ¸¬è©¦ç¨‹å¼
MacBook Air M3 (16GB) å„ªåŒ–ç‰ˆæœ¬

æ¸¬è©¦æµç¨‹:
1.  **è¨˜æ†¶æ¤å…¥ (Memory Seeding)**: å‘æ¨¡å‹å±•ç¤ºä¸€å¼µåœ–ç‰‡å’Œä¸€å€‹è©³ç´°çš„æç¤ºè©ï¼Œè¦æ±‚å…¶è©³ç´°æè¿°ã€‚
2.  **è¨˜æ†¶å›æº¯ (Memory Recall)**: ä¸å†æ¬¡å±•ç¤ºåœ–ç‰‡ï¼Œé€£çºŒå•ä¸‰å€‹é—œæ–¼å‰›å‰›åœ–ç‰‡å…§å®¹çš„é€šç”¨æ€§å•é¡Œã€‚
3.  **å¾ªç’°**: ç‚ºæ¯å€‹æ¨¡å‹çš„æ¯å¼µæ¸¬è©¦åœ–ç‰‡é‡è¤‡ä»¥ä¸Šæ­¥é©Ÿã€‚
4.  **è¼¸å‡º**: å°‡å®Œæ•´çš„å°è©±æ­·å²è¨˜éŒ„ç‚º JSON æª”æ¡ˆã€‚
"""

import os
import time
import json
import gc
import psutil
from datetime import datetime
from pathlib import Path
from PIL import Image
import torch

# å¼•å…¥ vlm_tester.py ä¸­çš„æ¨¡å‹è¼‰å…¥å™¨å’Œè¼”åŠ©å‡½æ•¸
# ç‚ºäº†ä¿æŒç¨‹å¼ç¢¼ç¨ç«‹æ€§ï¼Œæˆ‘å€‘ç›´æ¥å¾è©²æª”æ¡ˆè¤‡è£½å¿…è¦çµ„ä»¶
# (åœ¨å¯¦éš›å°ˆæ¡ˆä¸­ï¼Œå¯èƒ½æœƒå°‡é€™äº›å…±äº«çµ„ä»¶é‡æ§‹ç‚ºç¨ç«‹æ¨¡çµ„)

# --- å¾ vlm_tester.py è¤‡è£½çš„è¼”åŠ©å‡½æ•¸ ---

def get_memory_usage():
    """ç²å–ç•¶å‰è¨˜æ†¶é«”ä½¿ç”¨é‡ï¼ˆGBï¼‰"""
    process = psutil.Process()
    memory_info = process.memory_info()
    return memory_info.rss / (1024 ** 3)

def clear_model_memory(model, processor):
    """æ¸…ç†æ¨¡å‹è¨˜æ†¶é«”"""
    print("ğŸ§¹ æ¸…ç†æ¨¡å‹è¨˜æ†¶é«”...")
    del model, processor
    gc.collect()
    if torch.backends.mps.is_available():
        torch.mps.empty_cache()
    time.sleep(2)

# --- å¾ vlm_tester.py è¤‡è£½çš„æ¨¡å‹è¼‰å…¥å™¨ ---

class VLMModelLoader:
    """VLM æ¨¡å‹è¼‰å…¥å™¨ - èˆ‡ vlm_tester.py ä¿æŒä¸€è‡´"""
    
    @staticmethod
    def load_smolvlm2_video(model_id="HuggingFaceTB/SmolVLM2-500M-Video-Instruct"):
        from transformers import AutoProcessor, AutoModelForImageTextToText
        print(f"è¼‰å…¥ {model_id}...")
        processor = AutoProcessor.from_pretrained(model_id)
        model = AutoModelForImageTextToText.from_pretrained(model_id)
        return model, processor
    
    @staticmethod
    def load_smolvlm_instruct(model_id="HuggingFaceTB/SmolVLM-500M-Instruct"):
        from transformers import AutoProcessor, AutoModelForVision2Seq
        print(f"è¼‰å…¥ {model_id}...")
        processor = AutoProcessor.from_pretrained(model_id)
        model = AutoModelForVision2Seq.from_pretrained(model_id)
        return model, processor
    
    @staticmethod
    def load_moondream2(model_id="vikhyatk/moondream2"):
        from transformers import AutoModelForCausalLM, AutoTokenizer
        print(f"è¼‰å…¥ {model_id}...")
        model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)
        tokenizer = AutoTokenizer.from_pretrained(model_id)
        if torch.backends.mps.is_available():
            model = model.to('mps')
        return model, tokenizer
    
    @staticmethod
    def load_llava_mlx(model_id="mlx-community/llava-v1.6-mistral-7b-4bit"):
        print(f"è¼‰å…¥ MLX-LLaVA {model_id}...")
        try:
            from mlx_vlm import load
            model, processor = load(model_id)
            print("MLX-LLaVA è¼‰å…¥æˆåŠŸ!")
            return model, processor
        except ImportError:
            raise RuntimeError("MLX-VLM å¥—ä»¶æœªå®‰è£ (pip install mlx-vlm)ï¼Œç„¡æ³•æ¸¬è©¦æ­¤æ¨¡å‹ã€‚")
    
    @staticmethod
    def load_phi3_vision(model_id="lokinfey/Phi-3.5-vision-mlx-int4"):
        print(f"è¼‰å…¥ MLX Phi-3.5-Vision {model_id}...")
        try:
            from mlx_vlm import load
            model, processor = load(model_id, trust_remote_code=True)
            print("MLX Phi-3.5-Vision è¼‰å…¥æˆåŠŸ!")
            return model, processor
        except ImportError:
            raise RuntimeError("MLX-VLM å¥—ä»¶æœªå®‰è£ (pip install mlx-vlm)ï¼Œç„¡æ³•æ¸¬è©¦æ­¤æ¨¡å‹ã€‚")

# --- æ–°çš„è¨˜æ†¶åŠ›æ¸¬è©¦æ ¸å¿ƒç¨‹å¼ ---

class VLMMemoryTester:
    """VLM è¨˜æ†¶åŠ›æ¸¬è©¦å™¨"""
    
    def __init__(self):
        self.results = {
            "test_timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "system_info": {
                "device": "MacBook Air M3",
                "memory": "16GB",
                "mps_available": torch.backends.mps.is_available()
            },
            "models": {}
        }
        
        # æ¸¬è©¦æ¨¡å‹é…ç½® (èˆ‡ vlm_tester.py ç›¸åŒ)
        self.models_config = {
            "SmolVLM2-500M-Video-Instruct": { "loader": VLMModelLoader.load_smolvlm2_video, "model_id": "HuggingFaceTB/SmolVLM2-500M-Video-Instruct" },
            "SmolVLM-500M-Instruct": { "loader": VLMModelLoader.load_smolvlm_instruct, "model_id": "HuggingFaceTB/SmolVLM-500M-Instruct" },
            "Moondream2": { "loader": VLMModelLoader.load_moondream2, "model_id": "vikhyatk/moondream2" },
            "LLaVA-v1.6-Mistral-7B-MLX": { "loader": VLMModelLoader.load_llava_mlx, "model_id": "mlx-community/llava-v1.6-mistral-7b-4bit" },
            "Phi-3.5-Vision-Instruct": { "loader": VLMModelLoader.load_phi3_vision, "model_id": "lokinfey/Phi-3.5-vision-mlx-int4" }
        }
        
        # ğŸ“ çµ±ä¸€æ¸¬è©¦æ¢ä»¶
        self.unified_max_tokens = 250
        self.unified_image_size = 1024
        
        # ğŸ§  è¨˜æ†¶åŠ›æ¸¬è©¦æç¤ºè©
        self.seeding_prompt = "You are a forensic expert. Describe this image in extreme detail, listing every object, person, color, and spatial relationship you can identify. This is for a critical investigation."
        self.recall_questions = [
            "Based on the description you just gave me, what were the most prominent colors in the image?",
            "Were there any people visible in the image? If so, describe their general appearance or clothing without making up details.",
            "Summarize the main subject or scene of the image in one sentence."
        ]
        
        # ğŸ’¡ æ ¸å¿ƒæ¦‚å¿µä¿®æ­£ (ID: fix-concept)
        # å»ºç«‹ä¸€å€‹ç´”é»‘è‰²çš„ç©ºç™½åœ–ç‰‡ï¼Œç”¨æ–¼åœ¨è¿½å•éšæ®µæ»¿è¶³ processor çš„æŠ€è¡“è¦æ±‚ï¼ŒåŒæ™‚ä¸æ´©æ¼åŸå§‹åœ–åƒè³‡è¨Š
        self.black_image = Image.new('RGB', (224, 224), 'black')

    def get_test_images(self):
        """ç²å–æ¸¬è©¦åœ–åƒåˆ—è¡¨ (èˆ‡ vlm_tester.py ç›¸åŒ)"""
        possible_paths = [
            Path("src/testing/testing_material/images"),
            Path("testing_material/images"),
            Path("./testing_material/images")
        ]
        images_dir = next((path for path in possible_paths if path.exists()), None)
        if not images_dir:
            return []
        
        image_files = []
        for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:
            image_files.extend(images_dir.glob(ext))
            image_files.extend(images_dir.glob(ext.upper()))
        return sorted(image_files)

    def run_all_tests(self):
        """åŸ·è¡Œæ‰€æœ‰æ¨¡å‹çš„è¨˜æ†¶åŠ›æ¸¬è©¦"""
        print("="*60)
        print("ğŸ¤– é–‹å§‹ VLM æ¨¡å‹çŸ­æœŸå°è©±è¨˜æ†¶æ¸¬è©¦ ğŸ¤–")
        print("="*60)
        
        test_images = self.get_test_images()
        if not test_images:
            print("âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æ¸¬è©¦åœ–åƒã€‚è«‹ç¢ºä¿åœ–åƒä½æ–¼ src/testing/testing_material/images/")
            return

        print(f"ğŸ–¼ï¸ æ‰¾åˆ° {len(test_images)} å¼µæ¸¬è©¦åœ–åƒã€‚")
        total_start_time = time.time()

        for model_name, config in self.models_config.items():
            self.test_single_model(model_name, config, test_images)
        
        total_time = time.time() - total_start_time
        self.results["total_test_time"] = total_time
        print(f"\nâœ… æ‰€æœ‰æ¸¬è©¦å®Œæˆï¼Œç¸½æ™‚é–“: {total_time:.2f} ç§’")
        self.save_results()

    def test_single_model(self, model_name, config, test_images):
        """æ¸¬è©¦å–®ä¸€æ¨¡å‹"""
        print(f"\n{'='*50}\nğŸ”¬ é–‹å§‹æ¸¬è©¦æ¨¡å‹: {model_name}\n{'='*50}")
        
        memory_before = get_memory_usage()
        print(f"è¼‰å…¥å‰è¨˜æ†¶é«”: {memory_before:.2f} GB")
        
        model_results = {
            "model_id": config["model_id"],
            "image_tests": []
        }
        
        try:
            start_time = time.time()
            model, processor = config["loader"]()
            load_time = time.time() - start_time
            
            memory_after = get_memory_usage()
            print(f"è¼‰å…¥å¾Œè¨˜æ†¶é«”: {memory_after:.2f} GB (è€—æ™‚ {load_time:.2f} ç§’)")
            
            model_results.update({
                "load_time": load_time,
                "memory_before": memory_before,
                "memory_after": memory_after,
                "memory_diff": memory_after - memory_before
            })

            for image_path in test_images:
                print(f"\n--- æ¸¬è©¦åœ–åƒ: {image_path.name} ---")
                image_test_result = self.run_conversation_test(model, processor, image_path, model_name)
                model_results["image_tests"].append(image_test_result)
            
        except Exception as e:
            print(f"âŒ æ¸¬è©¦æ¨¡å‹ {model_name} æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
            model_results["error"] = str(e)
        
        finally:
            if 'model' in locals():
                clear_model_memory(model, processor)
                memory_after_cleanup = get_memory_usage()
                print(f"æ¸…ç†å¾Œè¨˜æ†¶é«”: {memory_after_cleanup:.2f} GB")
                model_results["memory_after_cleanup"] = memory_after_cleanup

            self.results["models"][model_name] = model_results
            self.save_results(f"intermediate_{model_name}")

    def run_conversation_test(self, model, processor, image_path, model_name):
        """åŸ·è¡Œå®Œæ•´çš„å–®åœ–å°è©±æ¸¬è©¦ï¼ˆæ¤å…¥ -> è¿½å•ï¼‰"""
        
        image_test_result = {
            "image_name": image_path.name,
            "seeding_phase": {},
            "recall_phase": {},
            "full_conversation_history": [],
            "debug_info": {} # ç”¨æ–¼å„²å­˜é¡å¤–çš„èª¿è©¦è³‡è¨Š
        }
        
        # æº–å‚™åœ–åƒ
        image = Image.open(image_path).convert('RGB')
        
        # LLaVA çš„ç‹€æ…‹å•é¡Œéœ€è¦æ¯æ¬¡éƒ½é‡æ–°è¼‰å…¥æ¨¡å‹ä¾†è§£æ±º
        if "LLaVA" in model_name:
            print("  >> LLaVA-MLX: Reloading model to clear state before new conversation...")
            try:
                config = self.models_config[model_name]
                # ç¢ºä¿èˆŠæ¨¡å‹å’Œè™•ç†å™¨è¢«æ­£ç¢ºæ¸…ç†
                if 'model' in locals() and model is not None:
                    clear_model_memory(model, processor)
                model, processor = config["loader"]()
                print("  >> LLaVA-MLX: Reload successful.")
            except Exception as e:
                print(f"  >> LLaVA-MLX: Reload failed: {e}")
                image_test_result["error"] = f"LLaVA reload failed: {e}"
                return image_test_result

        # æ­¥é©Ÿä¸€: è¨˜æ†¶æ¤å…¥
        print("1ï¸âƒ£  æ­¥é©Ÿ 1: è¨˜æ†¶æ¤å…¥ (Seeding)")
        seeding_response, seeding_time, conversation_history = self.run_inference(
            model, processor, model_name, self.seeding_prompt, image=image, history=[]
        )
        image_test_result["seeding_phase"] = {
            "prompt": self.seeding_prompt,
            "response": seeding_response,
            "inference_time": seeding_time
        }
        image_test_result["full_conversation_history"] = conversation_history
        image_test_result["debug_info"]["seeding_prompt"] = conversation_history[-2] if len(conversation_history) > 1 else {}

        print(f"    - å›æ‡‰ (å‰100å­—): {seeding_response[:100].strip()}...")
        print(f"    - æ¨ç†æ™‚é–“: {seeding_time:.2f} ç§’")
        
        # å¦‚æœæ¤å…¥å¤±æ•—ï¼Œæå‰çµ‚æ­¢
        if "éŒ¯èª¤ï¼š" in seeding_response:
            print("    - æ¤å…¥å¤±æ•—ï¼Œè·³éè¿½å•ã€‚")
            return image_test_result

        # æ­¥é©ŸäºŒ: è¨˜æ†¶å›æº¯
        print("\n2ï¸âƒ£  æ­¥é©Ÿ 2: è¨˜æ†¶å›æº¯ (Recall)")
        for i, question in enumerate(self.recall_questions):
            print(f"  - è¿½å• {i+1}/3: {question}")
            
            # ä¾è³´ run_inference å…§éƒ¨é‚è¼¯ä¾†æ­£ç¢ºè™•ç†æ­·å²å’Œåœ–ç‰‡
            recall_response, recall_time, conversation_history = self.run_inference(
                model, processor, model_name, question, image=image, history=conversation_history
            )
            
            image_test_result["recall_phase"][f"question_{i+1}"] = {
                "prompt": question,
                "response": recall_response,
                "inference_time": recall_time
            }
            image_test_result["full_conversation_history"] = conversation_history
            image_test_result["debug_info"][f"recall_prompt_{i+1}"] = conversation_history[-2] if len(conversation_history) > 1 else {}
            
            print(f"    - å›æ‡‰: {recall_response.strip()}")
            print(f"    - æ¨ç†æ™‚é–“: {recall_time:.2f} ç§’")
            
        return image_test_result

    def run_inference(self, model, processor, model_name, prompt, image, history):
        """
        é€šç”¨æ¨ç†å‡½æ•¸ï¼ˆå·²é‡æ§‹ï¼‰
        ç‚ºæ¯ä¸€é¡æ¨¡å‹å¯¦ç¾äº†å„è‡ªæ­£ç¢ºçš„ã€ç¬¦åˆå…¶å®˜æ–¹ç¯„ä¾‹çš„å°è©±ç‹€æ…‹ç®¡ç†æ–¹æ³•ã€‚
        """
        start_time = time.time()
        response = ""
        new_history = list(history)
        
        try:
            # --------------------------------------------------------------------------
            # åˆ†æ”¯ 1: æ¨™æº– Transformers æ¨¡å‹ (e.g., SmolVLM)
            # --------------------------------------------------------------------------
            if "SmolVLM" in model_name:
                # é—œéµä¿®æ­£ (ID: fix-smolvlm)
                is_seeding_prompt = not history
                
                content = [{"type": "text", "text": prompt}]
                if is_seeding_prompt:
                    content.insert(0, {"type": "image"}) # åƒ…åœ¨ç¬¬ä¸€è¼ªæ·»åŠ åœ–åƒä½”ä½ç¬¦
                
                new_history.append({"role": "user", "content": content})
                final_prompt = processor.apply_chat_template(new_history, tokenize=False, add_generation_prompt=True)

                # æ ¹æ“šéšæ®µé¸æ“‡ä½¿ç”¨çš„åœ–ç‰‡ï¼šç¬¬ä¸€æ¬¡ç”¨çœŸå¯¦åœ–ç‰‡ï¼Œè¿½å•ç”¨é»‘ç•«é¢
                image_to_use = image if is_seeding_prompt else self.black_image
                
                inputs = processor(text=final_prompt, images=image_to_use, return_tensors="pt")
                with torch.no_grad():
                    outputs = model.generate(**inputs, max_new_tokens=self.unified_max_tokens, do_sample=False)
                
                input_len = inputs["input_ids"].shape[1]
                generated_ids = outputs[0][input_len:]
                response = processor.decode(generated_ids, skip_special_tokens=True).strip()
                
                # æ›´æ–°æ­·å²è¨˜éŒ„
                new_history.append({"role": "assistant", "content": response})

            # --------------------------------------------------------------------------
            # åˆ†æ”¯ 2: Moondream2 (ç‰¹æ®Š API - ä¸Šä¸‹æ–‡æ³¨å…¥ç­–ç•¥)
            # --------------------------------------------------------------------------
            elif "Moondream2" in model_name:
                # é—œéµä¿®æ­£ (ID: fix-moondream)
                enc_image = model.encode_image(image) # å§‹çµ‚ä½¿ç”¨åŸå§‹åœ–ç‰‡çš„åµŒå…¥
                is_seeding_prompt = not history

                if is_seeding_prompt:
                    final_prompt = prompt
                else:
                    last_response = ""
                    if history and history[-1]["role"] == "assistant":
                        last_response = history[-1]["content"]
                    final_prompt = f"Based on your previous description of an image which was: '{last_response}'. Now, please answer this new question: {prompt}"
                
                response = model.answer_question(enc_image, final_prompt, processor)
                new_history.append({"role": "user", "content": prompt})
                new_history.append({"role": "assistant", "content": response})

            # --------------------------------------------------------------------------
            # åˆ†æ”¯ 3: MLX æ¨¡å‹ (e.g., LLaVA, Phi-3.5 - å®˜æ–¹å°è©±æ¨¡æ¿ç­–ç•¥)
            # --------------------------------------------------------------------------
            elif "LLaVA" in model_name or "Phi-3.5" in model_name:
                # é—œéµä¿®æ­£ (ID: fix-mlx)
                from mlx_vlm import generate
                
                is_seeding_prompt = not history
                
                # 1. æ§‹å»ºæ¨™æº– messages åˆ—è¡¨
                current_turn = {"role": "user", "content": prompt}
                if is_seeding_prompt and "Phi-3.5" in model_name:
                    current_turn["content"] = f"<|image_1|>\n{prompt}"
                
                messages_for_template = history + [current_turn]
                
                # 2. ä½¿ç”¨å®˜æ–¹æ¨¡æ¿ç”Ÿæˆ final_prompt
                final_prompt = processor.tokenizer.apply_chat_template(
                    messages_for_template, 
                    tokenize=False, 
                    add_generation_prompt=True
                )

                gen_kwargs = {
                    "model": model,
                    "processor": processor,
                    "prompt": final_prompt,
                    "max_tokens": self.unified_max_tokens,
                    "verbose": False
                }
                
                # MLX è¿½å•æ™‚ï¼Œå®Œå…¨ä¸æä¾› image åƒæ•¸
                if is_seeding_prompt:
                    temp_image_path = "temp_mlx_image.png"
                    image.save(temp_image_path)
                    gen_kwargs["image"] = temp_image_path
                
                raw_response = generate(**gen_kwargs)
                
                if 'image' in gen_kwargs and os.path.exists(gen_kwargs['image']):
                    os.remove(gen_kwargs['image'])

                # é—œéµä¿®æ­£ (ID: fix-mlx-tuple-bug) - æ­£ç¢ºè™•ç† tuple ä¸¦æ¸…ç† response
                if isinstance(raw_response, tuple):
                    response_text = raw_response[0]
                else:
                    response_text = str(raw_response)

                # mlx_vlm.generate åªè¿”å›ç”Ÿæˆçš„éƒ¨åˆ†ï¼Œä¸éœ€è¦å¾å®Œæ•´ prompt ä¸­æ›¿æ›
                # æˆ‘å€‘åªéœ€è¦æ¸…ç†å¯èƒ½æ®˜ç•™çš„ç‰¹æ®Š token
                stop_tokens = ["<|end|>", "<|endoftext|>", "ASSISTANT:", "USER:", "<|im_end|>"]
                clean_response = response_text
                for token in stop_tokens:
                    clean_response = clean_response.replace(token, "")
                response = clean_response.strip()
                
                # æ›´æ–°æ­·å²è¨˜éŒ„
                new_history.append(current_turn)
                new_history.append({"role": "assistant", "content": response})
            
            else:
                response = "éŒ¯èª¤ï¼šæœªçŸ¥çš„æ¨¡å‹é¡å‹ï¼Œç„¡æ³•é€²è¡Œæ¨ç†ã€‚"
            
            # è±å¯Œ debug_info (ID: refine-json)
            if not new_history[-2].get('debug_info'):
                 new_history[-2]['debug_info'] = {}
            new_history[-2]['debug_info']['final_prompt_to_model'] = final_prompt
            if 'image_to_use' in locals() and image_to_use == self.black_image:
                 new_history[-2]['debug_info']['used_black_image_test'] = True


        except Exception as e:
            import traceback
            print(traceback.format_exc())
            response = f"éŒ¯èª¤ï¼šæ¨ç†éç¨‹ä¸­ç™¼ç”Ÿç•°å¸¸ - {str(e)}"
            # ç™¼ç”ŸéŒ¯èª¤æ™‚ï¼Œä¹Ÿè¦æ›´æ–°æ­·å²è¨˜éŒ„ä»¥åæ˜ éŒ¯èª¤
            if not new_history or new_history[-1]['role'] != 'user':
                 new_history.append({"role": "user", "content": prompt})
            new_history.append({"role": "assistant", "content": response})

        inference_time = time.time() - start_time
        return response, inference_time, new_history

    def save_results(self, suffix=""):
        """å„²å­˜æ¸¬è©¦çµæœ"""
        # æ”¯æ´å¾ä¸åŒç›®éŒ„åŸ·è¡Œç¨‹å¼
        possible_results_dirs = [
            Path("src/testing/results"),  # å¾å°ˆæ¡ˆæ ¹ç›®éŒ„åŸ·è¡Œ
            Path("results"),              # å¾ src/testing ç›®éŒ„åŸ·è¡Œ
            Path("./results")             # ç•¶å‰ç›®éŒ„
        ]
        
        # ä½¿ç”¨ç¬¬ä¸€å€‹å¯è¡Œçš„è·¯å¾‘
        results_dir = next((path for path in possible_results_dirs if path.is_dir() or path.parent.is_dir()), Path("results"))
        results_dir.mkdir(parents=True, exist_ok=True)
        
        if suffix:
            filename = f"memory_test_results_{suffix}.json"
        else:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"memory_test_results_{timestamp}.json"
        
        filepath = results_dir / filename
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                # ä½¿ç”¨è‡ªå®šç¾©åºåˆ—åŒ–å™¨ä¾†è™•ç† Path å°è±¡
                def path_serializer(obj):
                    if isinstance(obj, Path):
                        return str(obj)
                    raise TypeError(f"Object of type {obj.__class__.__name__} is not JSON serializable")
                json.dump(self.results, f, ensure_ascii=False, indent=2, default=path_serializer)
            print(f"\nğŸ’¾ çµæœå·²å„²å­˜è‡³: {filepath}")
        except Exception as e:
            print(f"âŒ å„²å­˜çµæœæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

def main():
    """ä¸»å‡½æ•¸"""
    tester = VLMMemoryTester()
    tester.run_all_tests()
    print("\nè¨˜æ†¶åŠ›æ¸¬è©¦å®Œæˆï¼")

if __name__ == "__main__":
    main() 