# üéØ Complete Model Configuration Guide

## ‚úÖ **Correct Model Names for `app_config.json`**

When you want to change the model, use these **exact names** in `src/config/app_config.json`:

### **Available Models:**

| Model Name in `app_config.json` | Run Script | Description |
|----------------------------------|------------|-------------|
| `"moondream2"` | `src/models/moondream2/run_moondream2.py` | Standard Moondream2 |
| `"moondream2_optimized"` | `src/models/moondream2/run_moondream2_optimized.py` | Optimized Moondream2 |
| `"phi3_vision"` | `src/models/phi3_vision_mlx/run_phi3_vision.py` | Standard Phi-3.5-Vision |
| `"phi3_vision_optimized"` | `src/models/phi3_vision_mlx/run_phi3_vision_optimized.py` | Optimized Phi-3.5-Vision |
| `"llava_mlx"` | `src/models/llava_mlx/run_llava_mlx.py` | LLaVA with MLX |
| `"smolvlm"` | `src/models/smolvlm/run_smolvlm.py` | Standard SmolVLM |
| `"smolvlm2_500m_video"` | `src/models/smolvlm2/run_smolvlm2_500m_video.py` | SmolVLM2 Video |
| `"smolvlm2_500m_video_optimized"` | `src/models/smolvlm2/run_smolvlm2_500m_video_optimized.py` | SmolVLM2 Video Optimized |

## üîÑ **How to Switch Models**

### **Step 1: Update Configuration**
Edit `src/config/app_config.json`:
```json
{
  "active_model": "phi3_vision_optimized"
}
```

### **Step 2: Start the Correct Model Server**
```bash
# For phi3_vision_optimized
cd src/models/phi3_vision_mlx
python run_phi3_vision_optimized.py

# For moondream2_optimized  
cd src/models/moondream2
python run_moondream2_optimized.py

# For smolvlm2_500m_video_optimized
cd src/models/smolvlm2
python run_smolvlm2_500m_video_optimized.py
```

### **Step 3: Start Backend**
```bash
cd src/backend
python main.py
```

### **Step 4: Start Frontend**
```bash
cd src/frontend
python -m http.server 5500
```

## üõ†Ô∏è **Quick Start with Helper Script**

Instead of manual steps, use:
```bash
python start_model.py
```

This automatically starts the correct model based on your `app_config.json`.

## üîç **Troubleshooting Steps**

### **1. Check All Components**
```bash
python debug_connection.py
```

Expected output:
```
‚úÖ Model Server is running and responding
‚úÖ Backend API is running and responding  
‚úÖ Frontend is running and responding
‚úÖ All components are running!
```

### **2. Test Chat Functionality**
```bash
python test_chat_completion.py
```

Expected output:
```
‚úÖ Backend chat completion successful!
‚úÖ Model server direct test successful!
‚úÖ All tests passed!
```

### **3. Test Dual-Loop Memory System**
```bash
# Test the complete system with dual-loop memory
python tests/stage_3_3/test_stage_3_3_final.py

# Test simulated steps
python tests/stage_3_3/test_simulated_steps.py
```

### **4. Common Issues & Solutions**

#### **Issue: "Connection Error" in Frontend**
- **Cause**: Model server not running or crashed
- **Solution**: Restart the model server for your active model

#### **Issue: "ASK ABOUT THE IMAGE is empty"**
- **Cause**: Frontend not getting the correct configuration
- **Solution**: 
  1. Check `app_config.json` has correct model name
  2. Restart backend: `cd src/backend && python main.py`
  3. Clear browser cache and refresh

#### **Issue: "Internal Server Error" (500)**
- **Cause**: Model failed to load or crashed
- **Solution**: 
  1. Stop model server (Ctrl+C)
  2. Restart with: `python run_[model_name].py`
  3. Check terminal for error messages

#### **Issue: Model server won't start**
- **Cause**: Missing dependencies
- **Solution**: Install required packages:
  ```bash
  # For MLX models (phi3_vision, llava_mlx, smolvlm2)
  pip install mlx-vlm
  
  # For standard models (moondream2, smolvlm)
  pip install transformers torch
  ```

#### **Issue: State Tracker not responding**
- **Cause**: RAG knowledge base not loaded or State Tracker not initialized
- **Solution**:
  1. Check backend logs for RAG initialization
  2. Verify `data/tasks/coffee_brewing.yaml` exists
  3. Restart backend server

#### **Issue: Query classification not working**
- **Cause**: Query processor not properly configured
- **Solution**:
  1. Check `src/state_tracker/query_processor.py` is up to date
  2. Verify English language patterns are loaded
  3. Test with: `python tests/stage_3_3/test_simulated_steps.py`

## üìã **Current Status Check**

Your current configuration:
- **Active Model**: `phi3_vision` 
- **Expected Server**: `src/models/phi3_vision_mlx/run_phi3_vision.py`
- **Backend Port**: 8000
- **Frontend Port**: 5500
- **Model Server Port**: 8080
- **Dual-Loop Memory**: ‚úÖ Active
- **RAG Knowledge Base**: ‚úÖ Loaded
- **State Tracker**: ‚úÖ Operational

## üéØ **Recommended Models (Based on Latest VQA 2.0 Results)**

### **üìä Performance Rankings (2025-07-29 13:12:58)**

| Model | VQA Accuracy | Simple Accuracy | Avg Inference | Status | Use Case |
|-------|:------------:|:---------------:|:-------------:|:------:|:--------:|
| **ü•á Moondream2** | **62.5%** | **65.0%** | 8.35s | ‚úÖ | **Best Overall** |
| **ü•à SmolVLM2** | **52.5%** | **55.0%** | 8.41s | ‚úÖ | **Balanced** |
| **‚ö° SmolVLM** | **36.0%** | **35.0%** | **0.39s** | ‚úÖ | **Fastest** |
| **ü•â Phi-3.5** | **35.0%** | **35.0%** | 5.29s | ‚úÖ | **Fast** |
| **‚ö†Ô∏è LLaVA-MLX** | **21.0%** | **20.0%** | 24.15s | üö´ | **Avoid** |

### **üéØ Production Recommendations**

#### **For High-Accuracy Applications:**
- **`"moondream2"`** - Highest accuracy (65.0% simple, 62.5% VQA)
- **Best for**: Production VQA, accuracy-critical tasks
- **Trade-off**: Slower loading (16.61s), vision-only

#### **For Real-Time Applications:**
- **`"smolvlm"`** - Fastest inference (0.39s average)
- **Best for**: Speed-critical applications, real-time processing
- **Trade-off**: Lower accuracy (35.0% simple, 36.0% VQA)

#### **For Balanced Performance:**
- **`"smolvlm2_500m_video"`** - Good accuracy/speed balance (55.0%, 8.41s)
- **Best for**: General-purpose VQA, video processing
- **Trade-off**: Moderate speed, good accuracy

#### **‚ö†Ô∏è Avoid for Production:**
- **`"llava_mlx"`** - Critical performance issues (24.15s inference, 20.0% accuracy)
- **Issues**: Extremely slow, poor accuracy, batch processing problems
- **Status**: Not recommended for any production use case

### **üö® Critical Context Understanding Limitation**
**ALL MODELS have 0% context understanding capability:**
- Cannot maintain conversation memory
- Cannot recall previous image information
- Multi-turn conversations require external memory (our dual-loop system addresses this)
- Each question must include the image for reliable results

## üß† **Dual-Loop Memory System Integration**

The AI Manual Assistant now features a sophisticated dual-loop memory system:

### **üîÑ Subconscious Loop**
- **Model Server** ‚Üí **Backend** ‚Üí **State Tracker** ‚Üí **RAG** ‚Üí **Memory**
- Continuous background monitoring of your workspace
- Automatic step identification and progress tracking

### **‚ö° Instant Response Loop**
- **User Query** ‚Üí **State Tracker** ‚Üí **Immediate Response**
- Sub-50ms response time for queries like:
  - "What step am I on?"
  - "What tools do I need?"
  - "How much progress have I made?"
  - "Help me with this step"

### **üéØ Testing the Memory System**
```bash
# Test complete dual-loop system
python tests/stage_3_3/test_stage_3_3_final.py

# Test simulated steps
python tests/stage_3_3/test_simulated_steps.py

# Test query classification
python tests/stage_3_3/test_simulated_steps.py
```

## üö® **Important Notes**

1. **Always restart the model server** after changing `app_config.json`
2. **Use exact model names** from the table above
3. **Check all three components** are running (model server, backend, frontend)
4. **Clear browser cache** if frontend shows old information
5. **Check terminal logs** for detailed error messages
6. **Test dual-loop memory** with query interface at `http://localhost:5500/query.html`
7. **Verify RAG knowledge base** is loaded in backend logs

## üîß **Fixed Issues**

- ‚úÖ Fixed `.loaded` attribute errors in health checks
- ‚úÖ Fixed chat completion endpoints  
- ‚úÖ Updated model configuration mapping
- ‚úÖ Created diagnostic and testing tools
- ‚úÖ Implemented dual-loop memory system
- ‚úÖ Added query classification with 100% accuracy
- ‚úÖ Enhanced error handling and user experience
- ‚úÖ Standardized system to English language

## üìä **Performance Metrics (Latest Results)**

### **üéØ VLM Performance (VQA 2.0 - 20 Questions)**
- **Moondream2**: 65.0% simple accuracy, 8.35s inference, 16.61s load time
- **SmolVLM2**: 55.0% simple accuracy, 8.41s inference, 1.48s load time
- **SmolVLM**: 35.0% simple accuracy, 0.39s inference, 4.05s load time
- **Phi-3.5**: 35.0% simple accuracy, 5.29s inference, 1.71s load time
- **LLaVA-MLX**: 20.0% simple accuracy, 24.15s inference, 6.07s load time

### **üß† System Performance**
- **Backend Processing**: < 100ms for VLM integration
- **State Tracker Response**: < 50ms for instant queries
- **RAG Search**: < 10ms for vector matching
- **Memory Usage**: < 1MB for sliding window
- **System Stability**: 100% recovery rate
- **Query Classification**: 100% accuracy

### **‚ö†Ô∏è Universal Limitations**
- **Context Understanding**: 0% capability across all models
- **Text Reading**: Poor performance on text within images
- **Counting Tasks**: 0-50% accuracy on numerical reasoning
- **Color Perception**: Frequent errors (white vs. gray, blue vs. green)

Your system now features comprehensive VLM testing results and a complete dual-loop memory system for intelligent task assistance with full awareness of model limitations!