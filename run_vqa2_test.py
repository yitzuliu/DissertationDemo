#!/usr/bin/env python3
"""
VQA 2.0 Quick Test Runner - å„ªåŒ–ç‰ˆæœ¬
å¿«é€ŸVQA 2.0è©•ä¼°è…³æœ¬ï¼Œæ”¯æŒå°æ‰¹é‡åœ–åƒä¸‹è¼‰å’Œè©³ç´°çµæœåˆ†æ

Usage:
    python run_vqa2_test.py --num_questions 20
    python run_vqa2_test.py --num_questions 20 --explanation
    
Author: AI Manual Assistant Team  
Date: 2025-01-27
"""

import os
import sys
import json
import argparse
import time
from pathlib import Path
from datetime import datetime

# Add project src to path
project_root = Path(__file__).parent
src_dir = project_root / "src"
sys.path.insert(0, str(src_dir))

try:
    from testing.vqa2_tester import VQA2Tester
    from testing.vqa2_config import VQA2_CONFIG
    from testing.vqa2_utils import print_vqa2_analysis, save_vqa2_results
except ImportError as e:
    print(f"âŒ Import error: {e}")
    print("è«‹ç¢ºèªæ‚¨åœ¨æ­£ç¢ºçš„ç›®éŒ„ä¸­ï¼Œä¸¦ä¸”æ‰€æœ‰VQA 2.0æ–‡ä»¶éƒ½å·²å‰µå»º")
    sys.exit(1)

def main():
    parser = argparse.ArgumentParser(description='Run VQA 2.0 evaluation with optimized image downloading')
    
    # Basic options
    parser.add_argument('--num_questions', type=int, default=20,
                       help='Number of questions to test (default: 20 for fast testing)')
    parser.add_argument('--models', nargs='+', 
                       default=['SmolVLM-Instruct'],
                       help='Models to test (default: SmolVLM-Instruct)')
    
    # Download options
    parser.add_argument('--force_download', action='store_true',
                       help='Force re-download of images')
    parser.add_argument('--sample_only', action='store_true', default=True,
                       help='Use optimized sample download for small batches (default: True)')
    parser.add_argument('--fallback_images', action='store_true',
                       help='Generate fallback images if COCO download fails')
    
    # Output options
    parser.add_argument('--output_dir', default='results',
                       help='Output directory for results')
    parser.add_argument('--save_detailed', action='store_true', default=True,
                       help='Save detailed results with evaluation metrics')
    
    # Testing options
    parser.add_argument('--explanation', action='store_true',
                       help='Show explanation of results format before testing')
    
    args = parser.parse_args()
    
    if args.explanation:
        print("ğŸ“š æ­£åœ¨é¡¯ç¤ºVQA 2.0çµæœæ ¼å¼èªªæ˜...")
        explain_script = project_root / "src" / "testing" / "example_vqa2_results.py"
        if explain_script.exists():
            os.system(f"python {explain_script}")
        else:
            print("èªªæ˜æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œç¹¼çºŒæ¸¬è©¦...")
        print("\n" + "="*60)
        input("æŒ‰Enteréµç¹¼çºŒæ¸¬è©¦ï¼Œæˆ–Ctrl+Cé€€å‡º...")
    
    print("ğŸš€ å•Ÿå‹•VQA 2.0å¿«é€Ÿæ¸¬è©¦")
    print(f"ğŸ“Š æ¸¬è©¦å•é¡Œæ•¸ï¼š{args.num_questions}")
    print(f"ğŸ¤– æ¸¬è©¦æ¨¡å‹ï¼š{', '.join(args.models)}")
    
    if args.num_questions <= 50:
        print("âš¡ ä½¿ç”¨å„ªåŒ–ä¸‹è¼‰æ¨¡å¼ï¼ˆå–®å¼µåœ–åƒä¸‹è¼‰ï¼‰")
    else:
        print("ğŸ“¦ ä½¿ç”¨å®Œæ•´æ•¸æ“šé›†ä¸‹è¼‰æ¨¡å¼")
    
    # Initialize tester with optimized settings
    try:
        tester = VQA2Tester(
            data_dir="data/vqa2",
            results_dir=args.output_dir,
            download_images=True
        )
        print("âœ… VQA2Tester åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ VQA2Tester åˆå§‹åŒ–å¤±æ•—: {e}")
        return 1
    
    start_time = time.time()
    
    try:
        # Run evaluation
        print("\nğŸ“ é–‹å§‹VQA 2.0è©•ä¼°...")
        results = tester.run_evaluation(
            models_to_test=args.models,
            num_questions=args.num_questions,
            save_results=True
        )
        
        # Print summary
        print("\n" + "="*60)
        print("ğŸ“ˆ æ¸¬è©¦å®Œæˆï¼çµæœæ‘˜è¦ï¼š")
        print("="*60)
        
        for model_name, model_results in results.items():
            if model_name == "test_metadata":
                continue
                
            accuracy = model_results.get("accuracy", 0)
            vqa_accuracy = model_results.get("average_vqa_accuracy", 0)
            correct = model_results.get("correct_answers", 0)
            total = model_results.get("total_questions", 0)
            avg_time = model_results.get("average_inference_time", 0)
            
            print(f"\nğŸ¤– {model_name}:")
            print(f"   âœ… æ­£ç¢ºç­”æ¡ˆï¼š{correct}/{total}")
            print(f"   ğŸ“Š ç°¡å–®æº–ç¢ºåº¦ï¼š{accuracy:.1%}")
            print(f"   ğŸ¯ VQAæº–ç¢ºåº¦ï¼š{vqa_accuracy:.1%}")
            print(f"   â±ï¸  å¹³å‡æ¨ç†æ™‚é–“ï¼š{avg_time:.2f}ç§’")
            
            # Performance assessment
            if vqa_accuracy >= 0.6:
                assessment = "ğŸ† è¡¨ç¾å„ªç§€"
            elif vqa_accuracy >= 0.4:
                assessment = "ğŸ¯ è¡¨ç¾ä¸­ç­‰"
            else:
                assessment = "ğŸ”§ éœ€è¦æ”¹é€²"
            print(f"   {assessment}")
        
        # Show file locations
        print(f"\nğŸ“ è©³ç´°çµæœå·²ä¿å­˜åˆ°ï¼š{args.output_dir}/")
        print("   - JSONæ ¼å¼ï¼šå®Œæ•´çµæœæ•¸æ“š")
        print("   - TXTæ ¼å¼ï¼šå¯è®€æ‘˜è¦") 
        print("   - CSVæ ¼å¼ï¼šéŒ¯èª¤åˆ†æï¼ˆå¦‚æœ‰ï¼‰")
        
        total_time = time.time() - start_time
        print(f"\nâ±ï¸  ç¸½æ¸¬è©¦æ™‚é–“ï¼š{total_time:.1f}ç§’")
        
        # Quick analysis tips
        print("\nğŸ’¡ çµæœåˆ†ææç¤ºï¼š")
        print("1. VQAæº–ç¢ºåº¦æ¯”ç°¡å–®æº–ç¢ºåº¦æ›´æº–ç¢ºï¼ˆè€ƒæ…®æ¨™è¨»è€…ä¸€è‡´æ€§ï¼‰")
        print("2. æª¢æŸ¥detailed resultsä¸­çš„evaluation_detailsäº†è§£éŒ¯èª¤é¡å‹")
        print("3. æ¯”è¼ƒä¸åŒæ¨¡å‹åœ¨ä¸åŒå•é¡Œé¡å‹ä¸Šçš„è¡¨ç¾")
        print("4. æ³¨æ„confidenceèˆ‡accuracyçš„ç›¸é—œæ€§")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  æ¸¬è©¦è¢«ä½¿ç”¨è€…ä¸­æ–·")
        return 1
    except Exception as e:
        print(f"\nâŒ æ¸¬è©¦å¤±æ•—ï¼š{str(e)}")
        print("\nğŸ”§ troubleshootingå»ºè­°ï¼š")
        print("1. æª¢æŸ¥ç¶²çµ¡é€£æ¥ï¼ˆä¸‹è¼‰COCOåœ–åƒéœ€è¦ï¼‰")
        print("2. ç¢ºèªæ¨¡å‹å¯ç”¨æ€§")
        print("3. æª¢æŸ¥ç£ç›¤ç©ºé–“")
        print("4. å˜—è©¦ä½¿ç”¨ --fallback_images é¸é …")
        return 1
    
    return 0

def run_quick_vqa2_test():
    """èˆŠç‰ˆæœ¬å…¼å®¹æ€§å‡½æ•¸"""
    print("ğŸ”„ ä½¿ç”¨æ–°ç‰ˆæ¸¬è©¦è…³æœ¬...")
    return main()

if __name__ == "__main__":
    sys.exit(main())
    """Run a quick VQA 2.0 test with sample data"""
    print("ğŸš€ Starting VQA 2.0 Quick Test")
    print("=" * 50)
    
    # Initialize tester
        # Remove old code that has syntax errors
        pass  # This section will be replaced by the new main function
    
    # Save results
    output_dir = Path("logs/vqa2_results")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = output_dir / f"quick_test_{timestamp}.json"
    
    summary_results = {
        'test_type': 'quick_test',
        'timestamp': timestamp,
        'sample_size': len(sample_questions),
        'models_tested': list(results.keys()),
        'results': results,
        'config': {
            'max_questions_per_model': 10,
            'dataset': 'VQA 2.0 validation sample'
        }
    }
    
    save_vqa2_results(summary_results, results_file)
    
    # Print summary
    print("\nğŸ“‹ Test Summary")
    print("=" * 30)
    
    best_accuracy = 0
    best_model = None
    
    for model_name, model_results in results.items():
        accuracy = model_results.get('accuracy', 0)
        print(f"{model_name}: {accuracy:.3f} accuracy")
        
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            best_model = model_name
    
    if best_model:
        print(f"\nğŸ† Best performing model: {best_model} ({best_accuracy:.3f})")
    
    print(f"\nğŸ’¾ Full results saved to: {results_file}")
    print("\nâœ… Quick test completed!")

def run_sample_analysis_only():
    """Run only sample data analysis without model testing"""
    print("ğŸ“Š VQA 2.0 Sample Analysis Only")
    print("=" * 40)
    
    try:
        tester = VQA2Tester()
        
        # Load or create sample data
        sample_questions, sample_annotations = tester.load_sample_data(sample_size=50)
        
        # Analyze sample
        analysis = tester.analyze_sample(sample_questions, sample_annotations)
        print_vqa2_analysis(analysis)
        
        # Show some example questions
        print("\nğŸ” Example Questions:")
        print("-" * 30)
        
        for i, question in enumerate(sample_questions[:3]):
            question_id = question['question_id']
            question_text = question['question']
            
            print(f"\nExample {i+1}:")
            print(f"  Question: {question_text}")
            
            if question_id in sample_annotations:
                annotation = sample_annotations[question_id]
                answer = annotation['multiple_choice_answer']
                print(f"  Answer: {answer}")
        
        print("\nâœ… Sample analysis completed!")
        
    except Exception as e:
        print(f"âŒ Error during analysis: {e}")

def show_help():
    """Show help information"""
    print("ğŸ”§ VQA 2.0 Test Runner Help")
    print("=" * 40)
    print("\nAvailable commands:")
    print("  python run_vqa2_test.py              - Run quick test (default)")
    print("  python run_vqa2_test.py --analysis  - Run analysis only")
    print("  python run_vqa2_test.py --help      - Show this help")
    print("\nFor full testing, use:")
    print("  python vqa2_tester.py")

def main():
    """Main function"""
    if len(sys.argv) > 1:
        if '--help' in sys.argv or '-h' in sys.argv:
            show_help()
            return
        elif '--analysis' in sys.argv:
            run_sample_analysis_only()
            return
    
    # Default: run quick test
    run_quick_vqa2_test()

if __name__ == "__main__":
    main()
