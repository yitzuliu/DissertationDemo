# RAG ç³»çµ±é‹ä½œæŠ€è¡“æ–‡æª”

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æª”è©³ç´°èªªæ˜ Vision Intelligence Hub ä¸­ RAG (Retrieval-Augmented Generation) ç³»çµ±çš„å®Œæ•´é‹ä½œæ©Ÿåˆ¶ï¼ŒåŒ…æ‹¬ç³»çµ±æ¶æ§‹ã€å·¥ä½œæµç¨‹ã€åŒ¹é…ç­–ç•¥å’Œå›æ‡‰ç”Ÿæˆã€‚

## ğŸ—ï¸ ç³»çµ±æ¶æ§‹

### **æ ¸å¿ƒçµ„ä»¶**

```
RAG ç³»çµ±æ¶æ§‹
â”œâ”€â”€ ğŸ“ æ•¸æ“šå±¤ (Data Layer)
â”‚   â”œâ”€â”€ data/tasks/          # ä»»å‹™çŸ¥è­˜æ–‡ä»¶
â”‚   â”œâ”€â”€ cache/embeddings/    # å‘é‡ç·©å­˜
â”‚   â””â”€â”€ cache/embeddings_optimizer/  # å„ªåŒ–ç·©å­˜
â”‚
â”œâ”€â”€ ğŸ”§ è™•ç†å±¤ (Processing Layer)
â”‚   â”œâ”€â”€ TaskKnowledgeLoader  # ä»»å‹™è¼‰å…¥å™¨
â”‚   â”œâ”€â”€ ChromaVectorSearchEngine  # å‘é‡æœç´¢å¼•æ“
â”‚   â”œâ”€â”€ VectorOptimizer      # å‘é‡å„ªåŒ–å™¨
â”‚   â””â”€â”€ Validation          # æ•¸æ“šé©—è­‰
â”‚
â”œâ”€â”€ ğŸ¯ é‚è¼¯å±¤ (Logic Layer)
â”‚   â”œâ”€â”€ RAGKnowledgeBase    # çŸ¥è­˜åº«æ ¸å¿ƒ
â”‚   â”œâ”€â”€ MatchResult         # åŒ¹é…çµæœ
â”‚   â””â”€â”€ SearchStrategy      # æœç´¢ç­–ç•¥
â”‚
â””â”€â”€ ğŸŒ æ¥å£å±¤ (Interface Layer)
    â”œâ”€â”€ State Tracker       # ç‹€æ…‹è¿½è¹¤å™¨
    â”œâ”€â”€ VLM Integration     # VLM é›†æˆ
    â””â”€â”€ Response Generator  # å›æ‡‰ç”Ÿæˆå™¨
```

### **æ•¸æ“šæµåœ–**

```
ç”¨æˆ¶è§€å¯Ÿ â†’ VLM è™•ç† â†’ RAG åŒ¹é… â†’ å›æ‡‰ç”Ÿæˆ
    â†“           â†“          â†“          â†“
è¦–è¦ºè¼¸å…¥   æ–‡æœ¬æè¿°   å‘é‡æœç´¢   æ™ºèƒ½å›æ‡‰
    â†“           â†“          â†“          â†“
åœ–åƒæ•¸æ“š   è§€å¯Ÿæ–‡æœ¬   ç›¸ä¼¼åº¦è¨ˆç®—   æŒ‡å°å…§å®¹
```

## ğŸ”„ å®Œæ•´å·¥ä½œæµç¨‹

### **éšæ®µ 1ï¼šç³»çµ±åˆå§‹åŒ–**

#### **1.1 ä»»å‹™è¼‰å…¥**
```python
# è‡ªå‹•æƒæ data/tasks/ ç›®éŒ„
yaml_files = list(self.tasks_directory.glob("*.yaml")) + list(self.tasks_directory.glob("*.yml"))

for file_path in yaml_files:
    task_name = file_path.stem  # æå–æ–‡ä»¶åï¼ˆä¸å«æ“´å±•åï¼‰
    task_knowledge = self.load_task(task_name, file_path)
    self.loaded_tasks[task_name] = task_knowledge
```

#### **1.2 å‘é‡åŒ–è™•ç†**
```python
# ç‚ºæ¯å€‹ä»»å‹™æ­¥é©Ÿç”Ÿæˆå‘é‡åµŒå…¥
for task_name, task in self.loaded_tasks.items():
    for step in task.steps:
        # çµ„åˆæ­¥é©Ÿä¿¡æ¯
        step_text = f"{step.title} {step.task_description} {' '.join(step.visual_cues)}"
        
        # ç”Ÿæˆå‘é‡åµŒå…¥
        embedding = self.vector_engine.encode_text(step_text)
        
        # å­˜å„²åˆ°å‘é‡æ•¸æ“šåº«
        self.vector_engine.add_document(
            task_name=task_name,
            step_id=step.step_id,
            text=step_text,
            embedding=embedding
        )
```

#### **1.3 ç·©å­˜å„ªåŒ–**
```python
# é è¨ˆç®—æ‰€æœ‰åµŒå…¥ä¸¦ç·©å­˜
if precompute_embeddings:
    self.vector_optimizer.precompute_all_embeddings(self.loaded_tasks)
```

### **éšæ®µ 2ï¼šè§€å¯Ÿè™•ç†**

#### **2.1 VLM è¦–è¦ºåˆ†æ**
```python
# VLM è™•ç†è¦–è¦ºè¼¸å…¥
def process_visual_observation(image_data):
    # 1. åœ–åƒé è™•ç†
    processed_image = preprocess_image(image_data)
    
    # 2. VLM åˆ†æ
    observation_text = vlm_model.analyze(processed_image)
    
    # 3. æ–‡æœ¬æ¸…ç†å’Œæ¨™æº–åŒ–
    cleaned_observation = clean_and_normalize_text(observation_text)
    
    return cleaned_observation
```

#### **2.2 è§€å¯Ÿæ–‡æœ¬ç”Ÿæˆ**
```python
# ç”Ÿæˆçµæ§‹åŒ–çš„è§€å¯Ÿæè¿°
observation_data = {
    "raw_text": observation_text,
    "visual_elements": extract_visual_elements(observation_text),
    "actions": extract_actions(observation_text),
    "objects": extract_objects(observation_text),
    "context": extract_context(observation_text)
}
```

### **éšæ®µ 3ï¼šRAG åŒ¹é…**

#### **3.1 å¤šå±¤æ¬¡åŒ¹é…ç­–ç•¥**

##### **å„ªå…ˆç´š 1ï¼šç²¾ç¢ºè¦–è¦ºç·šç´¢åŒ¹é…**
```python
def exact_visual_cue_matching(observation, visual_cues):
    """ç²¾ç¢ºè¦–è¦ºç·šç´¢åŒ¹é…"""
    matched_cues = []
    for cue in visual_cues:
        if cue.lower() in observation.lower():
            matched_cues.append(cue)
    return matched_cues

# è¦–è¦ºç·šç´¢æ˜ å°„è¡¨
visual_cue_mapping = {
    "coffee_brewing": {
        "coffee_beans": {"step_id": 1, "confidence": 0.95},
        "grinder": {"step_id": 3, "confidence": 0.90},
        "kettle": {"step_id": 2, "confidence": 0.85},
        "filter_paper": {"step_id": 4, "confidence": 0.88}
    },
    "daily_journaling": {
        "journal_notebook": {"step_id": 1, "confidence": 0.92},
        "writing_pen": {"step_id": 2, "confidence": 0.89},
        "reflective_expression": {"step_id": 3, "confidence": 0.87}
    }
}
```

##### **å„ªå…ˆç´š 2ï¼šèªç¾©ç›¸ä¼¼åº¦åŒ¹é…**
```python
def semantic_similarity_matching(observation, task_steps):
    """èªç¾©ç›¸ä¼¼åº¦åŒ¹é…"""
    similarities = []
    
    for task_name, steps in task_steps.items():
        for step in steps:
            # çµ„åˆæ­¥é©Ÿæ–‡æœ¬
            step_text = f"{step.title} {step.task_description} {' '.join(step.visual_cues)}"
            
            # è¨ˆç®—èªç¾©ç›¸ä¼¼åº¦
            similarity = calculate_semantic_similarity(observation, step_text)
            
            similarities.append({
                "task_name": task_name,
                "step_id": step.step_id,
                "similarity": similarity,
                "step_text": step_text
            })
    
    # æŒ‰ç›¸ä¼¼åº¦æ’åº
    similarities.sort(key=lambda x: x["similarity"], reverse=True)
    return similarities
```

##### **å„ªå…ˆç´š 3ï¼šæ¨™ç±¤å’Œé¡åˆ¥åŒ¹é…**
```python
def tag_and_category_matching(observation, task_metadata):
    """æ¨™ç±¤å’Œé¡åˆ¥åŒ¹é…"""
    matches = []
    
    for task_name, metadata in task_metadata.items():
        # æª¢æŸ¥æ¨™ç±¤åŒ¹é…
        tag_matches = []
        for tag in metadata.get("tags", []):
            if tag.lower() in observation.lower():
                tag_matches.append(tag)
        
        # æª¢æŸ¥é¡åˆ¥åŒ¹é…
        category = metadata.get("category", "")
        category_match = category.lower() in observation.lower()
        
        if tag_matches or category_match:
            matches.append({
                "task_name": task_name,
                "tag_matches": tag_matches,
                "category_match": category_match,
                "confidence": len(tag_matches) * 0.3 + (1 if category_match else 0) * 0.2
            })
    
    return matches
```

#### **3.2 ç¶œåˆåŒ¹é…ç®—æ³•**
```python
def comprehensive_matching(observation, knowledge_base):
    """ç¶œåˆåŒ¹é…ç®—æ³•"""
    results = []
    
    # 1. ç²¾ç¢ºè¦–è¦ºç·šç´¢åŒ¹é…
    exact_matches = exact_visual_cue_matching(observation, knowledge_base.visual_cues)
    
    # 2. èªç¾©ç›¸ä¼¼åº¦åŒ¹é…
    semantic_matches = semantic_similarity_matching(observation, knowledge_base.task_steps)
    
    # 3. æ¨™ç±¤å’Œé¡åˆ¥åŒ¹é…
    tag_matches = tag_and_category_matching(observation, knowledge_base.task_metadata)
    
    # 4. ç¶œåˆè©•åˆ†
    for semantic_match in semantic_matches:
        score = semantic_match["similarity"]
        
        # å¦‚æœæœ‰ç²¾ç¢ºè¦–è¦ºç·šç´¢åŒ¹é…ï¼Œæå‡åˆ†æ•¸
        if any(cue in observation for cue in exact_matches):
            score *= 1.5
        
        # å¦‚æœæœ‰æ¨™ç±¤åŒ¹é…ï¼Œæå‡åˆ†æ•¸
        for tag_match in tag_matches:
            if tag_match["task_name"] == semantic_match["task_name"]:
                score += tag_match["confidence"] * 0.3
        
        results.append({
            **semantic_match,
            "final_score": score
        })
    
    # æŒ‰æœ€çµ‚åˆ†æ•¸æ’åº
    results.sort(key=lambda x: x["final_score"], reverse=True)
    return results
```

### **éšæ®µ 4ï¼šå›æ‡‰ç”Ÿæˆ**

#### **4.1 ç½®ä¿¡åº¦è©•ä¼°**
```python
def evaluate_confidence(match_result):
    """è©•ä¼°åŒ¹é…ç½®ä¿¡åº¦"""
    similarity = match_result.similarity
    
    if similarity > 0.8:
        return "high", "ğŸŸ¢ é«˜ç½®ä¿¡åº¦"
    elif similarity > 0.6:
        return "medium", "ğŸŸ¡ ä¸­ç­‰ç½®ä¿¡åº¦"
    elif similarity > 0.4:
        return "low", "ğŸŸ  ä½ç½®ä¿¡åº¦"
    else:
        return "very_low", "ğŸ”´ æ¥µä½ç½®ä¿¡åº¦"
```

#### **4.2 å›æ‡‰ç­–ç•¥é¸æ“‡**
```python
def select_response_strategy(confidence_level, match_result):
    """é¸æ“‡å›æ‡‰ç­–ç•¥"""
    
    if confidence_level == "high":
        return generate_detailed_guidance(match_result)
    elif confidence_level == "medium":
        return generate_general_guidance(match_result)
    elif confidence_level == "low":
        return generate_suggestive_guidance(match_result)
    else:
        return generate_inquiry_response(match_result)

def generate_detailed_guidance(match_result):
    """ç”Ÿæˆè©³ç´°æŒ‡å°"""
    step_details = get_step_details(match_result.task_name, match_result.step_id)
    
    return {
        "type": "detailed_guidance",
        "content": {
            "step_title": step_details["title"],
            "description": step_details["task_description"],
            "tools_needed": step_details["tools_needed"],
            "completion_indicators": step_details["completion_indicators"],
            "safety_notes": step_details["safety_notes"],
            "estimated_duration": step_details["estimated_duration"]
        },
        "confidence": match_result.similarity,
        "next_steps": get_next_step_suggestions(match_result)
    }

def generate_general_guidance(match_result):
    """ç”Ÿæˆä¸€èˆ¬æŒ‡å°"""
    return {
        "type": "general_guidance",
        "content": {
            "suggested_task": match_result.task_name,
            "suggested_step": match_result.step_id,
            "general_advice": get_general_advice(match_result),
            "alternative_suggestions": get_alternative_suggestions(match_result)
        },
        "confidence": match_result.similarity
    }

def generate_suggestive_guidance(match_result):
    """ç”Ÿæˆå»ºè­°æ€§æŒ‡å°"""
    return {
        "type": "suggestive_guidance",
        "content": {
            "possible_tasks": get_possible_tasks(match_result),
            "clarification_questions": generate_clarification_questions(match_result),
            "general_tips": get_general_tips()
        },
        "confidence": match_result.similarity
    }

def generate_inquiry_response(match_result):
    """ç”Ÿæˆè©¢å•å›æ‡‰"""
    return {
        "type": "inquiry",
        "content": {
            "message": "æˆ‘éœ€è¦æ›´å¤šä¿¡æ¯ä¾†æä¾›æº–ç¢ºçš„æŒ‡å°",
            "questions": [
                "æ‚¨æ­£åœ¨é€²è¡Œä»€éº¼é¡å‹çš„æ´»å‹•ï¼Ÿ",
                "æ‚¨çœ‹åˆ°äº†å“ªäº›å…·é«”çš„å·¥å…·æˆ–è¨­å‚™ï¼Ÿ",
                "æ‚¨çš„ç›®æ¨™æ˜¯ä»€éº¼ï¼Ÿ"
            ],
            "available_tasks": list_available_tasks()
        },
        "confidence": match_result.similarity
    }
```

## ğŸ¯ åŒ¹é…æ©Ÿåˆ¶è©³è§£

### **å¤šå±¤æ¬¡åŒ¹é…ç­–ç•¥**

#### **å±¤æ¬¡ 1ï¼šè¦–è¦ºç·šç´¢åŒ¹é…ï¼ˆæœ€é«˜å„ªå…ˆç´šï¼‰**
```python
# è¦–è¦ºç·šç´¢æ¬Šé‡é…ç½®
visual_cue_weights = {
    "exact_match": 1.0,      # ç²¾ç¢ºåŒ¹é…
    "partial_match": 0.7,    # éƒ¨åˆ†åŒ¹é…
    "semantic_match": 0.5,   # èªç¾©åŒ¹é…
    "category_match": 0.3    # é¡åˆ¥åŒ¹é…
}

def calculate_visual_cue_score(observation, visual_cues):
    """è¨ˆç®—è¦–è¦ºç·šç´¢åŒ¹é…åˆ†æ•¸"""
    score = 0.0
    
    for cue in visual_cues:
        if cue.lower() in observation.lower():
            score += visual_cue_weights["exact_match"]
        elif any(word in observation.lower() for word in cue.lower().split()):
            score += visual_cue_weights["partial_match"]
    
    return min(score, 1.0)  # é™åˆ¶æœ€å¤§åˆ†æ•¸ç‚º 1.0
```

#### **å±¤æ¬¡ 2ï¼šèªç¾©ç›¸ä¼¼åº¦åŒ¹é…**
```python
def calculate_semantic_similarity(text1, text2):
    """è¨ˆç®—èªç¾©ç›¸ä¼¼åº¦"""
    # ä½¿ç”¨é è¨“ç·´çš„å¥å­åµŒå…¥æ¨¡å‹
    embeddings = sentence_transformer.encode([text1, text2])
    
    # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
    similarity = cosine_similarity(
        embeddings[0].reshape(1, -1), 
        embeddings[1].reshape(1, -1)
    )[0][0]
    
    return float(similarity)
```

#### **å±¤æ¬¡ 3ï¼šä¸Šä¸‹æ–‡åŒ¹é…**
```python
def calculate_context_score(observation, task_context):
    """è¨ˆç®—ä¸Šä¸‹æ–‡åŒ¹é…åˆ†æ•¸"""
    context_keywords = {
        "coffee_brewing": ["coffee", "brewing", "drink", "morning", "kitchen"],
        "daily_journaling": ["journal", "writing", "reflection", "personal", "thoughts"]
    }
    
    task_keywords = context_keywords.get(task_context, [])
    matched_keywords = sum(1 for keyword in task_keywords if keyword in observation.lower())
    
    return matched_keywords / len(task_keywords) if task_keywords else 0.0
```

### **ç¶œåˆè©•åˆ†ç®—æ³•**
```python
def calculate_final_score(observation, match_result):
    """è¨ˆç®—æœ€çµ‚åŒ¹é…åˆ†æ•¸"""
    
    # åŸºç¤èªç¾©ç›¸ä¼¼åº¦
    base_similarity = match_result.similarity
    
    # è¦–è¦ºç·šç´¢åŠ åˆ†
    visual_cue_score = calculate_visual_cue_score(
        observation, 
        get_step_visual_cues(match_result.task_name, match_result.step_id)
    )
    
    # ä¸Šä¸‹æ–‡åŠ åˆ†
    context_score = calculate_context_score(
        observation, 
        get_task_category(match_result.task_name)
    )
    
    # ç¶œåˆè¨ˆç®—
    final_score = (
        base_similarity * 0.6 +      # èªç¾©ç›¸ä¼¼åº¦æ¬Šé‡ 60%
        visual_cue_score * 0.3 +     # è¦–è¦ºç·šç´¢æ¬Šé‡ 30%
        context_score * 0.1          # ä¸Šä¸‹æ–‡æ¬Šé‡ 10%
    )
    
    return min(final_score, 1.0)  # é™åˆ¶æœ€å¤§åˆ†æ•¸ç‚º 1.0
```

## ğŸ“Š æ€§èƒ½å„ªåŒ–

### **å‘é‡ç·©å­˜ç­–ç•¥**
```python
class VectorCache:
    def __init__(self, cache_dir):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.embeddings_cache = {}
        self.metadata_cache = {}
    
    def get_cached_embedding(self, text_hash):
        """ç²å–ç·©å­˜çš„å‘é‡åµŒå…¥"""
        cache_file = self.cache_dir / f"{text_hash}.npy"
        if cache_file.exists():
            return np.load(cache_file)
        return None
    
    def cache_embedding(self, text_hash, embedding):
        """ç·©å­˜å‘é‡åµŒå…¥"""
        cache_file = self.cache_dir / f"{text_hash}.npy"
        np.save(cache_file, embedding)
        self.embeddings_cache[text_hash] = embedding
```

### **æœç´¢å„ªåŒ–**
```python
class OptimizedSearchEngine:
    def __init__(self):
        self.index = None
        self.embeddings = []
        self.metadata = []
    
    def build_index(self, embeddings):
        """æ§‹å»ºæœç´¢ç´¢å¼•"""
        self.embeddings = np.array(embeddings)
        self.index = faiss.IndexFlatIP(self.embeddings.shape[1])
        self.index.add(self.embeddings.astype('float32'))
    
    def search(self, query_embedding, top_k=5):
        """å¿«é€Ÿæœç´¢"""
        query_vector = query_embedding.reshape(1, -1).astype('float32')
        similarities, indices = self.index.search(query_vector, top_k)
        
        return [
            {
                "index": int(idx),
                "similarity": float(sim),
                "metadata": self.metadata[idx]
            }
            for idx, sim in zip(indices[0], similarities[0])
        ]
```

## ğŸ”§ é…ç½®å’Œèª¿å„ª

### **ç³»çµ±é…ç½®åƒæ•¸**
```python
RAG_CONFIG = {
    # ç›¸ä¼¼åº¦é–¾å€¼
    "similarity_thresholds": {
        "high_confidence": 0.7,
        "medium_confidence": 0.5,
        "low_confidence": 0.35,
        "minimum_confidence": 0.2
    },
    
    # æœç´¢åƒæ•¸
    "search_params": {
        "top_k": 5,
        "max_search_time_ms": 100,
        "enable_caching": True,
        "cache_ttl_seconds": 3600
    },
    
    # å‘é‡æ¨¡å‹é…ç½®
    "vector_model": {
        "name": "all-MiniLM-L6-v2",
        "max_length": 512,
        "normalize_embeddings": True
    },
    
    # æ€§èƒ½é…ç½®
    "performance": {
        "precompute_embeddings": True,
        "enable_optimization": True,
        "max_concurrent_searches": 10
    }
}
```

### **æ€§èƒ½ç›£æ§**
```python
class RAGPerformanceMonitor:
    def __init__(self):
        self.metrics = {
            "total_searches": 0,
            "average_search_time": 0.0,
            "cache_hit_rate": 0.0,
            "accuracy_metrics": {}
        }
    
    def record_search(self, search_time, cache_hit, accuracy):
        """è¨˜éŒ„æœç´¢æ€§èƒ½"""
        self.metrics["total_searches"] += 1
        self.metrics["average_search_time"] = (
            (self.metrics["average_search_time"] * (self.metrics["total_searches"] - 1) + search_time) /
            self.metrics["total_searches"]
        )
        
        # æ›´æ–°ç·©å­˜å‘½ä¸­ç‡
        if cache_hit:
            self.metrics["cache_hit_rate"] = (
                (self.metrics["cache_hit_rate"] * (self.metrics["total_searches"] - 1) + 1) /
                self.metrics["total_searches"]
            )
    
    def get_performance_report(self):
        """ç²å–æ€§èƒ½å ±å‘Š"""
        return {
            "total_searches": self.metrics["total_searches"],
            "average_search_time_ms": self.metrics["average_search_time"] * 1000,
            "cache_hit_rate_percent": self.metrics["cache_hit_rate"] * 100,
            "performance_grade": self.calculate_performance_grade()
        }
```

## ğŸ§ª æ¸¬è©¦å’Œé©—è­‰

### **å–®å…ƒæ¸¬è©¦**
```python
def test_rag_matching():
    """æ¸¬è©¦ RAG åŒ¹é…åŠŸèƒ½"""
    kb = RAGKnowledgeBase()
    kb.initialize()
    
    # æ¸¬è©¦ç”¨ä¾‹
    test_cases = [
        {
            "observation": "æˆ‘çœ‹åˆ°å’–å•¡è±†å’Œç£¨è±†æ©Ÿ",
            "expected_task": "coffee_brewing",
            "expected_step": 1,
            "min_confidence": 0.6
        },
        {
            "observation": "ç”¨æˆ¶æ‹¿è‘—æ—¥è¨˜æœ¬æº–å‚™å¯«ä½œ",
            "expected_task": "daily_journaling",
            "expected_step": 1,
            "min_confidence": 0.6
        }
    ]
    
    for test_case in test_cases:
        result = kb.find_matching_step(test_case["observation"])
        
        assert result.task_name == test_case["expected_task"], \
            f"Expected {test_case['expected_task']}, got {result.task_name}"
        
        assert result.step_id == test_case["expected_step"], \
            f"Expected step {test_case['expected_step']}, got {result.step_id}"
        
        assert result.similarity >= test_case["min_confidence"], \
            f"Confidence {result.similarity} below threshold {test_case['min_confidence']}"
```

### **æ€§èƒ½æ¸¬è©¦**
```python
def test_rag_performance():
    """æ¸¬è©¦ RAG æ€§èƒ½"""
    kb = RAGKnowledgeBase()
    kb.initialize()
    
    # æ€§èƒ½åŸºæº–
    performance_targets = {
        "search_time_ms": 100,
        "accuracy_rate": 0.8,
        "memory_usage_mb": 50
    }
    
    # åŸ·è¡Œæ€§èƒ½æ¸¬è©¦
    start_time = time.time()
    for _ in range(100):
        result = kb.find_matching_step("æ¸¬è©¦è§€å¯Ÿ")
    
    total_time = (time.time() - start_time) * 1000
    avg_time = total_time / 100
    
    assert avg_time <= performance_targets["search_time_ms"], \
        f"Average search time {avg_time}ms exceeds target {performance_targets['search_time_ms']}ms"
```

## ğŸ“ˆ ç›£æ§å’Œç¶­è­·

### **å¥åº·æª¢æŸ¥**
```python
def rag_health_check():
    """RAG ç³»çµ±å¥åº·æª¢æŸ¥"""
    health_status = {
        "status": "healthy",
        "issues": [],
        "warnings": [],
        "metrics": {}
    }
    
    try:
        kb = RAGKnowledgeBase()
        kb.initialize()
        
        # æª¢æŸ¥åŸºæœ¬åŠŸèƒ½
        result = kb.find_matching_step("æ¸¬è©¦è§€å¯Ÿ")
        if not result:
            health_status["issues"].append("Basic search functionality failed")
        
        # æª¢æŸ¥æ€§èƒ½æŒ‡æ¨™
        stats = kb.get_system_stats()
        if stats["vector_engine"]["avg_search_time_ms"] > 100:
            health_status["warnings"].append("Search performance below target")
        
        # æª¢æŸ¥ç·©å­˜ç‹€æ…‹
        if stats["vector_optimizer"]["cache_hits"] == 0:
            health_status["warnings"].append("No cache hits detected")
        
        health_status["metrics"] = stats
        
    except Exception as e:
        health_status["status"] = "error"
        health_status["issues"].append(f"System error: {str(e)}")
    
    return health_status
```

### **æ—¥èªŒè¨˜éŒ„**
```python
def log_rag_operation(operation_type, details, performance_metrics):
    """è¨˜éŒ„ RAG æ“ä½œ"""
    log_entry = {
        "timestamp": datetime.now().isoformat(),
        "operation_type": operation_type,
        "details": details,
        "performance": performance_metrics,
        "system_version": RAG_SYSTEM_VERSION
    }
    
    # å¯«å…¥æ—¥èªŒæ–‡ä»¶
    with open("logs/rag_operations.log", "a") as f:
        json.dump(log_entry, f)
        f.write("\n")
```

## ğŸ”® æœªä¾†æ“´å±•

### **è¨ˆåŠƒä¸­çš„åŠŸèƒ½**
1. **å‹•æ…‹å­¸ç¿’**ï¼šæ ¹æ“šç”¨æˆ¶åé¥‹èª¿æ•´åŒ¹é…ç­–ç•¥
2. **å¤šæ¨¡æ…‹æ”¯æŒ**ï¼šæ”¯æŒéŸ³é »ã€è¦–é »ç­‰å¤šç¨®è¼¸å…¥
3. **å€‹æ€§åŒ–é©é…**ï¼šæ ¹æ“šç”¨æˆ¶åå¥½èª¿æ•´å›æ‡‰é¢¨æ ¼
4. **å¯¦æ™‚æ›´æ–°**ï¼šæ”¯æŒä»»å‹™çŸ¥è­˜çš„å‹•æ…‹æ›´æ–°

### **API æ“´å±•**
```python
# RESTful API è¨­è¨ˆ
@app.post("/api/v1/rag/search")
async def rag_search(request: RAGSearchRequest):
    """RAG æœç´¢ API"""
    pass

@app.get("/api/v1/rag/tasks")
async def get_available_tasks():
    """ç²å–å¯ç”¨ä»»å‹™åˆ—è¡¨"""
    pass

@app.post("/api/v1/rag/feedback")
async def submit_feedback(request: RAGFeedbackRequest):
    """æäº¤ç”¨æˆ¶åé¥‹"""
    pass
```

---

**ç‰ˆæœ¬**ï¼š1.0.0  
**æœ€å¾Œæ›´æ–°**ï¼š2025-08-01  
**ç¶­è­·è€…**ï¼šVision Intelligence Hub é–‹ç™¼åœ˜éšŠ 